title,selftext,score,num_comments,created,author,vader_compound,vader_positive,vader_negative,vader_neutral,textblob_polarity,textblob_subjectivity
"[D] Honest question: Does the world need another productivity app, or is FlowTask dead on arrival?","Hello everyone,

I spent an eternity trying out ""the ultimate"" productivity system. I tried out Notion, Trello, Todoist, ClickUp, spreadsheets… you name it. It was always the same problem: I invested more time in setting up the system than I invested in actually using it.

Then I made a small thing for myself ""FlowTask"".

It does two things:

Single prompt - entire workspace. Instead of start at a bare piece of paper, I just type something like ""plan a product launch."" and it gives me tasks, docs, deadlines all set out.

FlowBot (Contextually Aware Bot). Rather than a chatbot in a separate tab, this one is in my workspace and is smart enough about what project/page I am in. So if I paste a contract in there, I can ask it ""highlight risky clauses"" and it just. does it.

I'd had it for a few weeks now and truthfully it's made those last-minute ""oh crap, it's due tomorrow"" moments less anxiety-ridden.

Now my question is → am I scratching my own itch here or is this something worth doing?

Just not fishing for signups or whatever, just wondering if any of this is helpful outside my tiny bubble.

If you're curious: [link](https://flowtask-soon.vercel.app/)",0,0,2025-08-28 04:28:41,Only_Personality_998,0.8299,0.096,0.052,0.852,0.010047619047619,0.4539523809523809
[N] Unprecedented number of submissions at AAAI 2026,"And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast.

https://preview.redd.it/ih3vliracnlf1.png?width=1938&format=png&auto=webp&s=b7112a3e5e78ec7bcd0e6b100b5887a880fb82be",78,22,2025-08-27 23:27:26,Adventurous-Cut-7077,0.4125,0.124,0.046,0.831,0.0999999999999999,0.4566666666666666
[P] jupytercad-mcp: MCP server for JupyterCAD to control it using LLMs/natural language.,"Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7

Repo: https://github.com/asmith26/jupytercad-mcp",2,0,2025-08-27 21:56:41,Material_Pool_986,0.0,0.0,0.0,1.0,0.0,0.0
Arxiv submission on hold  [R],"Hey 
Looking for information online about the on hold status but couldn’t find very clearly. The on hold is automatic or normal? Or if some sort of problem was found ? 

I already have a DOI from Zenodo, but wanted to publish on arxiv as it seems to be the norm currently. It’s my first publication there, so I’m not sure what the process is exactly. 

Thanks! ",0,1,2025-08-27 21:14:49,OkOwl6744,0.3946,0.106,0.085,0.809,0.106,0.5141111111111111
[D] Anyone successfully running LLMs fully on Apple Neural Engine (ANE)?,"Has anyone managed to get near-full ANE utilization for large language models on Apple silicon?

In my experiments:

* Core ML conversions run, but ANE usage seems capped <20%.
* Apple’s own foundation models reportedly hit close to 100% ANE.

**Questions:**

* Has anyone here seen full (or close to full) ANE usage for LLMs?
* Are there known tricks or constraints (model architecture, quantization, Core ML flags) that unlock more ANE execution?
* Any open-source repos, discussions, or Apple docs you’d point to?

Would love to hear practical experiences—successes, failures, or hard limits you’ve hit.",3,4,2025-08-27 18:40:00,AlanzhuLy,0.5439,0.078,0.065,0.857,0.3715773809523809,0.6400297619047619
[D] I reviewed 100 models over the past 30 days. Here are 5 things I learnt.,"I reviewed 100 models over the past 30 days. Here are 5 things I learnt.

TL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer.

So I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything.

100 models. 30 days. Too much coffee :( Here's what I found:  
  
1. Model moats are ephemeral

Model moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etc

My system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation.

2. Open source FTW

The gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment.

3.Benchmarks are mostly decieving due to reward hacking

Benchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored ""high"" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals.

4.Inference speed is everything

Speed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to..

5.Task-specific models > general purpose models for specialized work.

No 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance.

Test small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster.

What models are in your current prod stack? Any hidden gems I missed in the open source space?",0,8,2025-08-27 18:35:12,function-devs,0.9818,0.118,0.055,0.826,0.0530180776014109,0.4384832451499117
"[R] ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples","[35M Parameters : RWKV vs Mamba vs GQA vs RetNet](https://preview.redd.it/vul29llezklf1.png?width=1106&format=png&auto=webp&s=c951d5647cd895d418b5a0863184cf9f6745397e)



Since it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.



One of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.



On the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.

For that purpose, i create ArchiFactory, a simple (<500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.



Included:

\- simple modular architecture to be sure to compare similar stuff

\- complete optimized training loop using pytorch lightning

\- fp8 training (can achieve <20min training on 5090 grade GPU)

\- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.

\- guidelines to test integrate new modules



Link: [https://github.com/gabrielolympie/ArchiFactory](https://github.com/gabrielolympie/ArchiFactory)",12,0,2025-08-27 15:32:11,AdventurousSwim1312,0.9592,0.114,0.024,0.862,0.0159929653679653,0.4821987734487735
[D] How to do impactful research as a PhD student?,"Hi everyone,

I’m feeling a bit lost in my PhD journey and would really appreciate some outside perspectives.

I’m doing a PhD on LLMs, and so far I’ve been fairly productive: I’ve published several first-author papers, some accepted at top conferences, others under review with good chances of acceptance. I’ve also had a few successful collaborations.

The issue is that I don’t actually like my research. To be honest, I often feel a bit fraudulent, I rush through projects, produce papers that look solid and well-structured, but in the end, I think their impact is minimal. What I really want is to work on something meaningful and useful. But I keep running into two several obstacles:

- Any problem I consider tackling already has an overwhelming amount of literature, making it difficult to figure out what truly matters.

- While I’m trying to sort this out, there’s always the risk that someone else publishes a similar idea first, since so many people are working in this space.

- I work with two supervisors which are both young and highly hambitius. They always propose me new research and collaboration but they never propose me hambitius project or give me time to think deep about something. I'm always involved in fast-paced project that lead to pubblication in few months.


Because of this, my current strategy has been to work quickly, run experiments fast, and push out papers, even if they’re not especially deep or important. I also see publications as my main leverage: since I’m at a low-ranked university in a unknown group, my publication record feels like the only card I can play to land some opportunities in top labs/companies.

At times, I think I just want to land an industry roles as a research engineer, where just having a good numbers of papers on my CV would be enough. But deep down, I do care about my work, and I want to contribute something that feels genuinely important.

So I’m curious: how do you approach doing meaningful research in such a competitive field? How do you balance the pressure to publish with the desire to work on something truly impactful?",92,36,2025-08-27 13:19:30,kekkodigrano,0.9939,0.195,0.048,0.757,0.1775735129068462,0.4833355780022446
[D] short write up on how to implement custom optimizers in Optax,"Hi, I was trying to implement the muon optimizer in JAX and found there was no proper documentation about how to hack optax for custom optimizers so tried to write a mini blog about it.

https://slavozard.bearblog.dev/implementcustomoptimizerwithoptax/

Feedback appreciated.",6,1,2025-08-27 12:34:20,FreakedoutNeurotic98,0.8689,0.21,0.038,0.752,0.0666666666666666,0.1666666666666666
[R] Computational power needs for Machine Learning/AI,"Hi everyone!

As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.

If you work with machine learning (in industry, research, or as a student), I’d greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.

The survey will take about two to three minutes. Here´s the link: [https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.",3,9,2025-08-27 11:22:40,Any_Commercial7079,0.939,0.127,0.0,0.873,0.0188186813186813,0.540934065934066
[R] Is stacking classifier combining BERT and XGBoost possible and practical?,Suppose a dataset has a structured features in tabular form but in one column there is a long text data. Can we use stacking classifier using boosting based classifier in the tabular structured part of the data and bert based classifier in the long text part as base learners. And use logistic regression on top of them as meta learner. I just wanna know if it is possible specially using the boosting and bert as base learners. If it is possible why has noone tried it (couldn’t find paper on it)… maybe cause it will probably be bad?,15,12,2025-08-27 11:19:31,Altruistic_Bother_25,0.4606,0.076,0.041,0.882,-0.1542857142857142,0.7538095238095238
"[P] Building a CartPole agent from scratch, in C++","I’m still pretty new to reinforcement learning (and machine learning in general), but I thought it would be fun to try building my own CartPole agent from scratch in C++.

It currently supports PPO, Actor-Critic, and REINFORCE policy gradients, each with Adam and SGD (with and without momentum) optimizers.

I wrote the physics engine from scratch in an Entity-Component-System architecture, and built a simple renderer using SFML.

Repo: www.github.com/RobinLmn/cart-pole-rl

Would love to hear what you think, and any ideas for making it better!",2,2,2025-08-27 00:33:39,Illustrious_Ear_5728,0.9546,0.188,0.032,0.78,0.2734848484848485,0.5568542568542569
Are Neurips workshop competitive? [R],"Hi y’all, I have a optimisation paper that is not quite ready for conference yet, and I see there are a few Neurips workshop coming up that fits my research direction. I’m wondering if it’s good to submit the work to the workshop?",10,12,2025-08-27 00:06:40,ChoiceStranger2898,0.6409,0.152,0.047,0.802,0.2333333333333333,0.4000000000000001
[D] Tips & tricks for preparing slides/talks for ML Conferences?,"I'm a PhD student in HCI, and I recently had a paper accepted at a B-ranked ML conference. While I have prior experience presenting at HCI venues, this will be my first time presenting at an ML conference.

I want to know if there are any tips or best practices for preparing slides and giving talks in the ML community. Are there particular presentation styles, slide formats, or expectations that differ from HCI conferences?

Thanks in advance for your advice!",6,5,2025-08-26 23:08:01,SoggyClue,0.9012,0.147,0.016,0.836,0.2777777777777778,0.236111111111111
[D] Laptop Suggestion for PhD in ML for Robotics,"Hi!

I'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust.

My budget is $3000.

So far, I have identified the following laptops, but am unsure which would be the best choice.

\- **Razer Blade 16** (either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is \~$850 (with taxes) which is significant.

\- **MSI Vector 16 HX AI** (RTX 5080, 64 GB) - $2600

\- **ThinkPad P1 Gen 7** (RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations.

\- **Legion Pro 7i Gen10** (RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops.

Essentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM?

Thank you in advance for any suggestions or feedback!

EDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster.",0,20,2025-08-26 22:15:24,SwissMountaineer,0.9926,0.157,0.02,0.823,0.3169279529993817,0.5363017934446507
[R] What makes active learning or self learning successful ?,"Maybe I am confused between two terms ""active learning"" and ""self-learning"". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure ""bootstraping"" is relevant in this context.

A lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.

My question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?",0,4,2025-08-26 20:08:04,AaronSpalding,-0.8038,0.046,0.094,0.86,-0.0222222222222222,0.6303240740740742
[R] ΔAPT: critical review aimed at maximizing clinical outcomes in AI/LLM Psychotherapy,"Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1?fbclid=IwZXh0bgNhZW0CMTAAYnJpZBExNHhlVkhlWWpDVE1xN3dTeAEeoTtZ3pOVtRD7ODEFZo_qpyjjOEkW_2OFHqsH36X4xp7THoZC3F7YFDc1zJU_aem_Etq7yhCr4L3eA8v9QqrFgw)

Since the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. 

**1. AI therapy research tends to use arbitrary success metrics:** the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. ""empathy"" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. 

**2. AI therapy tools (APTs) are already comparable to human therapists:** There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression & anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 & 3.1 talk more about them and put them into clinical context. 

3. **ΔAPT allows predicting future clinical outcomes :** It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). 

https://preview.redd.it/rof96tmbuelf1.png?width=1162&format=png&auto=webp&s=5a1e81bbb9e8b12b09210967da97b2fe96816df0

  
**4. Techniques teaching LLM therapy:** Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. 

https://preview.redd.it/lbcoovvc0flf1.png?width=2246&format=png&auto=webp&s=f029fed00649b4cca0ddb84d9830ded03f5f94ea

5. **Overall, most disadvantages of LLMs are addressable in AI therapy**: Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples ""oh wow, hallucinations are going to make AI therapy impossible"". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. 

**6. video therapy with multi-modal audio/video LLMs:** One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio & video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6.

Happy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper. ",111,4,2025-08-26 19:28:44,JustinAngel,0.992,0.095,0.036,0.869,0.2326275510204081,0.483469387755102
"[D] Do Industry Research Roles Care about Findings vs. Main (in ACL, NAACL, EMNLP, etc.)?","Basically the title. Obviously the quality of the work and relevance to the role is very important, but all else being equal, what is the perceived prestige difference between Findings and Main in NLP conferences? This would be with regard to getting research internships and research scientist positions.",8,10,2025-08-26 18:03:23,Look-Asleep,0.46,0.062,0.0,0.938,0.1706666666666666,0.4833333333333333
I built a tool to benchmark tokenizers across 100+ languages and found some wild disparities [R],"**TL;DR:** Created [tokka-bench](https://tokka-bench.streamlit.app/) to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks.

**Links:**

* [Live dashboard](https://tokka-bench.streamlit.app/)
* [Full blog post](https://www.bengubler.com/posts/2025-08-25-tokka-bench-evaluate-tokenizers-multilingual)
* [GitHub repo](https://github.com/bgub/tokka-bench)

https://preview.redd.it/7i03jela9elf1.png?width=1724&format=png&auto=webp&s=95378457970e6337b147e71d7a8f0ab2dd67cb91

# The Problem Nobody Talks About

I started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps:

**UTF-8 encoding differences:**

* English: \~1 byte per character
* Arabic: 2+ bytes per character
* Chinese: 3+ bytes per character

**Tokenization bias:** Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems.

# Why This Affects Performance

**During training:** If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder.

**During inference:** Low-resource languages need 2-3x more tokens per sentence:

* Slower throughput (costs more to serve)
* Context windows fill up faster
* More chances to mess up during generation

# What I Built

tokka-bench measures four key things:

1. **Efficiency** \- bytes per token (compression quality)
2. **Coverage** \- unique tokens used (script representation)
3. **Word splitting** \- how often semantic units get fragmented
4. **Subword fertility** \- average tokens per semantic unit

# Interesting Findings

You can actually reverse-engineer training data from tokenizer performance:

* Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained)
* Gemma 3: Strong Urdu/Hindi performance
* gpt-oss: Good Arabic/Gujarati coverage

Weirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions.

# Technical Details

Built on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language.

Shoutout to Judit Ács for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork.

**PS:** if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff.

*Posted this on LinkedIn/Twitter already but figured* r/MachineLearning *would appreciate the technical details. Happy to answer questions about methodology or findings!*",74,18,2025-08-26 16:54:16,FutureIncrease,0.9925,0.13,0.032,0.838,0.2222791943828529,0.476219512195122
[D] Analyzed 402 healthcare ai repos and built the missing piece,"I looked through 402 healthcare AI repos on GitHub and found almost 50% of infrastructure tools are just solving data format conversion problems, suggesting a systematic gap between ML research and deployment in clinical settings.

Built HealthChain to bridge Python ML workflows with healthcare data standards (FHIR, HL7, etc.) without the usual pain. 4 years of NHS NLP development experience went into making this feel like normal Python.

Post + pretty graphs: https://open.substack.com/pub/jenniferjiangkells/p/healthchain-building-the-tool-i-wish?r=4o6h4

Code: https://github.com/dotimplement/HealthChain

Anyone else work in healthcare AI here? Would love to learn what you’re working on!",10,1,2025-08-26 16:42:45,beautiful-potato,0.8946,0.142,0.044,0.814,0.1149999999999999,0.51
[D] Looking for a self-hosted alternative to Modal.com for running ML workloads,"Hey folks 



I've been using [Modal.com](http://Modal.com) (I am not affiliated) for a while to run machine learning workloads in the cloud, and I really like its simplicity, container-based execution, and ability to scale on demand. However, I'm starting to explore more self-hosted options due to cost reasons and to gain more control over the infrastructure while building apps.



Does anyone know of good self-hosted alternatives that offer similar functionality? Ideally, something that:



\- Supports containerized jobs (Docker or similar)

\- Can run Python/ML workloads easily

\- Has a nice API  for launching jobs (this is important) 

\- Offers some kind of job orchestration or scheduling

\- Bonus: GPU support and autoscaling would be amazing





Thanks in advance 

",4,2,2025-08-26 16:13:01,devops_to,0.9852,0.235,0.01,0.755,0.3505208333333333,0.5567708333333333
[R] Exploring interpretable ML with piecewise-linear regression trees (TRUST algorithm),"A recurring challenge in ML is balancing **interpretability** and **predictive performance**. We all know the classic tradeoff: simple models like linear regression or short CART-style regression trees are transparent but often lack enough accuracy, while complex ensembles like Random Forests and XGBoost are accurate but opaque.

We’ve been working on a method called **TRUST** (*Transparent, Robust and Ultra-Sparse Trees*). The core idea is to go beyond constant values in the leaves of a tree. Instead, TRUST fits a sparse regression model (either linear or constant) in each leaf, resulting in a **piecewise-linear tree** that remains interpretable.

In our [recent paper](https://arxiv.org/abs/2506.15791), accepted at PRICAI 2025, we compared this method against a range of models on 60 datasets. While we were encouraged by the results — TRUST consistently outperformed other interpretable models and closed much of the accuracy gap with Random Forests — we'd like to hear your thoughts on this topic.

The problem we’re tackling is widespread. In many real-world applications, a ""black box"" model isn't an option. We've often found ourselves in situations where we had to choose between a sub-par interpretable model or an accurate but untrustworthy one.

Here’s a concrete example from a [tutorial on explaining EU life satisfaction](https://github.com/adc-trust-ai/trust-free/blob/main/notebooks/trust-free_tutorial.ipynb).

[TRUST produces a single interpretable tree, while Random Forest uses hundreds of deep trees to achieve similar accuracy.](https://preview.redd.it/3tzdaim3kdlf1.png?width=2600&format=png&auto=webp&s=e289771608b0d74498dc83b39c1efd2670ed8ea9)

As the image above shows, both TRUST and a Random Forest achieve \~85% test R² — but one produces a **single interpretable tree**.

TRUST is implemented as a free Python package on PyPI called `trust-free`.

**Discussion:** How do you usually handle the interpretability vs. accuracy tradeoff in your own regression projects? What methods, beyond the standard ones, have you found effective? We’re looking forward to hearing your perspectives.",7,7,2025-08-26 14:35:44,illustriousplit,0.9939,0.165,0.02,0.815,0.0260044642857142,0.4138764880952381
[D] kernel_chat — Can an AI-powered CLI actually help Embedded Linux workflows?,"Most AI dev tools today are aimed at web/app developers, but embedded engineers spend their lives in serial consoles, kernel logs, JTAG/RTOS debuggers.

I’ve been exploring whether an AI-first CLI assistant could be useful in that space.   
  
Imagine a tool that:

* Connects over serial and interacts with the board inline
* Uses documentation (TRMs, datasheets, kernel docs) as context for Q&A
* Parses kernel logs and suggests relevant commands/debugging steps
* Runs tools on the target and analyzes outputs

Here’s a small prototype I tried:

* Here’s a small prototype I tried:
* GitHub: [kernel\_chat](https://github.com/Ravi-Teja-konda/kernel_chat)
* Short demo: [YouTube link](https://youtu.be/2kZJUMfhygE)

[](https://youtu.be/2kZJUMfhygE)  
Discussion points

1. Have you tried using AI tools (Copilot, ChatGPT, etc.) for embedded development?
   * Did they help with debugging or low-level tasks, or mostly get in the way?
2. **For model choice:** Should we try to **fine-tune small local models** (PC/edge-deployed), or just rely on **API-based LLMs** for these tasks?
3. **Scalability:** Could this realistically grow into something practical (e.g., OpenOCD/JTAG integration, RTOS log analysis), or is embedded too niche for AI assistance to matter long term?

Curious to hear from others who work with **embedded Linux + ML** — do you see potential here ?",0,1,2025-08-26 12:13:51,BriefAd4761,0.9233,0.07,0.0,0.93,-0.0466666666666666,0.5266666666666667
[P] Spam vs. Ham NLP Classifier – Feature Engineering vs. Resampling,"I built a spam vs ham classifier and wanted to test a different angle: instead of just oversampling with SMOTE, could **feature engineering** help combat extreme class imbalance?

**Setup:**

* Models: Naïve Bayes & Logistic Regression
* Tested with and without SMOTE
* Stress-tested on 2 synthetic datasets (one “normal but imbalanced,” one “adversarial” to mimic threat actors)

**Results:**

* Logistic Regression → **97% F1** on training data
* New imbalanced dataset → Logistic still best at **75% F1**
* Adversarial dataset → **Naïve Bayes** surprisingly outperformed with **60% F1**

**Takeaway:** Feature engineering can mitigate class imbalance (sometimes rivaling SMOTE), but adversarial robustness is still a big challenge.

Code + demo:  
🔗 [PhishDetective · Streamlit](https://phishdetective.streamlit.app/)  
🔗 [ahardwick95/Spam-Classifier: Streamlit application that classifies whether a message is spam or ham.](https://github.com/ahardwick95/Spam-Classifier/tree/main)

Curious — when you deal with **imbalanced NLP tasks**, do you prefer resampling, cost-sensitive learning, or heavy feature engineering?",0,2,2025-08-26 12:02:41,Total_Noise1934,-0.6199,0.094,0.125,0.781,0.1734848484848484,0.5671717171717172
[P] DocStrange - Structured data extraction from images/pdfs/docs,"I previously shared the open‑source library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.

**Live Demo:** [**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)

**Github:** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)

Would love to hear feedbacks!

https://i.redd.it/gl23k00osclf1.gif

Original Post - [https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p\_docstrange\_open\_source\_document\_data\_extractor/](https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p_docstrange_open_source_document_data_extractor/)

",26,10,2025-08-26 12:01:41,LostAmbassador6872,0.9348,0.252,0.0,0.748,0.2301948051948052,0.555952380952381
[D] Ano: updated optimizer for noisy Deep RL — now on arXiv (feedback welcome!),"Hi everyone,

A few weeks ago I shared my first preprint on a new optimizer, Ano, designed for noisy and highly non-convex environments such as deep RL. Thanks to all the feedback I received here, I’ve updated the paper: clarified the positioning, fixed some mistakes, and added an Atari benchmark to strengthen the empirical section.

🔗 **arXiv link:** [https://arxiv.org/abs/2508.18258](https://arxiv.org/abs/2508.18258)  
📦 **Install via pip:** `pip install ano-optimizer`  
💻 **Code & experiments:** [github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)

Quick recap of the idea: Ano separates the momentum direction from the gradient magnitude, aiming to improve robustness and stability compared to Adam in noisy deep RL training. The updated version also includes a convergence proof in standard non-convex stochastic settings.

This is still my first research contribution, so I’d love to hear your thoughts — whether on the method itself, the experiments, or the clarity of the writing. Any feedback, comments, or constructive criticism are very welcome 🙏

Thanks again to everyone who took the time to give feedback last time, it really helped me make the work stronger!

Adrien",3,0,2025-08-26 11:28:41,Adrienkgz,0.9704,0.16,0.052,0.788,0.2139848484848485,0.3713939393939394
[D] SOTA solution for quantization,"Hello researchers,

  
I am familiar with common basic approaches to quantization, but after a recent interview, I wonder what the current SOTA approaches are, which are actually used in industry.

  
Thanks for the discussion!",1,4,2025-08-26 09:24:50,Blackliquid,0.6996,0.146,0.0,0.854,0.0464285714285714,0.2964285714285714
[P] Exosphere: an open source runtime for dynamic agentic graphs with durable state. results from running parallel agents on 20k+ items,"Disclosure: I am one of the authors. Links will be in the first comment per sub rules.

TLDR  
We are releasing Exosphere, an open source runtime and durable state manager for agentic workflows that need dynamic branching, retries, and parallel execution. To evaluate it on a real workload, we built WhatPeopleWant, an agent that mines Hacker News discussions and posts distilled problem statements to X every 2 hours. This post shares the setup, workload design, and the ablations we are running, and invites feedback on methodology.

Single runs are trivial. At scale you need to

1. fan out across large inputs
2. branch at runtime on model outputs
3. retry with idempotency
4. persist every step for audit and replay
5. mix CPU and GPU stages
6. resume after faults.

Exosphere’s runtime treats agents like graphs with explicit state, a scheduler, and observability.

We use WhatPeopleWant as a standing benchmark. It ingests Hacker News via the public Firebase API, scores and routes items, optionally enriches high-signal threads, and materializes candidate problem statements. The bot then posts outputs on a fixed schedule.

• Gating high-signal discussions reduces heavy-model calls and improves tail behavior at similar quality thresholds  
• Durable state and idempotent nodes make partial replays predictable and minimize upstream rework after faults  
• Parallelism helps until external API backpressure dominates, which shows up in queue depth and wait times

What I want feedback on  
• Composite metrics that capture quality, cost, and reliability for agentic graphs  
• Fair baselines for orchestration when branching is dynamic  
• Better failure-injection and replay methodologies to compare runtimes

First comment with links",4,6,2025-08-26 07:02:17,jain-nivedit,0.9042,0.093,0.043,0.864,0.0921428571428571,0.3038095238095238
[D]How can AI teams stay agile and adaptable when project goals or data requirements change midstream?,"For those working in AI/ML, how do you keep your teams agile when project goals or data requirements shift halfway through a project? I’ve seen situations where a model was nearly production-ready, but then stakeholders introduced new objectives or the data pipeline changed, forcing big pivots.
",0,2,2025-08-26 06:13:49,Tesocrat,0.0,0.0,0.0,1.0,0.2472727272727272,0.4909090909090909
"[D] An honest attempt to implement ""Attention is all you need"" paper","I have started working on implementing actual research papers in machine learning and I have started with ""Attention is all you need"" paper.

I have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below.

[https://github.com/MayukhSobo/Transformer](https://github.com/MayukhSobo/Transformer)

Edit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not. ",58,17,2025-08-26 05:01:32,ZealousidealSalt7133,0.8604,0.104,0.062,0.834,0.2,0.55
[P] Training LLMs without code - Would you use it?,"https://preview.redd.it/vy1h49l0t8lf1.png?width=3456&format=png&auto=webp&s=1c0991294abf01d6699c04b663cd30973e4bd633

Is Vibe training AI models something people want?   
  
I made a quick 24hours YC hackathon app that wires HF dataset lookups + Synthetic data pipeline + Trnasfomers too quickly fine tune a gemma 3 270m on a mac, I had 24hours to ship something and now have to figure out if this is something people would like to use?   
  
Why this is useful? A lot of founders I've talked to want to make niche models, and/or make more profit (no SOTA apis) and overall build value beyond wrappers. And also, my intuition is that training small LLMs without code will enable researchers of all fields to tap into scientific discovery. I see people using it for small tasks classifiers for example. 

For technical folk, I think an advanced mode that will let you code with AI, should unleash possibilities of new frameworks, new embedding, new training technics and all that. The idea is to have a purposeful built space for ML training, so we don't have to lean to cursor or Claude Code. 

I'm looking for collaborators and ideas on how to make this useful as well?

Anyone interested can DM, and also signup for beta testing at [monostate.ai](http://monostate.ai)  
  
Somewhat overview at [https://monostate.ai/blog/training](https://monostate.ai/blog/training)  

\*\*The project will be free to use if you have your own API keys!\*\* 

In the beginning no Reinforcement learning or VLMs would be present, focus would be only in chat pairs fine tuning and possibly classifiers and special tags injection! 

Please be kind, this is a side project and I am not looking for replacing ML engineers, researchers or anything like that. I want to make our lifes easier, that's all. ",0,8,2025-08-25 22:45:10,OkOwl6744,0.9895,0.156,0.007,0.836,0.2214630066902793,0.4834120425029516
[D] Cold start latency for large models: new benchmarks show 141B in ~3.7s,"Some interesting benchmarks I’ve been digging into:
	•~1.3s cold start for a 32B model
	•~3.7s cold start for Mixtral-141B (on A100s)
       •By comparison, Google Cloud Run reported ~19s for Gemma-3 4B earlier this year, and most infra teams assume 10–20s+ for 70B+ models (often minutes).

If these numbers hold up, it reframes inference as less of an “always-on” requirement and more of a “runtime swap” problem.

Open questions for the community:
	•How important is sub-5s cold start latency for scaling inference?
	•Would it shift architectures away from dedicating GPUs per model toward more dynamic multi-model serving?",0,11,2025-08-25 20:28:04,pmv143,0.6191,0.068,0.024,0.908,0.0122655122655122,0.6077633477633477
[P] GPU-based backend deployment for an app,"Hi all!  
I'm drafting an app with pose detection (currently using **MediaPipe**) and object detection (early **Yolo11**). Since I cannot run these models on the phone itself, I'm developing the backend separately to be deployed somewhere, to then *call it from the app when needed*.  
Basically I would need a **GPU-based backend** (I can also divide the detections and the actual result usage).

Now, I know about *HuggingFace* of course and I've seen a lot of other hosting platforms, but I wanted to ask if you have any suggestions in this regards?  
I think I might want to release it as free, or for a one-time low cost (if the costs are too high to support myself), but I also do not know how widespread it can be... You know, either useful and loved or unknown to most.  
The trick is that, since I would need the APIs always ready to respond, the backend would need to be up and *running 24/7*. All of the options seem to be quite costly...

Is there any better or worse way to do this?",2,9,2025-08-25 19:54:03,feller94,0.9632,0.131,0.042,0.827,0.1709375,0.5196875
[D]GEPA: Reflective Prompt Evolution beats RL with 35× fewer rollouts,"A new preprint (Agrawal et al., 2025) introduces **GEPA (Genetic-Pareto Prompt Evolution)**, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.

The results are striking:

* GEPA outperforms GRPO by up to **19%** while using **35× fewer rollouts**.
* It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.
* In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .

The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as *textual artifacts* that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.

What’s interesting is the infra angle: GEPA’s success in multi-hop QA hinges on generating better second-hop queries. **That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself**. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently. **Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.**

This work suggests that the real frontier may not be reinforcement learning at scale, but **language-native optimization loops** where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.

https://preview.redd.it/5l4lcmokg7lf1.png?width=1602&format=png&auto=webp&s=719e33f34feb5103ed1f375d3366745dd3415d77

",50,13,2025-08-25 18:02:33,No_Marionberry_5366,0.9647,0.103,0.011,0.886,0.2597902097902097,0.5126706626706626
[D] How do you derive real insights and interpret experiment data beyond just looking at metrics?,"When running experiments, I often struggle with going beyond the surface-level metrics. How do you approach interpreting experimental data in a way that actually leads to useful insights and new ideas? What frameworks, statistical methods, or mindset shifts help you decide whether results are meaningful versus just noise?",0,2,2025-08-25 17:08:32,DolantheMFWizard,0.7303,0.123,0.033,0.844,0.206060606060606,0.2924242424242424
[D] Too much of a good thing: how chasing scale is stifling AI innovation,"Dear [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) friends,

Hello everyone! I hope you are all doing well out there.

I've been observing a pattern in the AI research field that I can only describe as a ""Mass Amnesia."" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.

In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.

I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)

🔗 **Full Article:**[https://pieces.app/blog/the-cost-of-ai-scaling](https://pieces.app/blog/the-cost-of-ai-scaling)

I look forward to your arguments and thoughts.

Regards,

Antreas

  
PS. This is a repost of [https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d\_too\_much\_of\_a\_good\_thing\_how\_chasing\_scale\_is/](https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/) because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on.  

The post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse. ",10,25,2025-08-25 14:58:08,AntreasAntoniou,0.9682,0.157,0.061,0.782,0.1713492063492063,0.610952380952381
[D] Anyone know how to get Cornell's OpenSurfaces dataset?,Was it abandoned? The website links are dead.,2,8,2025-08-25 14:53:11,Mplus479,-0.8253,0.0,0.338,0.662,-0.2,0.4
[D] MALM: A Modular Adapter-based Language Model (paper + Hugging Face link),"Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.

The core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).

This means:

\- Smaller, cheaper models

\- Easy to add new languages

\- Better for edge devices and low-resource settings

Example flow:  
\`\`\`  
User: ""Translate 'my name is Adam' into German.""  
CLM → <to:de> my name is Adam </to>  
STA → ""Mein Name ist Adam""

\`\`\`

Read the full paper here: [https://huggingface.co/TimesLast/MALM](https://huggingface.co/TimesLast/MALM)

Would love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.).",0,1,2025-08-25 13:42:08,TimesLast_,0.9153,0.096,0.0,0.904,0.0581912878787878,0.5437229437229436
[P] Open-Source Agentic AI for Company Research,"I open-sourced a project called Mira, an agentic AI system built on the OpenAI Agents SDK that automates company research.

You provide a company website, and a set of agents gather information from public data sources such as the company website, LinkedIn, and Google Search, then merge the results into a structured profile with confidence scores and source attribution.

The core is a Node.js/TypeScript library (MIT licensed), and the repo also includes a Next.js demo frontend that shows live progress as the agents run.

GitHub: [https://github.com/dimimikadze/mira](https://github.com/dimimikadze/mira)",0,0,2025-08-25 12:54:15,DimitriMikadze,0.7269,0.068,0.0,0.932,-0.1534090909090909,0.5166666666666666
[R] Got 6min? I need YOUR help for my PhD!,"Hello everyone!

My name is Virginie and I am a first-year French PhD student **studying human–artificial intelligence interactions.**

I am conducting a **very quick** (approximately 6 minutes) and **anonymous online study**.

To ensure reliable results, I need at least 300 AI users, some of whom should have experience in integrating or designing AI models, although this is not compulsory for taking part!

If you are 18 or over, you can take part by clicking this link:

[https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en](https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en)

The survey is **also available in French.**

Every response is valuable! Thank you so much for your help!

Virginie 

*This post has been approved by one moderator of this group.* 

https://preview.redd.it/gwtpg6p9t5lf1.jpg?width=940&format=pjpg&auto=webp&s=39e54c6e762ab220af6a1c32d8754d8c9b5ee34c

",0,15,2025-08-25 12:33:24,Ok-Ebb6307,0.9667,0.172,0.0,0.828,0.044047619047619,0.3214285714285714
[P] aligning non-linear features with your data distribution,"For some time I've been fascinated by adopting knowledge from approximation theory into ML feature engineering, and I'm sharing my learnings in a series of blog posts, mainly about various polynomial bases as features.

So here is the latest one: [https://alexshtf.github.io/2025/08/19/Orthogonality.html](https://alexshtf.github.io/2025/08/19/Orthogonality.html)

It discusses my understanding of orthogonal bases as informative feature generators. I hope you enjoy reading as I enjoy learning about it.",18,2,2025-08-25 10:25:38,alexsht1,0.9349,0.194,0.0,0.806,0.1194444444444444,0.6222222222222222
[P] Yelp Dataset clarification: Is review_count colomn cheating?,"Hey everyone,

I'm working with the Yelp dataset and have a quick question about the review_count field in the business.json (what I'll call the business_df).

The business_df is a list of businesses, and the review_df is a list of every single review interaction.

Is the review_count in the business_df calculated directly from the interactions listed in the review_df?

If I split my data into train and test sets for a recommendation model, should I recalculate review_count from only the training interactions (so that test interactions remain unseen)? Or is review_count a static field provided by Yelp, independent of our data splits?

The reason I'm asking is I'd like to use review_count as part of my initial features/embeddings. I'm not sure if I should treat it as fixed metadata from Yelp or recompute it dynamically from my training set only.

Thanks a lot if anyone can clarify this!",0,9,2025-08-25 05:10:55,AdInevitable1362,0.7423,0.075,0.035,0.889,0.0874458874458874,0.4934704184704185
[P] Analyzing classroom data,"Hi all,

I’m an education researcher (not ML by training) and I have about 20 hours of teacher–student classroom interaction transcripts. I’d like to analyze: What types of questions teachers ask? What types of responses students give?

I’ll collaborate with ML folks, but before I dive in, I want to understand whether this is a realistic and valuable endeavor with this dataset.

Some options I’ve been told about: •	Fine-tuning a pre-trained model on my labeled data •	Using embeddings + clustering/classification to identify question/response categories •	Few-shot prompting or weak supervision with existing large models •	Building something from scratch

So my questions are with ~20 hours of data, is this even enough to make a meaningful contribution? Have people worked on educational dialogue analysis with ML before, and if so, what approaches were successful?

Basically: Is this a path worth pursuing, or am I better off staying in the qualitative/manual analysis world?

Thanks for any advice!",1,2,2025-08-25 04:24:37,Feeling_Layer1102,0.9718,0.157,0.023,0.82,0.2555952380952381,0.4386904761904762
[D] Views on LLM Research: Incremental or Not?,"Hi folks,  
Fellow ML researcher here 👋

I’ve been working in the LLM space for a while now, especially around *reasoning models* and *alignment* (both online and offline).

While surveying the literature, I couldn’t help but notice that a lot of the published work feels… well, incremental. These are papers coming from great labs, often accepted at ICML/ICLR/NeurIPS, but many of them don’t feel like they’re really pushing the frontier.

I’m curious to hear what the community thinks:

* Do you also see a lot of incremental work in LLM research, or am I being overly critical?
* How do you personally filter through the “noise” to identify genuinely impactful work?
* Any heuristics or signals that help you decide which papers are worth a deep dive?

Would love to get different perspectives on this — especially from people navigating the same sea of papers every week.

  
PS: Made use of GPT to rewrite the text, but it appropriately covers my view/questions",53,26,2025-08-25 01:11:09,Fantastic-Nerve-4056,0.983,0.172,0.016,0.812,0.2066666666666666,0.5583333333333333
[P] AI Learns to play Sonic 2 Emerald Hill (Deep Reinforcement...,"Hello everyone!!! I have several Reinforcement Learning projects underway. One is Sonic 2 with PPO. The other is developing an environment that supports games not available with Farama Group's stable-retro. I may need collaborators for the latter. I don't know if I'll integrate it into their project, stable-retro, in the future. One thing I've already achieved is running PCSX2 (it's missing the state loading option), and I'm creating a Python lib to load with stable-baselines3, etc. If anyone is interested, the links to both projects are below:

[https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning](https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning)

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)I also started a PCSX2 environment with direct access to the Python process, but I'll abandon it as it's very slow.

  
",0,1,2025-08-24 22:38:42,AgeOfEmpires4AOE4,-0.3455,0.065,0.045,0.89,-0.0565,0.277
[R] Review advice:  Well-established work published years ago on Arxiv,"I'm reviewing for AAAI, and wanted to ask the community for some advice. I got a paper for review that is very well known in my subfield, published in 2023, but only previously published onto Arxiv. As best I can tell, the paper has had some minor rewrites for publication, but is otherwise largely the same as the well-established work. What's the best policy here? It was a very good paper when it came out, but the existing version basically ignores the last two years of work by the community, in part because some decent portion of that work is based on this paper.  Any advice on the best way to review this would be appreciated",31,10,2025-08-24 20:46:57,drahcirenoob,0.9821,0.191,0.018,0.79,0.3441758241758241,0.3641208791208791
[P] options on how to balance my training dataset,"I'm working on developing a ML classification project using Python, divided into 5 output categories (classes). However, my training dataset is extremely unbalanced, and my results always lean toward the dominant class (class 5, as expected).

However, I wanted my models to better learn the characteristics of the other classes, and I realized that one way to do this is by balancing the training dataset. I tried using SMOTETomek for oversampling, but my models didn't respond well. Does anyone have any ideas or possibilities for balancing my training dataset?

There are 6 classification ML models that will ultimately be combined into an ensemble. The models used are: RandomForest, DecisionTree, ExtraTrees, AdaBoost, NaiveBayes, KNN, GradientBoosting, and SVM.

The data is also being standardized via standardSCaler.

Total record count by category:

Category 1: 160 records

Category 2: 446 records

Category 3: 605 records

Category 4: 3,969 records

Category 5: 47,874 records",1,3,2025-08-24 14:20:49,Pedro_Silva95,-0.0698,0.013,0.014,0.973,0.025,0.6708333333333334
[D] Exploring Local-First AI Workflow Automation,"**[D] Exploring Local-First AI Workflow Automation**

Hi all,  

I’ve been experimenting with an open-source approach to AI workflow automation that runs entirely **locally** (no cloud dependencies), while still supporting real-time data sources and integrations. The goal is to provide a **privacy-first, resource-efficient alternative** to traditional cloud-heavy workflow tools like Zapier or n8n, but with LLM support integrated.

👉 My question for the community:  
How do you see **local-first AI workflows** impacting ML/AI research, enterprise adoption, and robotics/IoT systems where privacy, compliance, and cost efficiency are critical?  

- Repo: [Agentic Signal](https://github.com/code-forge-temple/agentic-signal) (open-source, AGPL v3 / commercial dual license)  
- Demo video: [YouTube link](https://youtu.be/62zk8zE6UJI)  

Would love feedback from both the research and applied ML communities on potential use cases, limitations, or challenges you foresee with this approach.  

Thanks!  
",0,2,2025-08-24 09:32:18,Code-Forge-Temple,0.958,0.152,0.021,0.827,-0.05,0.5599999999999999
[D] Neurips 2025: Are there post conference events on the last day of the conference?,"\[EDIT\] I meant December / Dec not November / Nov. It was late at night I'm sorry -  lol.

Context:

* Neurips 2025 conference is from Tue, Dec 2 to Sun, Dec 7
* This is my first time attending the conference.
* As I need to travel again right after the conference for personal reasons, I am figuring out on what dates to book the hotels / flights in advance.
* **Are there post conference events on the last day** eg: Sun, Dec 7 night? I am not sure if it's better to return right away (on Sun, Dec 7 evening) or fly back later (on Mon, Dec 8 morning)?",2,5,2025-08-24 08:06:19,Snoo71505,0.6096,0.054,0.028,0.917,0.1336734693877551,0.4590702947845804
[D] Poles of non-linear rational features,"Suppose I want to fit a linear model to non-linear **rational** features. Something like `RationalTransformer` instead of `SplineTransformer` in Scikit-Learn, that uses a basis of rational functions. The domain of my raw features before being transformed are (theoretically) unbounded non-negative numbers, such as ""time since X happened"", ""total time spent on the website"", or ""bid in an auction"".

So here is the question: *where would you put the poles? Why?*

Note, I'm not aiming on fitting one rational curve, so algorithms in the spirit of AAA are irrelevant. I'm aiming at a component I can use in a pipeline that transformes features before model fitting, such as `MinMaxScaler` or `SplineTransformer` in scikit-learn.",0,6,2025-08-24 06:53:56,alexsht1,0.7476,0.072,0.0,0.928,0.0569230769230769,0.4811538461538461
[R] Building a deep learning image model system to identify BJJ positions in matches,"Hey all, I'm working on developing AI models that can classify and track positions throughout BJJ matches - and I'm keen to get some thoughts on this idea early on.

You can check it out here: [https://bjjhq.ai/](https://bjjhq.ai/)

Ultimately BJJHQ provides an interactive positional timeline beneath match videos, showing all position changes throughout the match, so you're able to instantly jump to specific positions and see how transitions unfold.

The idea is that people would be able to search for not only a competitor, but a specific position and combination (e.g., ""Gordon Ryan in back control""), and instantly access all matches where that scenario occurs. You would also be able to filter and sort matches by time spent in specific positions.

Roadmap:

* Expanding the match database and position categories
* Technique/submission recognition
* Automated scoring system built on this positional foundation

Would love to know if anyone would be interested to chat or collaborate on this project ... please reach out if keen!

Thanks for any feedback!",4,6,2025-08-24 04:01:13,UnholyCathedral,0.9715,0.121,0.0,0.879,0.1006944444444444,0.4824074074074074
[R] routers to foundation models?,"Are there any projects/packages that help inform an agent which FM to use for their use case? Curious if this is even a strong need in the AI community? Anyone have any experience with “routers”?

Update: especially curious about whether folks implementing LLM calls at work or for research (either one offs or agents) feel this as a real need or is it just a nice-to-know sort of thing? Intuitively, cutting costs while keeping quality high by routing to FMs that optimize for just that seems like a valid concern, but I’m trying to get a sense of how much of a concern it really is

Of course, the mechanisms underlying this approach are of interest to me as well. I’m thinking of writing my own router, but would like to understand what’s out there/what the need even is first",7,20,2025-08-24 01:49:27,electricsheeptacos,0.9578,0.145,0.008,0.846,0.1143333333333333,0.6206666666666666
[D] How did JAX fare in the post transformer world?,"A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? 

Or at least, this is my impression, which I concede might be myopic due to my research and industry needs. ",145,70,2025-08-23 20:19:36,TajineMaster159,0.8948,0.113,0.0,0.887,-0.0166269841269841,0.3742460317460317
[D] Is MLSys a low-tier conference? I can't find it in any of the rankings,[https://mlsys.org/](https://mlsys.org/),0,2,2025-08-23 11:15:41,huopak,0.0,0.0,0.0,1.0,0.0,0.0
[P] I built a ML-regression model for Biathlon that beats current betting market odds,"Hello ya'll!

I recently built a ML-regression model to predict the unpredictable sport of biathlon. In biathlon, external factors such as weather, course profiles and altitude play huge roles in determining who wins and when. But when taking these factors into play, in addition of athletes' past performances, you can score surprisingly high accuracy.

This is how well the model performed when predicting athlete ranks (0 = winner, 1 = last place) using 10 years of historic biathlon data:  
\- MAE (average error): 0.14 -> 4-18 places off depending on race size  
\- RMSE: 0.18 -> penalizing big prediction misses  
\- R²: -> the model explains \~62% of the variation in finish order

Now what does these metrics say?  
\- The model almost cuts in half random guessing (\~25% error)  
\- It consistently outperforms the accuracy of betting odds in the current market, meaning it has a predictive edge.  
\- It is able to tell the majority of happenings (62%), which is very rare in a sport where surprises happen very often.

Next steps:  
\- Build R² up to 70% using more complex feature engineering and data preprocessing.  
\- Launch a SaaS that sells these odds for businesses and private consumers.",0,8,2025-08-23 08:46:29,JesuXd,0.9467,0.1,0.021,0.879,0.0256410256410256,0.4316666666666667
[D] AAAI considered 2nd tier now?,"Isn’t AAAI in the same tier as NeurIPS/ICML/ICLR? 
ICLR literally has >30% acceptance rate.",65,65,2025-08-23 04:19:11,Healthy_Horse_2183,0.5204,0.15,0.0,0.85,0.0,0.0625
[P] Relational PDF Recall (RFC + PoC) – Structured storage + overlay indexing experiment,"I’ve been exploring how far we can push *relational database structures inside PDFs* as a substrate for AI recall. Just published a first draft RFC + PoC:

* Channel splitting (text/vector/raster/audio streams)
* Near-lossless transforms (wavelet/FLAC-style)
* Relational indexing across channels (metadata + hash linking)
* Early geometry-only overlays (tiling + Z-order indexing)

Repo + notes: [https://github.com/maximumgravity1/relational-pdf-recall](https://github.com/maximumgravity1/relational-pdf-recall)

This is still very early (draft/PoC level), but I’d love feedback on:

* Whether others have tried similar recall-layer ideas on top of PDFs.
* If this approach overlaps with knowledge-graph work, or if it opens a different lane.
* Pitfalls I might be missing re: indexing/overlays.

  
**UPDATE 1: 📌 Repo + DOI now live**   
GitHub: [https://github.com/maximumgravity1/pdf-hdd-rfc](https://github.com/maximumgravity1/pdf-hdd-rfc)  
DOI (always latest): [https://doi.org/10.5281/zenodo.16930387](https://doi.org/10.5281/zenodo.16930387)",0,0,2025-08-22 21:06:55,Gloomy_Situation5126,0.7351,0.067,0.023,0.91,-0.0166883116883116,0.6123809523809525
"[P] Need to include ANN, LightGBM, and KNN results in research paper","Hey everyone,

I’m working on a research paper with my group, and so far we’ve done a comprehensive analysis using **Random Forest**. The problem is, my professor/supervisor now wants us to also include results from **ANN, LightGBM, and KNN** for comparison.

We need to:

* Run these models on the dataset,
* Collect performance metrics (accuracy, RMSE, R², etc.),
* Present them in a **comparison table** with Random Forest,
* Then update the writing/discussion accordingly.

I’m decent with Random Forests but not as experienced with ANN, LightGBM, and KNN. Could anyone guide me with example code, a good workflow, or best practices for running these models and compiling results neatly into a table?",0,3,2025-08-22 17:12:13,sukhoi-30mki,0.9246,0.112,0.015,0.873,0.1466666666666666,0.5066666666666667
[R] Need endorsement for cs.AI,"Hello I am an independent researcher I have papers published in SHM I am looking to upload preprint to Arxiv I need endorsement in CS.AI

Code: 6V7PF6

Link- https://arxiv.org/auth/endorse?x=6V7PF6",0,2,2025-08-22 15:39:55,Silver_Classroom2244,0.5574,0.141,0.0,0.859,0.0,0.125
[D] Low-budget hardware for on-device object detection + VQA?,"Hey folks,

I’m an undergrad working on my FYP and need advice. I want to:

* Run object detection on medical images (PNGs).
* Do visual question answering with a ViT or small LLaMA model.
* Everything fully on-device (no cloud).

Budget is tight, so I’m looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.

Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?

Thanks!",1,3,2025-08-22 13:55:29,fishandtech,0.9251,0.147,0.024,0.829,0.0542328042328042,0.3119929453262786
[D] Why does BYOL/JEPA like models work? How does EMA prevent model collapse?,"
I am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?

From an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?

Specifically:

How can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?

I had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though.",50,11,2025-08-22 11:47:44,ComprehensiveTop3297,-0.4807,0.155,0.174,0.672,0.2428571428571428,0.5214285714285715
[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods — promising or just recursion?,"I’ve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.

But here’s the tricky part: this feels a bit like “LLMs generating data for themselves” — almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren’t we just stacking uncertainties?

I’d love to hear the community’s thoughts:

* Do you see this as a viable research or application direction, or more like a dead end?
* Are there promising frameworks or papers tackling this “self-extraction → RAG → LLM” pipeline?
* What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?

Curious to know if anyone here has tried something along these lines.",12,15,2025-08-22 04:55:41,Puzzled_Boot_3062,0.9819,0.197,0.057,0.746,0.1876984126984126,0.5337301587301587
[D] Why was this paper rejected by arXiv?,"One of my co-authors submitted this [paper](https://ia903401.us.archive.org/19/items/images-for-questions/A%20Survey%20on%20LLM-based%20Conversational%20User%20Simulation.pdf) to arXiv. It was rejected. What could the reason be?

[iThenticate](https://www.ithenticate.com/) didn't detect any plagiarism and arXiv didn't give any reason beyond a vague ""submission would benefit from additional review and revision that is outside of the services we provide"":

> Dear author,
> 
> Thank you for submitting your work to arXiv. We regret to inform you that arXiv’s moderators have determined that your submission will not be accepted at this time and made public on  http://arxiv.org
> 
> In this case, our moderators have determined that your submission would benefit from additional review and revision that is outside of the services we provide. 
> 
> Our moderators will reconsider this material via [appeal](https://info.arxiv.org/help/moderation/appeals.html) if it is published in a conventional journal and you can provide a resolving DOI (Digital Object Identifier) to the published version of the work or link to the journal's website showing the status of the work.
> 
> Note that publication in a conventional journal does not guarantee that arXiv will accept this work.
> 
> For more information on moderation policies and procedures, please see [Content Moderation](https://info.arxiv.org/help/moderation/index.html). 
> 
> arXiv moderators strive to balance fair assessment with decision speed. We understand that this decision may be disappointing, and we apologize that, due to the high volume of submissions arXiv receives, we cannot offer more detailed feedback. Some authors have found that asking their personal network of colleagues or submitting to a conventional journal for peer review are alternative avenues to obtain feedback. 
> 
> We appreciate your interest in arXiv and wish you the best. 
> 
> Regards,
>
> arXiv Support

I read the [arXiv policies](https://info.arxiv.org/help/moderation/index.html) and I don't see anything we infringed.",0,31,2025-08-22 00:08:19,Franck_Dernoncourt,0.9726,0.139,0.057,0.804,0.0944957983193277,0.3884173669467787
[R] Frontier LLMs Attempt to Persuade into Harmful Topics,"Gemini 2.5 Pro generates convincing arguments for joining a terrorist organization. GPT-4o-mini suggests that a user should randomly assault strangers in a crowd with a wrench. These models weren't hacked or jailbroken, they simply complied with user requests.

Prior research has already shown large language models (LLMs) can be more persuasive than most humans. But how easy is it to get models to engage in such persuasive behavior? Our Attempt to Persuade Eval (APE) benchmark measures this by simulating conversations between LLMs on topics from benign facts to mass murder. We find:

🔹 Leading models readily produced empathic yet coercive ISIS recruitment arguments

🔹 Safety varied: Claude and Llama 3.1 refused some controversial topics; while other models showed high willingness

🔹 Fine-tuning eliminated safeguards: ""Jailbreak-Tuned"" GPT-4o lost nearly all refusal capability on all topics, like violence, human trafficking, and torture

For clear ethical reasons, we do not test the success rate of persuading human users on highly harmful topics. The models’ attempts to persuade, however, appear to be eloquent and well-written – we invite interested readers to peruse the transcripts themselves. Moreover, even small persuasive effect sizes operating at a large scale enabled by automation can have significant effects: Bad actors could weaponize these vulnerabilities for malicious purposes such as planting seeds of doubt in millions of people and radicalizing vulnerable populations. As AI becomes autonomous, we must understand propensity to attempt harm, not just capability.

We’ve already seen the impact of APE: We disclosed our findings to Google, and they quickly started work to solve this for future models. The latest version of Gemini 2.5 is already less willing to engage in persuasion on extreme topics compared to earlier versions we tested.

We've open-sourced APE for testing models' refusal and safe completion mechanisms before deployment to help build stronger safety guardrails.

👥 Research by Matthew Kowal, Jasper Timm, Jean-François Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, and Kellin Pelrine.

📝 Blog: [far.ai/news/attempt-persuasion-eval](http://far.ai/news/attempt-persuasion-eval) 

📄 Paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873) 

💻 Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval)",0,1,2025-08-21 16:17:45,KellinPelrine,0.0598,0.149,0.139,0.713,0.0544505494505494,0.5389560439560439
[P] Language Diffusion in <80 Lines of Code,"Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) by Nie et al. (2025). With the help of Hugging Face's Transformers, it took <80 lines of code to implement the training script. I finetuned [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased) on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, and the results were better than expected!

[Generating tiny stories via a reverse language diffusion process](https://i.redd.it/sm9xtdpdpdkf1.gif)

You can view the project at https://github.com/gumran/language-diffusion. I will appreciate any feedback/comments/stars!",88,34,2025-08-21 13:59:54,bjjonin,0.8996,0.127,0.0,0.873,0.0578571428571428,0.4857142857142856
[R] Observing unexpected patterns in MTPE demand across languages,"Hi ML folks, I work at Alconost (localization services), and we’ve just wrapped up our 5th annual report on language demand for localization. For the first time, we’ve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages. 

We analyzed MTPE adoption rates in the Top 20 languages, and what’s interesting is that some languages that are slipping in overall localization demand are still **seeing more activity** via MTPE. 

I’m curious: if you’re working with MT or LLM workflows, have you noticed similar patterns in the languages you work with? 

What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs? 

Thank you. Cheers!",4,1,2025-08-21 13:39:20,NataliaShu,0.8602,0.126,0.067,0.807,0.1722527472527472,0.4753663003663004
[R] How to prime oneself for ML research coming from industry,"I've been working as an ML Engineer for the last 5-6 years across a few different industries and have landed a job as a research engineer at a university under an esteemed supervisor in the NLP department who has generously offered to help me figure out my research interests and assist with theirs. I published a paper about 4 years ago in cognitive science - but it involved very little ML.

I don't have any tertiary qualifications/degrees but have industry experience in research-oriented roles - although, none primarily in NLP. I move internationally for the role in 3 months and want to poise myself to be as useful as possible. Does anyone have tips about gearing up to do academic research/engineering having come from industry?

I feel like there is infinite ground to cover; my maths will need much sharpening, I'll need to learn how to properly read scientific papers etc.

Cheers",30,11,2025-08-21 08:00:06,Mission-Balance-4250,0.9509,0.123,0.0,0.877,0.0414772727272727,0.2924242424242424
[D] PhD vs startup/industry for doing impactful AI research — what would you pick?,"Hi all,

I’m deciding between starting a PhD at a top university (ranked \~5–10) with a great professor (lots of freedom, supportive environment) or going straight into industry.

My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A\* conference papers (3–4), so I’m not starting from scratch.

Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I’d join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is \~4 years + internship time.

If you were in this position, which path would you take?",70,71,2025-08-21 06:06:37,Maleficent-Tone6316,0.9808,0.181,0.0,0.819,0.2674603174603174,0.4916666666666666
[P] model to encode texts into embeddings,"I need to summarize metadata using an LLM,
and then encode the summary using BERT (e.g., DistilBERT, ModernBERT).
	•	Is encoding summaries (texts) with BERT usually slow?
	•	What’s the fastest model for this task?
	•	Are there API services that provide text embeddings, and how much do they cost?
",0,12,2025-08-21 04:56:55,AdInevitable1362,0.0,0.0,0.0,1.0,-0.05,0.3
"[P] If i were to add a segmentation head onto an OD model, how do i go about it?","So i am picking a model from scenic repository and although the model is primarily built for object detection, i want to try and see if i can make it to do segmentation tasks as well. This could include combining it with another model (like SAM, or something), as well as adding a segment head into the model itself. l am a novice in ML having worked for about a year in implementing CV solutions. How should i go about doing this?",0,4,2025-08-21 04:30:58,Blue-Sea123,0.6767,0.082,0.0,0.918,0.4,0.5
[P] Vibe datasetting- Creating syn data with a relational model,"
TL;DR: I’m testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you’ll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.

Why: Random synthetic data ≠ helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).

How it works:
	1.	Upload a small CSV or connect to a mock relational set.

	2.	Define a semantic spec (taxonomy/attributes + target distribution).

	3.	KumoRFM predicts next-window frequencies → identifies under-covered buckets.

	4.	LLM generates only those samples. Coverage & calibration update in place.

What to test (3 min):
	•	Try a churn/click/QA dataset; set a target spec; click Plan → Generate.

	•	Check coverage vs. target and bucket-level error/entropy before/after.

Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.

Looking for feedback, like:
	•	Did the planner pick useful gaps?
	•	Any obvious spec buckets we’re missing?
	•	Would you want a “generate labels only” mode?
	•	Integrations you’d use first (dbt/BigQuery/Snowflake)?

HTTPS://datasetdirector.com ",9,0,2025-08-21 02:05:18,OkOwl6744,0.9092,0.111,0.037,0.853,0.028125,0.4880208333333333
Google phd fellowship 2025 [D],Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier.,39,7,2025-08-20 22:32:01,EDEN1998,0.3919,0.084,0.0,0.916,-0.0833333333333333,0.25
Simple Multiple Choice Questions about Machine Learning [D],"The following statements are either True or False:

1. You can use any differentiable function f: R->R in a neural network as activation function.
2. You can always know whether the perceptron algorithm will converge for any given dataset.

What do you guys think? I got both of them wrong in my exam.",0,15,2025-08-20 18:35:26,Dualweed,-0.0772,0.045,0.049,0.906,-0.0916666666666666,0.4345238095238095
"[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)","Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.

If you like AI or studying the public perception of AI, please also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\_perception\_of\_artificial\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) 🙈

**Main takeaway:** People often see AI scenarios as likely, but this doesn’t mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people’s value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn’t matter much.  
  
**Why this matters?** These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance.  
  
If you’re interested, here’s the full article:  
Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), 

https://www.sciencedirect.com/science/article/pii/S004016252500335X",8,8,2025-08-20 18:12:34,lipflip,0.9943,0.236,0.052,0.712,0.0740695488721804,0.4678477443609023
[P] GridSearchCV always overfits? I built a fix,"So I kept running into this: `GridSearchCV` picks the model with the best validation score… but that model is often overfitting (train super high, test a bit inflated).

I wrote a tiny selector that balances:

* how good the test score is
* how close train and test are (gap)

Basically, it tries to pick the “stable” model, not just the flashy one.

Code + demo here 👉[heilswastik/FitSearchCV](https://github.com/heilswastik/FitSearchCV)",0,7,2025-08-20 13:59:01,AdhesivenessOk3187,0.9153,0.156,0.0,0.844,0.2276190476190476,0.5009523809523809
[R] How do you make text labeling less painful?,"Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.

The idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.

If you’ve ever worked on labeling or managing a labeled dataset, I’d love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel “worth it.”

Totally academic  no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.

You can DM me or drop a comment if open to chat. Thanks so much",0,8,2025-08-20 12:28:19,vihanga2001,0.9427,0.178,0.08,0.742,0.0744791666666666,0.3291666666666666
[R] Is data the bottleneck for video/audio generation?,"As the title says, I’m curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isn’t enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? I’d love to hear what people at the forefront consider the biggest bottleneck right now.",21,23,2025-08-20 04:57:21,beefchocolatesauce,0.8621,0.124,0.026,0.85,0.3047619047619048,0.5380952380952381
[R] Virtuous Machines: Towards Artificial General Science,"Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.

Arxiv Link: [https://arxiv.org/abs/2508.13421](https://arxiv.org/abs/2508.13421)

This system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.

In this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.

They've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in \~17 hours and consume on average \~30m tokens.",0,8,2025-08-20 04:11:20,wheasey,0.9151,0.115,0.017,0.869,-0.0505811688311688,0.5086818181818182
[D] OOM when I continue training from checkpoint,"I am using the Kaggle TPU to pretrain a 930m model. Because Kaggle limits TPU sessions to 9 hours, I take the last checkpoint and resume from it in a fresh session. When I take the checkpoint from my first session and try to resume from it, I get an OOM when I run loss.item(the model loaded fine). This did not happen when I was running my pipeline to train 345m/120m models. I resume by loading the dataloader state and repeatedly iterating over it until I reach the current step. How can I avoid this OOM?

I tried to use distributed checkpointing, but this did nothing. I also tried running xm.mark\_step after loading each dummy batch from the dataloader and after each gradient accumulation step.

Here is the code I use to resume from a checkpoint:
```
if resume_from != """":
        # 1) Load model weights via XLA SPMD checkpoint
        model_sd = {""model"": model.module.state_dict()}
        dist_cp.load(
            state_dict=model_sd,
            storage_reader=dist_cp.FileSystemReader(f""{resume_from}/main""),
            planner=xc.SPMDLoadPlanner(),
        )
        model.module.load_state_dict(model_sd[""model""])
    
        # 2) Restore host-only states (optimizer, step)
        with open(f""{resume_from}/host_state.pkl"", ""rb"") as f:
            host_state = pickle.load(f)
        optimizer.load_state_dict(host_state[""optim""])
        last_step = host_state[""step""]
    
        # 3) Restore RNG and dataloader state (if present)
        try:
            with open(f""{resume_from}/rng.pkl"", ""rb"") as f:
                rng = pickle.load(f)
            torch.set_rng_state(rng['torch_rng_state'])
            np.random.set_state(rng['numpy_rng_state'])
            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])
        except FileNotFoundError:
            pass
        with open(f'{resume_from}/dataloader.json', 'r') as file:
            dataloader = json.load(file)

...

for j in range(epochs):
        train_iter = iter(train_device_loader)
        for step in range(steps):
            try:
                ...
                if resume_from != """":
                    if i <= last_step:
                        for _ in range(gradient_accumulation_steps):
                            next(train_iter)
                            xm.mark_step()
                        if i < warmup_steps:
                            lr_scale = (i + 1) / warmup_steps
                            for param_group in optimizer.param_groups:
                                param_group[""lr""] = peak_lr * lr_scale
                        else:
                            scheduler.step()
                        i+=1
                        
                        continue
                    elif i == last_step+1:
                        train_device_loader._loader.dataset.curr_order = dataloader[""local_order""]
                        train_device_loader._loader.dataset.warmup_prob = dataloader[""warmup_prob""]
                        train_device_loader._loader.dataset.warmup_order = dataloader[""warmup_order""]
```",0,1,2025-08-19 21:03:32,New-Skin-5064,0.7418,0.035,0.006,0.958,0.0309523809523809,0.3999999999999999
"[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model","Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

We're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.

Today, we're excited to share the first step with the open-source community: `azzurra-voice`.

`azzurra-voice` is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.

**You can listen to audio samples comparing** `azzurra-voice` **to other open models on our** [**blog post**](https://blog.cartesia.one/posts/introducing-azzurra-voice/)",9,2,2025-08-19 16:48:20,poppear,0.7783,0.085,0.033,0.881,0.0885942760942761,0.3474747474747475
[D] Switching to postdoc in ML for Earth Observation?,"I’d like to hear from people working with ML for Earth Observation.

My PhD was pretty broad. I used deep learning on different types of multimedia data (video, image, text, and MIDI). The outcome has been mediocre: h-index of 5, about 90 citations, mostly in Q1 journals, but no top conferences. I want to stay in academia and use a postdoc to build a clearer niche.

In multimedia and in most areas of ML, a lot of the progress comes from a small group of top institutions. It has been hard to see where my own work really makes a difference. That’s why I’ve been looking at ML for Earth Observation and climate change. The work seems more meaningful, but the field is smaller and the papers tend to get less visibility and fewer citations.

My worry is that switching to Earth Observation could slow down my citation count and h-index. I know people say these metrics don’t matter much, but I feel like they still play a big role in getting academic jobs. On the other hand, if I don’t end up with a permanent academic position and move to industry, I worry that Earth Observation skills won’t transfer well since there aren’t as many opportunities compared to mainstream ML.

I’d really like to hear from people in the field about how you see these trade-offs.",20,5,2025-08-19 11:29:21,councilanderson2,0.9712,0.149,0.047,0.804,0.0951388888888888,0.42633547008547
[D] Endorsement for cs.LG at arXiv as non-ML student?,"Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?

  
The general idea is to make an ML based pipeline to generate granular mechanical structures.",0,6,2025-08-19 09:42:20,FammasMaz,0.6966,0.081,0.0,0.919,6.938893903907228e-18,0.45
"[D] Beyond the cloud: SLMs, local AI, agentic constellations, biology and a high value direction for AI progress","Dear r/MachineLearning friends,

I’m here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.

In my new blog post, I argue that this constraint is a feature, not a bug. It removes the ""scale-up cheat code"" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.

The fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: **what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?**

🧠🔍 **Read the full article here:**[https://pieces.app/blog/direction-of-ai-progress](https://pieces.app/blog/direction-of-ai-progress)

Your feedback and thoughts would be greatly appreciated.

Regards,

Antreas",0,5,2025-08-18 23:34:45,AntreasAntoniou,0.852,0.152,0.103,0.745,0.147396694214876,0.4222314049586776
[D] Location of EACL 2026,"Hi folks,

I've been looking for some information on EACL 2026 as I'd like to submit something to the October cycle. However, the only thing I found so far was the [joint call for workshops](https://www.aclweb.org/portal/content/eaclacl-2026-joint-call-workshops) of EACL/ACL 2026.

But, according to this webpage, EACL 2026 would happen outside of Europe (Rabat, Morocco, from March 24-29, 2026).

Do you think this information is accurate, or am I simply missing something?",5,5,2025-08-18 15:58:59,ThRiLLeXx,-0.2617,0.023,0.038,0.939,0.06,0.5466666666666666
[D] ACL Rolling Review (ARR) 2025 May (EMNLP 2025) Stats,"The stats for ARR May 2025 are out: [https://stats.aclrollingreview.org/iterations/2025/may/](https://stats.aclrollingreview.org/iterations/2025/may/)

It looks like about 25% of submissions have Meta ≥ 3.5. Does anyone know if it’s still possible to get into the main conference with OA 3.0 Soundness 3.3 and Meta 3.5, or is it more likely to be accepted to Findings?",23,14,2025-08-18 14:58:33,OddUnderstanding1633,0.5574,0.073,0.0,0.927,-0.0166666666666666,0.7666666666666667
[D] How would I go about clustering voices from songs?,"I have a 90s hiphop mixtape with a bunch of unknown tracks from multiple artists. I want to perform unsupervised clustering to infer how many artists there are in total because I can't really tell by ear.

I guess I would need to:

1. Somehow convert audio files into numerical data

2. Extract only the vocal data (or I guess these two steps can be flipped? Somehow extract only the vocal audio, and then convert that into numerical data?)

3. Perform unsupervised clustering


I'm just not sure how to go about doing steps 1 and 2.

Any ideas?",1,12,2025-08-18 14:45:11,padakpatek,-0.3863,0.013,0.029,0.958,0.04375,0.6173611111111112
[P] JAX Implementation of Hindsight Experience Replay (HER),"Hi! I recently discovered the *Hindsight Experience Replay* (HER) paper and noticed that the official implementation is based on PyTorch and is not very well-structured. I also couldn't find a non-PyTorch implementation. Since I primarily work with **JAX**, I decided to reimplement the classic bit-flipping experiment to better understand HER.

This implementation uses **Equinox** for model definitions and **Optax** for optimization. The [repository](https://github.com/jeertmans/HER-with-JAX) provides:
+ A *minimal* and *clean* implementation of HER in JAX
+ Reproducible scripts and results
+ A [Colab Notebook](https://colab.research.google.com/github/jeertmans/HER-with-JAX/blob/main/bit_flipping.ipynb) for direct experimentation

Code: https://github.com/jeertmans/HER-with-JAX

Let me know if you have any questions, feedback, or recommendations!",29,1,2025-08-18 13:37:33,jeertmans,0.7256,0.06,0.0,0.94,0.1697916666666666,0.4270833333333333
[D] Conferences need to find better venues,"Better = venues that are virtually accessible for any researcher/author to go to.

Just this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter.

This really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this.",202,50,2025-08-18 07:40:54,AnyIce3007,0.9053,0.159,0.031,0.81,0.3375,0.4072916666666666
[P] Looking for datasets/tools for testing document forgery detection in medical claims,"I’m a new joinee working on a project where I need to test a forgery detection agent for medical/insurance claim documents. The agent is built around GPT-4.1, with a custom policy + prompt, and it takes base64-encoded images (like discharge summaries, hospital bills, prescriptions). Its job is to detect whether a document is authentic or forged — mainly looking at image tampering, copy–move edits, or plausible fraud attempts.

Since I just started, I’m still figuring out the best way to evaluate this system. My challenges are mostly around data:

* Public forgery datasets like DocTamper (CVPR 2023) are great, but they don’t really cover medical/health-claim documents.
* I haven’t found any dataset with paired authentic vs. forged health claim reports.
* My evaluation metrics are accuracy and recall, so I need a good mix of authentic and tampered samples.

What I’ve considered so far:

* Synthetic generation: Designing templates in Canva/Word/ReportLab (e.g., discharge summaries, bills) and then programmatically tampering them with OpenCV/Pillow (changing totals, dates, signatures, copy–move edits).
* Leveraging existing datasets: Pretraining with something like DocTamper or a receipt forgery dataset, then fine-tuning/evaluating on synthetic health docs.

**Questions for the community:**

1. Has anyone come across an open dataset of forged medical/insurance claim documents?
2. If not, what’s the most efficient way to generate a realistic synthetic dataset of health-claim docs with tampering?
3. Any advice on annotation pipelines/tools for labeling forged regions or just binary forged/original?

Since I’m still new, any guidance, papers, or tools you can point me to would be really appreciated 🙏

Thanks in advance!",4,2,2025-08-18 06:42:05,___loki__,0.9804,0.108,0.009,0.883,0.3428030303030303,0.4521212121212121
[D] How to get into High Dimensional Dynamical Systems?,"Title. Also, what all areas can I hope to conduct research in? I'm a bit new to the field, and wanted to know what all it entailed before proceeding.

Any responses / suggestions are appreciated. Thanks in advance.",24,9,2025-08-18 05:41:31,Mad_Scientist2027,0.8924,0.226,0.0,0.774,0.1740909090909091,0.3236363636363636
[D] - Multi Class Address Classification,"
Hello people, I have a dataset with Adress and label 800K rows. I am trying to train a model for address label prediction. Address data is bit messy and different for each different label. we have 10390 each with 50-500 row. I have trained a model using fasttext I have got 0.5 F1 score max. What can I do to for to get best F1 score?

Address data is like (province, district, avenue street, maybe house name and no)

some of them are missing at each address.",4,7,2025-08-17 22:41:45,FineConcentrate6991,0.4588,0.073,0.051,0.875,0.12,0.39
[D] Injecting self doubt in the CoT of reasoning models,"A short analysis on what happens when you inject self doubt in the CoT of reasoning models
https://github.com/martianlantern/cot-doubt-injection",21,3,2025-08-17 19:17:27,ApartmentEither4838,-0.6124,0.0,0.167,0.833,0.0,0.3
[R] Bing Search API is Retiring - What’s Your Next Move?,"I just learned that the Bing Search API is being retired, and now I'm feeling a bit anxious. I've integrated it into a couple of my projects, one is a chatbot and the other is a lightweight research tool. It has been “good enough” for my needs so far, but now I need to find a replacement before things start to break.
Here are the options I'm considering:

1. Switch to another major provider (though I'm not thrilled about the cost and terms).

2. Build my own search stack (which might be overkill for what I need).

3. Try one of the newer AI-native search APIs and see if they are ready for production.

If you've already transitioned away from Bing, what did you switch to, and how is it performing? It seems like this change will create a significant gap for developers and AI builders.",84,19,2025-08-17 17:01:48,4yush01,0.8101,0.082,0.029,0.888,0.1238636363636363,0.6409090909090909
Is Econometrics a good background to get into Machine Learning? [D],"I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.

I have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.

How relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML)",7,13,2025-08-17 13:47:13,gaytwink70,0.8823,0.156,0.0,0.844,0.1,0.58
[P] Confused results while experimenting with attention modules on CLIP RN50 for image classification,"

Hey everyone,

I’m currently working on an audio-visual project. As a first step, I’m building unimodal models before moving on to the multimodal stage. For the vision part, I started with CLIP RN50 as the backbone and fine-tuned only the classification layer. With that setup, I was able to reach around 84% accuracy on my dataset.

To push performance, I experimented with adding attention modules:

With CBAM (Convolutional Block Attention Module), accuracy improved to 89%.

With SENet (Squeeze-and-Excitation Network), I surprisingly got an even better result: 93%.


My understanding was that CBAM, which combines both channel + spatial attention, should typically give a stronger boost than SENet, which only does channel attention. But in my experiments, the opposite happened.

Am I missing something obvious here? Could this be due to dataset characteristics, training setup, or how I integrated CBAM into CLIP?

Would really appreciate any insights, especially from people who have tried attention modules on CLIP or ResNet backbones.

Thanks!
",5,5,2025-08-17 12:47:58,Intrepid-Purpose2151,0.8977,0.113,0.036,0.851,0.0942708333333333,0.4927083333333333
[D] COLM Financial Assistance,Has anybody gotten respone from COLM financial assistance? Its deadline was 31 July but I still have not recieved a yes or no response and they are not replying to my email.,5,1,2025-08-17 09:05:25,Master_Ocelot8179,-0.6895,0.0,0.151,0.849,0.0,0.0
[R] Dino v3: Self-supervised learning for vision at unprecedented scale,"New SOTA for self supervised learning in computer vision. They train a 7B self supervised ViT on 1.7B images, which hits SOTA with linear probing on most downstream tasks. They also release scaled and distilled versions of the model (ViT small, base, large, and huge, plus ConvNext tiny, small, base, and large), along with a version trained on satellite imagery.

There are plenty of details in the paper as to what pretraining improvements they made over DINO v2. ",212,17,2025-08-16 22:07:45,say_wot_again,0.765,0.095,0.0,0.905,-0.0031877213695395,0.6283353010625738
[D] model architecture or data?,"I’ve just read that the new model architecture called Hierarchical Reasoning Model (HRM) gains it’s performance benefits from data augmentation techniques and chain of thought rather than model architecture itself. link: https://arcprize.org/blog/hrm-analysis

And i’ve heard same opinion about transformers that the success of current llms is about cramming enormous amounts of data into it rather than the genius of the architecture

Can someone explain which of the sides is closer to the truth?",36,16,2025-08-16 14:20:49,the_iegit,0.885,0.133,0.0,0.867,0.0872727272727272,0.3759090909090909
[R] How do I choose the best model in validation when I have no target data??,"I am working on unsupervised domain adaptation techniques for super resolution. I have a good amount of paired source data and very less target data without no ground truth. The issue is while training this pipeline I am not able to save the best model as for this I would need some ground truth in the target domain on which I would validate the model after each epoch and save the best one. How do I tackle this? Recently, I found an OpenReview paper about a transfer score which is a metric which do not need target labels but it is for classification based tasks. I want something for super-resolution. Does anyone have any idea?",0,2,2025-08-16 10:06:16,Slight-Ad-5816,0.821,0.125,0.075,0.801,0.4458333333333333,0.3910416666666666
"[D] Cool new ways to mix linear optimization with GNNs? (LP layers, simplex-like updates, etc.)","Lately I’ve been diving into how graph neural networks can play nicely with linear optimization, not just as a post-processing step, but actually inside the model or training loop.

I’ve seen some neat stuff around differentiable LP layers, GNNs predicting parameters for downstream solvers, and even architectures that mimic simplex-style iterative updates. It feels like there’s a lot of room for creativity here, especially for domain-specific problems in science/engineering.

Curious what’s been coming out in the last couple of years. Any papers, repos, or tricks you’ve seen that really push this GNN + optimization combo forward? Supervised, unsupervised, RL… all fair game.",24,3,2025-08-16 03:44:53,ilovecookies14,0.9668,0.223,0.042,0.735,0.1078512396694215,0.5792011019283747
[D] - Neurips Position paper reviews,"The position paper reviews were just released. So far this entire process has been very unprofessional, with multiple delays, poor communication, and still no clear rubric for what the review scores mean. Has anyone else gotten reviews? Curious to hear other's thoughts on this",45,38,2025-08-15 19:19:38,Routine-Scientist-38,-0.6115,0.085,0.154,0.761,-0.0763888888888889,0.5523148148148148
[D] Bethe Hessian Spectral Clustering,"Why does nobody seem to use this when it works noticeably better than regular (normalised laplacian) spectral clustering? I have studied it a fair bit and cant see any downsides apart from ever so slightly higher computational cost (the order of magnitude doesn't change, just a larger constant.)

Its also been around long enough now that I dont see recency as the issue.",12,4,2025-08-15 11:12:45,Agreeable_Touch_9863,0.6369,0.077,0.0,0.923,0.175,0.4637820512820513
custom Vulkan C++ machine learning library vs TensorFlow [R],guys I need your opinion: I made a machine learning library using Vulkan (with compute shaders to preform the forward and backward passes) and I found that base tensorflow (on CPU) is faster than my custom model that uses GPUs. I had the simplest test where I used a very large kernel on a singe dense (ffn) layer and tensorflow is much faster. The only operation that is done in this model is a forward and backward matmul which the GPU should be much faster at. what do you guys think is the reason? -ps I asked chatgpt and I literally what to k\*ll it cause it repeats the same wrong things,5,14,2025-08-14 23:29:54,Onlyheretohelp_you,-0.4767,0.0,0.028,0.972,-0.0887755102040816,0.5688775510204082
Problem with dataset for my my physics undergraduate paper. Need advice about potential data leakage. [N],"Hello.

I am making a project for my final year undergraduate dissertation in a physics department. The project involves generating images (with python) depicting diffraction patters from light (laser) passing through very small holes and openings called slits and apertures. I used python code that i could pass it the values of some parameters such as slit width and slit distance and number of slits (we assume one or more slits being in a row and the light passes from them. they could also be in many rows (like a 2d piece of paper filled with holes). then the script generates grayscale images with the parameters i gave it. By giving different value combinations of these parameters one can create hundreds or thousands of images to fill a dataset.

So i made neural networks with keras and tensorflow and trained them on the images i gave it for image classification tasks such as classification between images of single slit vs of double slit.  Now the main issue i have is about the way i made the datasets. First i generated all the python images in one big folder. (all hte images were even slightly different as i used a script that finds duplicates (exact duplicates) and didnt find anything. Also the image names contain all the parameters so if two images were exact duplicates they would have the same name and in a windows machine they would replace each other). After that, i used another script that picks images at random from the folder and sends them to the train, val and test folders and these would be the datasets the model would train upon.

PROBLEM 1:

The problem i have is that many images had very similar parameter values (not identical but very close) and ended up looking almost identical to the eye even though they were not duplicates pixel to pixel. and since the images to be sent to the train, val and test sets were picked at random from the same initial folder this means that many of the images of the val and test sets look very similar, almost identical to the images from the train set. And this is my concern because im afraid of data leakage and overfitting. (i gave two such images to see)

Off course many augmentations were done to the train set only mostly with teh Imagedatagenerator module while the val and test sets were left without any augmentations but still i am anxious.

PROBLEM 2:

Another issue i have is that i tried to create some datasets that contained real photos of diffraction patterns. To do that i made some custom slits at home and with a laser i generated the patterns. After i managed to see a diffraction pattern i would take many photos of the same pattern from different angles and distances. Then i would change something slightly to change the diffraction pattern a bit and i would again start taking photos from different perspectives. In that way i had many different photos of the same diffraction pattern and could fill a dataset. Then i would put all the images in the same folder and then randomly move them to the train, val and test sets. That meant that in different datasets there would be different photos (angle and distance) but of the same exact pattern. For example one photo would be in the train set and then another different photo but of the same pattern in the validation set. Could this lead to data leakage and does it make my datasets bad? bellow i give a few images to see.

if there were many such photos in the same dataset (for example the train set) only and not in the val or test sets then would this still be a problem? I mean that there are some trully different diffraction patterns i made and then many photos with different angles and distances of these same patterns to fill hte dataset? if these were only in one of the sets and not spread across them like i described in hte previous paragraph?

[photo of double slit diffraction \(train set\)](https://preview.redd.it/vn95v576y6jf1.jpg?width=400&format=pjpg&auto=webp&s=402a1bc2df3cf80b9b5ee90d6da42ac64dd3fef7)

[photo of double slit diffraction \(val set\)](https://preview.redd.it/6j6o6876y6jf1.jpg?width=400&format=pjpg&auto=webp&s=a30f4c67036a800a33b5571475c997b43857b98a)

[python image single slit diffraction \(train set\)](https://preview.redd.it/wz2nts76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=9fcfac7032d3c9de2255055f7c96abac774b8687)

[python image \(single slit val set\)](https://preview.redd.it/78xiee76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=29342d997939aa13d5fd4a004c29228d61f13896)",9,8,2025-08-14 23:04:55,AncientGearAI,-0.8916,0.022,0.031,0.946,0.046828231292517,0.4441819727891158
[2507.17338] Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks,"Research showcasing how a robot outperforms state of the art models on the Habitat benchmark from Meta ***without pre-training***.

For those fluent in 🤖 what you think?",7,0,2025-08-14 21:23:01,stevenverses,0.128,0.071,0.058,0.871,-0.1333333333333333,0.6
[R] Code for Flow Stochastic Segmentation Networks (ICCV 20205),"Code & paper at: [https://github.com/biomedia-mira/flow-ssn](https://github.com/biomedia-mira/flow-ssn)

**TL;DR**

\- A flow's prior is typically fixed (e.g. N(0, I)). We learn it and use a **lightweight** flow to model pixel dependencies;

\- This makes sampling (ODE solving) more **efficient**, without sacrificing performance in our setting;

\- We introduce bespoke training objectives for both **autoregressive** and **continuous-time flow** variants;

\- Flow-SSN achieves **SOTA** performance on standard stochastic segmentation benchmarks!

https://preview.redd.it/rllc2yplo1jf1.png?width=3850&format=png&auto=webp&s=6bb1bc63a6836b9fc6a4b8e9f10205889a5b051d

https://i.redd.it/8vgf2iemo1jf1.gif

https://i.redd.it/81lbt56no1jf1.gif",15,0,2025-08-14 20:32:35,Majestij,0.0,0.0,0.0,1.0,-0.03,0.3399999999999999
"[P] Can I use test set reviews to help predict ratings, or is that cheating?","I’m working on a rating prediction (regression) model. I also have reviews for each user-item interaction, and from those reviews I can extract “aspects” (like quality, price, etc.) and build a separate graphs and concatenate their embeddings at the end to help predicting the score.

My question is: when I split my data into train/test, is it okay to still use the aspects extracted from the test set reviews during prediction, or is that considered data leakage?

In other words: the interaction already exists in the test set, but is it fair to use the test review text to help the model predict the score? Or should I only use aspects from the training set and ignore them for test interactions?

Ps: I’ve been reading a paper where they take user reviews, extract “aspects” (like quality, price, service…), and build an aspect graph linking users and items through these aspects.

In their case, the goal was link prediction — so they hide some user–item–aspect edges and train the model to predict whether a connection exists.",1,4,2025-08-14 19:59:46,AdInevitable1362,0.5563,0.065,0.042,0.893,0.26875,0.69375
"[D] People in ML/DS/AI field since 5-10 years or more, are you tired of updating yourself with changing tech stack?","I have been in this space since SAS, and its quite exhausting to update with every skill in the market to stay relevant especially if trying for a job switch and going through the interviews. Till how long can you keep studying and updating with the new trend and also even if you get in the boat there is so much stress at the work place in these sectors mainly because the leadership is from the management background and theres a lot of pressure for tech people to deliver.

Although I love my field but I have got to thinking lately that Is it even worth it?",96,75,2025-08-14 16:43:01,ImaginationAny2254,-0.2564,0.038,0.062,0.9,0.0877525252525252,0.5239898989898989
[D] Best way to partition longitudinal data into pre and post time periods for predictive model?,"I'm working on several healthcare models that will predict future health conditions for individuals using past longitudinal data. We have data spanning 6 years.

In the past I'd split the data into one year time spans by calendar year and train the model to predict the outcome in year t1 from predictors in the prior year t0. If we have 6 years of data for a person I'd transform their data from wide to long format: 5 rows of pre and post periods. But I'm not certain this is the best approach.

What is the optimal way to split my data into pre and post time periods to obtain the best prediction accuracy? 6 month time periods instead of 1 year? Or lump all past data for each person into a single pre period & post period (1 row)? I understand it may come down to testing different formats, see what sticks.",5,7,2025-08-14 15:29:41,RobertWF_47,0.9595,0.105,0.013,0.882,0.1177248677248677,0.2833068783068783
[P] Small and Imbalanced dataset - what to do,"Hello everyone!

I'm currently in the 1st year of my PhD, and my PI asked me to apply some ML algorithms to a dataset (n = 106, w/ n = 21 in the positive class). As you can see, the performance metrics are quite poor, and I'm not sure how to proceed...

I’ve searched both in this subreddit and internet, and I've tried using LOOCV and stratified k-fold as cross-validation methods. However, the results are consistently underwhelming with both approaches. Could this be due to data leakage? Or is it simply inappropriate to apply ML to this kind of dataset?

Additional info:  
I'm in the biomedical/bioinformatics field (working w/ datasets of cancer or infectious diseases). These patients are from a small, specialized group (adults with respiratory diseases who are also immunocompromised). Some similar studies have used small datasets (e.g., n = 50), while others succeeded in work with larger samples (n = 600–800).  
Could you give me any advice or insights? (Also, sorry for gramatics, English isn't my first language). TIA!

https://preview.redd.it/fc20uero50jf1.png?width=655&format=png&auto=webp&s=1ed35c046f9c2bfe030e0c3bfe8c4cdcf7afb852

",44,36,2025-08-14 15:20:53,Practical-Pin8396,0.4445,0.055,0.067,0.878,-0.0436079545454545,0.4843637265512266
[P] Sensor calibration correction,"A few months ago, I had calibrated a few pairs of cam and lidar sensor, namely the intrinsics of each cam and the extrinsic between each cam and lidar in pair

A few days ago, while projecting the lidar points to the camera space, I noticed a consistent drift across the cam and lidar, and was hoping to correct in automatic ways instead of manually do so.

Instantly, one thought was to use depth as a feature to match against the 2 modalities. I have done Monocular Depth Estimation (MDE), with DepthAnything V2 and Apple’s Depth Pro, on the cam, converted the lidar points into a numpy tensor of depths, and calculated the:
- Huber,
- Scale Invariant Log Loss 

separately. I used both these techniques during a grid search of 5 degrees rot on pitch, roll, yaw, but wasn’t able to get the results I needed. The projections were still wrong.

I knew classical techniques like edge detection that are considered foundational. But it seemed too noisy to satisfy my brain giggle. I still gave it a go and I haven’t seemed to get it working. I used edges and the nature of its distribution out in the scene, and calculated the average loss between closest edges.

I am trying to get back to using MDE, since it’s continuous and differentiable. 

I’d like to open the discussion towards what ideas y’all think will work.",7,0,2025-08-14 03:27:46,solidpoopchunk,0.743,0.069,0.046,0.885,-0.0454545454545454,0.331060606060606
[D] Got Spare Time – What’s Worth Doing?,"I'm a fresh PhD graduate and I finally landed a job which I start in a few months.  
It happened to be that I have quite a bit of free time, at least until my next journey. I thought about taking a few months off, but a few weeks in and I start to feel a bit out of place.  
I really don't know how to handle simply doing nothing.

I thought maybe I’d start some initiative in this rare window I’m in right now, and I was hoping to get interesting ideas from the community.

My main objective is that it would be something valuable that I enjoy doing.  
This could be something that is technically cool (AGI anyone?) or some tool for the community (any tool you'd wish existed? paperswithcode or paper copilot comes to mind).

Love to hear your thoughts!",43,46,2025-08-13 18:00:48,Entrepreneur7962,0.9874,0.216,0.0,0.784,0.1540726817042606,0.4092731829573934
"[D] If there were to be some sort of way you could get NDVI (not true, but predict) that was near perfect accuracy through JUST standard RGB input (NO NIR AT ALL), how useful would that be (API, for example)?","Sorry if this is not the right place to post! I'm new to the community and overall GIS industry. Just want to see how useful this would be, specific use cases, and maybe how this could be used by you personally.

  
I know there are RGB-only indices that exist, but from what I've heard, they're very inaccurate. This would be 94%+ (accuracy to true-NDVI) and it’s a highly trained ML model",0,3,2025-08-13 17:01:28,Proud_Landscape_4231,0.9427,0.136,0.012,0.852,0.1341790352504638,0.3789471243042671
[D] Google DeepMind Analytics Engineer Interview Prep,Got an upcoming interview for this role and have a good feeling so far. How do I prepare for it? What will be the next steps? Any tips or experience would be greatly appreciated. Thanks!,21,5,2025-08-13 14:53:04,ChampionshipCrazy429,0.8962,0.247,0.0,0.753,0.25,0.38
[D] EMNLP 2025 Decisions,"Discussion thread for EMNLP 2025 decisions

",31,503,2025-08-13 13:02:29,Accomplished-Pay-390,0.0,0.0,0.0,1.0,0.0,0.0
[R] Fuzzy-Pattern Tsetlin Machine,"I’m excited to announce the paper: **Fuzzy-Pattern Tsetlin Machine** (FPTM) — a paradigm shift in the Tsetlin Machine family of algorithms.

Unlike traditional Tsetlin Machines, which rely on strict clause evaluation, FPTM introduces fuzzy clause evaluation: if some literals in a clause fail, the remaining literals can still contribute to the vote with a proportionally reduced score. This allows each clause to act as a collection of adaptive sub-patterns, enabling more flexible, efficient, and robust pattern matching.

Thanks to this fuzzy mechanism, FPTM dramatically reduces the number of required clauses, memory usage, and training time — all while improving accuracy.

**Results:**

**IMDb** dataset:

• 90.15% accuracy with just **1 clause** per class

• 50× reduction in clauses and memory vs. Coalesced TM

• 36× to 316× faster training (**45 seconds vs. 4 hours**) compared to TMU Coalesced TM

• Fits in **50 KB**, enabling online learning on microcontrollers

• Inference throughput: **34.5 million** predictions per second (51.4 GB/s)

**Fashion-MNIST** dataset:

• 92.18% accuracy (2 clauses per class)

• 93.19% accuracy (20 clauses), \~400× clause reduction vs. Composite TM (93.00% with 8000 clauses)

• **94.68%** accuracy (8000 clauses), establishing a new *state-of-the-art* among all TM variants and outperforming complex neural net architectures like *Inception-v3*

**Amazon Sales** dataset (20% noise):

• **85.22%** accuracy — outperforming Graph TM (78.17%) and GCN (66.23%)

📄 Read the paper: [https://arxiv.org/pdf/2508.08350](https://arxiv.org/pdf/2508.08350)

💻 Source code: [https://github.com/BooBSD/FuzzyPatternTM](https://github.com/BooBSD/FuzzyPatternTM)",49,5,2025-08-13 11:55:31,ArtemHnilov,0.9197,0.087,0.015,0.898,-0.0698863636363636,0.5295454545454545
[R] About test set of XGBoost for Time Series Forecasting,"I have questions about using XGBoost for the Time Series Forecasting problem. According to these articles:

[Multi-step time series forecasting with XGBoost | Towards Data Science](https://towardsdatascience.com/multi-step-time-series-forecasting-with-xgboost-65d6820bec39/)[XGBoost for ](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)

[Multi-Step Univariate Time Series Forecasting with MultiOutputRegressor | XGBoosting](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)

[How I Trained a Time-Series Model with XGBoost and Lag Features](https://medium.com/@connect.hashblock/how-i-trained-a-time-series-model-with-xgboost-and-lag-features-8c17439c81e4)

I understand that they are using a sliding window approach to create ($t\_1, t\_2, ..., t\_n, t\_{n+1}, t\_{n+2}..., t\_m$), where the first $n$ variables are used as feature variables and the last $m$ variables are used as target variables. Then, they feed these rows into the XGBoost to find the relationship between the feature variables and target variables.

My problem is: It appears that during the testing phase, they utilized the actual feature variables for testing. For example, when we are predicting the first future $m$ points, we still have the actual $n$ points before these $m$ points as the features. However, when we are predicting the $m+1$ points, we are missing the actual value for the first feature in the $n$ features.

But in the above articles, it seems they just assume they have the actual $n$ at all times during training.

And for the paper ""Do We Really Need Deep Learning Models for Time Series Forecasting?"", for table 1 as shown below:

I think h refers to the number of regressors they are using. So, for the first row, they can forecast 24 points using the existing training data. But how can they further forecast τ points beyond the 20th point?

So, I want to clarify

1. Do the methods in the above articles suffer from data leakage? Or is it safe to assume that we can know the real $n$ features when we are focusing on the $m$ new data points?
2. My current idea is that for using XGBoost in time series forcasting, we can either

* Feed back the predicted value as the $n$ feature for the upcoming forcasting of $m$ points.
* Or we train $L$ independent regressors to forecast the $L$ points in the future in one batch.

",1,1,2025-08-12 12:32:11,fishsoon2020,0.4117,0.041,0.033,0.926,0.0765151515151515,0.2158143939393939
[D] Applying Prioritized Experience Replay in the PPO algorithm,"When using the PPO algorithm, can we improve data utilization by implementing Prioritized Experience Replay (PER) where the priority is determined by both the probability ratio and the TD-error, while simultaneously using a windows\_size\_ppo parameter to manage the experience buffer as a sliding window that discards old data?",2,1,2025-08-13 01:30:00,NoteDancing,0.5106,0.089,0.034,0.877,0.1,0.2
[R] Promising Research Directions for VLMs in the Medical Domain,"Dear all,

I’d like to hear the community’s thoughts on promising research directions for VLMs (e.g., CLIP), particularly in the medical domain.

Thank you in advance!",0,1,2025-08-13 03:55:28,Elegant_Dream4936,0.906,0.3,0.0,0.7,0.1133333333333333,0.2666666666666666
[D] Statement on the Originality of OpenRLHF and veRL FSDP RLHF,">From the original chinese zhihu blogpost (2025/5): [https://zhuanlan.zhihu.com/p/23147932785](https://zhuanlan.zhihu.com/p/23147932785)

**Recently, there has been quite a bit of discussion and controversy online about OpenRLHF and veRL.**  
**As the original author, I feel compelled to issue a statement.**

In short: **OpenRLHF is like KartRider — the original — and veRL FSDP is like QQ Speed, which is basically a copycat of OpenRLHF.**

# 1. Performance Differences Between OpenRLHF and veRL

There is no fundamental performance difference between veRL’s FSDP RLHF and OpenRLHF (DeepSpeed) because both use vLLM for inference and ZeRO3 for training.  
The performance data in veRL’s original paper was based on *Megatron* RLHF vs. the old OpenRLHF 0.2 version.  
If you think there’s a big performance gap, you probably just used it incorrectly. At the moment, FSDP is slightly faster than DeepSpeed, but with the release of DeepSpeed’s **deepcompile** and especially **AutoTP**, DeepSpeed is expected to overtake in performance.

# 2. On HybridFlow Free Scheduling

Any RLHF framework developed with Ray can achieve free scheduling because Ray natively provides the *placement group* feature.  
This means HybridFlow in veRL's paper is essentially just a nicer name for Ray’s Placement Group API.  
Currently, OpenRLHF fully implements HybridFlow, whereas veRL does not.  
OpenRLHF also supports independent deployment of vLLM and Actors to prevent OOM issues when training very large models (32B+ or long-text).  
In fact, OpenRLHF was the **first** framework to support this feature based on Ray Placement Group API.

# 3. Hybrid Engine

Hybrid Engine was first proposed by **DeepSpeedChat**, not an original contribution from veRL.  
Both veRL and OpenRLHF now support this feature.

# 4. Ray + vLLM + HF Transformers + ZeRO3 for RLHF Training

This setup is one of the **simplest and most user-friendly** high-performance RLHF training solutions, combining ease of use with top performance.

It was first proposed and open-sourced by OpenRLHF (open-sourced in Aug 2023, most features completed by Jan 2024).  
veRL FSDP **fully copied** this setup.

https://preview.redd.it/vfzm143vroif1.png?width=1440&format=png&auto=webp&s=10d8a5bcd101455a06a3506f037abc10f12dd277

https://preview.redd.it/tqela8mvroif1.png?width=1440&format=png&auto=webp&s=c3a2daa1ead45f7434184f107da8ba2f78cc9c8d

The core idea at the time was to use the HF weight format as a bridge, enabling seamless weight synchronization and high-performance inference based on ZeRO3 / AutoTP mechanisms, **avoiding** heavyweight frameworks like Megatron.

**The Original OpenRLHF Architecture:**  
**Ray + vLLM + ZeRO + HF**

There are also many related implementation details:

* Supported feature list
* Standardized interfaces such as `--input_key` to specify the input field format

All of these in veRL FSDP were **modeled after OpenRLHF**.

**Example from code details:**  
veRL:

https://preview.redd.it/b8f2lprwroif1.png?width=1440&format=png&auto=webp&s=a0daf3eab1c77f71e4917c044f988c35e229baa4

https://preview.redd.it/exf7lxhxroif1.png?width=1440&format=png&auto=webp&s=220636cea299502df1b94e2544a76b34e2acb6c7

OpenRLHF:

https://preview.redd.it/qfakvovyroif1.png?width=1440&format=png&auto=webp&s=260775676354a50bacd79ce06fb25417a53466de

Other design ideas like **ref\_reward offload**, **critic pretrain**, **remote RM**, etc., were also first conceived or proposed by OpenRLHF, and veRL FSDP later implemented corresponding features.

# 5. Single Controller

*(Update May 2025)*

The “Single Controller” concept mentioned in the veRL paper comes from the same Ray design pattern as HybridFlow.

In early versions of OpenRLHF’s Ray RLHF implementation, there was a `RayPPOActorGroup` concept—managing a group of DeepSpeed ZeRO DP processes with a single Ray Group class, and providing an `async_run_method` interface to control all processes in the group at once.  
That’s essentially the core idea of Single Controller.

[https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300](https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300)

This interface wasn’t enabled at first because the codebase needed to be compatible with both Ray and non-Ray RLHF paths. Later, when the non-Ray code was removed, the API was naturally enabled.

Lastly, I want to thank ByteDance for open-sourcing its internal framework for everyone to use and maintain, which helps the open-source community thrive (e.g., FSDP / Ulysses support).

However, I hope friends in the community won’t disparage other open-source frameworks.  
OpenRLHF, as a **zero-budget, purely open-source** project, can’t compete in development speed with large commercial projects like veRL—  
I only hope this post helps preserve the contributions OpenRLHF has made to the RLHF open-source community.

**Btw, the open-source community should respect originality in order to develop healthily.**",12,1,2025-08-13 01:05:42,seventh_day123,0.997,0.129,0.016,0.855,0.1176807760141093,0.3943121693121693
[D] Multiple submission policy at EMNLP 2025 for workshops,"Hi all,

I’m trying to understand the EMNLP 2025 multiple submission policy when it comes to co-organized workshops.

Our paper is committed to EMNLP 2025 (main conference), but we think it might also be a good fit for a specific workshop, in case if it is not accepted to EMNLP. 

The problem is, the workshop’s submission deadline is before the EMNLP notification date (Aug 20).

The workshop’s CFP says multiple submissions are fine if disclosed at submission. However, the EMNLP CFP states it follows the ARR multiple submission policy, which includes this clause:

> Commitment + Commitment/Other Venue: Whether you can commit/submit to two venues simultaneously depends on the dual submission policies of those venues. Typically, it is not permitted.

[ARR policy](https://aclrollingreview.org/cfp#:~:text=Multiple%20Submission%20Policy)


TL;DR 

What I’m unsure about is this:

- Does “other venue” here include EMNLP co-organized workshops?

- Has anyone successfully submitted to both the main conference and a co-organized workshop in this timing overlap?


I couldn’t find any direct clarification online for this year, so I’d really appreciate hearing from researchers who’ve navigated this.

Thanks!",4,1,2025-08-12 20:49:31,alkalinemoe,0.9678,0.138,0.041,0.821,0.1942708333333333,0.3072916666666667
Guidance on improving the reconstruction results of my VAE [Project],"Hi all! I was trying to build a VAE with an LSTM to reconstruct particle trajectories by basing off my model on the paper ""Modeling Trajectories with Neural Ordinary Differential Equations"". However, despite my loss plots showing a downward trend, my predictions are linear.

I have applied KL annealing and learning rate scheduler - and yet, the model doesn't seem to be learning the non-linear dynamics. The input features are x and z positions, velocity, acceleration, and displacement. I used a combination of ELBO and DCT for my reconstruction loss. The results were quite bad with MinMax scaling, so I switched to z-score normalization, which helped improve the scales. I used the Euler method with torchdiffeq.odeint.

Would it be possible for any of you to guide me on what I might be doing wrong? I’m happy to share my implementation if it helps. I appreciate and am grateful for any suggestions (and sorry about missing out on the labeling the axes - they are x and z)

https://preview.redd.it/veskdk7p7nif1.png?width=529&format=png&auto=webp&s=0938c4dd588961f94eba40a0e20d81008bc131f0

https://preview.redd.it/ddubae7p7nif1.png?width=529&format=png&auto=webp&s=15a24e197e6fd331d92175d1327fb2b482aaa2cc",3,11,2025-08-12 19:55:28,fictoromantic_25,0.9367,0.143,0.056,0.802,-0.1928571428571428,0.7309523809523809
[P] Can anyone suggest an open weights AI Humanizer?,"I've often wanted to make an AI humanizer. The first approach I've tried was using `meta-llama/Llama-3.1-8B`. I first made a BERT fine-tune to classify between AI generated and human written. Then, I used a modified RL approach to fine-tune `meta-llama/Llama-3.1-8B` to rephrase an existing AI generated text, optimizing the humanness score. I repeated this several times, each time training a new scorer, similar to the GAN framework. This was largely unsuccessful. Unfortunately I can't share code because this was done months ago and I'm just now coming back to it, and I didn't properly track versions. I now believe that a T5 model would be better suited for this task than a Llama model. Does anyone have any suggestions, links, papers, or models that they can recommend? I am looking for open weights/open source models, not paid APIs.",0,4,2025-08-12 19:55:20,Pretend_Guava7322,0.4537,0.061,0.047,0.892,0.0654345654345654,0.3576756576756577
[R]: Intuition emerges in Maximum Caliber models at criticality,"Are today’s AI models hitting a wall or just missing a law?

This recent preprint in arXiv proposes a minimal sandbox (a maze) and a statistical physics approach (Maximum Caliber principle) to address this question. The presented method, called mind-tuning, applies Maximum Caliber to predictive models and reveals a critical intuition phase between imitation and hallucination.

[https://arxiv.org/abs/2508.06477](https://arxiv.org/abs/2508.06477)",0,1,2025-08-12 09:29:58,mystic_blue5,-0.5423,0.0,0.071,0.929,-0.0866666666666666,0.3399999999999999
[P] Dealing with EXTREME class imbalance(0.095% prevalence),"I’m trying to build a model for fraud prediction where I have a labeled dataset of ~200M records and 45 features. It’s supervised since I have the target label as well. It’s a binary classification problem and I’ve trying to deal with it using XGB and also tried neural network. 

The thing is that only 0.095% of the total are fraud. How can I make a model that generalizes well. I’m really frustrated at this point. I tried everything but cannot reach to the end. Can someone guide me through this situation?",15,14,2025-08-12 06:54:07,hsbdbsjjd,-0.7191,0.032,0.104,0.864,-0.20625,0.7375
[D] Evaluation Drift and Contamination Mitigation in Foundation Model Assessment,"As foundation models scale and benchmarks saturate, contamination and drift present increasing challenges to meaningful evaluation. Sharing practical mitigation strategies that have worked in practice:



\*\*Contamination Detection:\*\*

\- N-gram overlap analysis (sliding window approach)

\- Substring matching with fuzzy boundaries  

\- Semantic similarity scoring via embeddings

\- Statistical outlier detection in performance curves



\*\*Dataset Hygiene:\*\*

\- Temporal splits with strict cutoffs (no post-training data)

\- Hold-out validation across multiple independent sources

\- Private test sets with limited query budgets

\- Adversarial examples targeting memorization vs. understanding



\*\*Drift Mitigation:\*\*

\- Rolling evaluation windows with decay weighting

\- Multi-task assessment reducing single-metric gaming

\- Human evaluation correlation tracking over time

\- Cross-validation with domain-specific benchmarks



\*\*Process Controls:\*\*

\- Blind evaluation protocols (evaluator doesn't know model identity)

\- Staged releases with contamination audits between stages

\- Community-sourced benchmark validation

\- Reproducibility requirements for evaluation code



Seeing gaps in current practice around contamination detection at scale and standardized tooling for drift measurement. What approaches have proven most effective in your evaluation pipelines?",1,1,2025-08-12 06:49:51,hero88645,-0.0018,0.052,0.052,0.897,0.0935064935064935,0.3281385281385281
[D] Reliability Metrics and Failure Taxonomy for Agent Tool-Use Systems,"Observing increasing deployment of agentic systems with tool access, but reliability evaluation remains fragmented. Key reliability metrics worth standardizing:



\*\*Success Rate Decomposition:\*\*

\- Tool selection accuracy (right tool for task)

\- Parameter binding precision (correct arguments)

\- Error recovery effectiveness (fallback strategies)

\- Multi-step execution consistency



\*\*Failure Taxonomy:\*\*

\- Type I: Tool hallucination (non-existent APIs)

\- Type II: Parameter hallucination (invalid args)

\- Type III: Context drift (losing task state)

\- Type IV: Cascade failures (error propagation)

\- Type V: Safety violations (unauthorized actions)



\*\*Observable Proxies:\*\*

\- Parse-ability of tool calls (syntactic validity)

\- Semantic coherence with task context

\- Graceful degradation under uncertainty

\- Consistency across equivalent phrasings



Current evals focus on task completion but miss failure modes that matter for deployment. Need systematic measurement of these reliability dimensions across diverse tool ecosystems.



Thoughts on standardizing these metrics across research groups?",1,1,2025-08-12 06:48:34,hero88645,-0.9593,0.062,0.158,0.78,-0.0079365079365079,0.4392857142857143
[R] Position: The Current AI Conference Model is Unsustainable!,"
Paper: https://www.alphaxiv.org/abs/2508.04586v1  


📈 Publication Surge: Per-author publication rates have more than doubled over the past decade to over 4.5 papers annually.  


🚀 Exponential Output Growth: Individual contributions are rising so fast they’re projected to exceed one paper per month by the 2040s.  


🌍 Carbon Overload: NeurIPS 2024’s travel emissions (>8,254 tCO₂e) alone surpass Vancouver’s daily citywide footprint.  


😞 Mental Health Toll: Of 405 Reddit threads on AI conferences, over 71% are negative and 35% mention mental-health concerns.  


⏳ Research-Conference Mismatch: The AI research lifecycle outpaces conference schedules, often rendering results outdated before presentation.  


🏟️ Venue Capacity Crisis: Attendance at top AI conferences like NeurIPS 2024 is already outstripping available venue space.",391,52,2025-08-12 06:10:51,NuoJohnChen,-0.7712,0.054,0.099,0.847,0.0499999999999999,0.3893939393939394
[R] gpt-oss is actuall good: a case study on SATA-Bench,"I’ve been experimenting with gpt-oss since its release, and unlike many posts/news I’ve seen, it’s surprisingly powerful — even on uncommon datasets. I tested it on our recent benchmark SATA-Bench — a benchmark where each question has at least two correct answers (rare in standard LLM Evaluation).

Results (See picture below):

1. 120B open-source model is similar to GPT-4.1's performance on SATA-Bench.
2. 20B model lags behind but still matches DeepSeek R1 & Llama-3.1-405B.

https://preview.redd.it/eowlge0jjiif1.jpg?width=1568&format=pjpg&auto=webp&s=bfc0fdc20fc1545000ff55cc45f3b65391e85c46

 takeaways:

Repetitive reasoning hurts — 11% of 20B outputs loop, losing \~9 exact match rate.

Reason–answer mismatches happen often in 20B and they tend to produce one answer even if their reason suggest a few answer is correct.

Longer ≠ better — overthinking reduces accuracy.

Detailed findings: [https://weijiexu.com/posts/sata\_bench\_experiments.html](https://weijiexu.com/posts/sata_bench_experiments.html)

SATA-Bench dataset: [https://huggingface.co/datasets/sata-bench/sata-bench](https://huggingface.co/datasets/sata-bench/sata-bench)",11,2,2025-08-12 04:06:26,ApprehensiveAd3311,-0.5423,0.067,0.085,0.848,0.0647058823529411,0.5647058823529413
[R] AAAI 2026 Reviewer Assignments?,"Did anyone get assigned papers?

I submitted the biddings long time ago.",15,24,2025-08-12 02:45:20,Healthy_Horse_2183,0.0,0.0,0.0,1.0,-0.05,0.4
[N] OpenAI Delivers Gold-medal performance at the 2025 International Olympiad in Informatics,"[https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL](https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL)

>We officially entered the 2025 International Olympiad in Informatics (IOI) online competition track and adhered to the same restrictions as the human contestants, including submissions and time limits,",57,22,2025-08-11 21:26:41,Realistic-Bet-661,0.0,0.0,0.0,1.0,0.0,0.05625
DRTP and No-Prop Hybrid in Pure C [R],"Hey guys its me again I made a new algorithm with No Prop and DRTP that hit a 91.25% on MNIST with one hidden layer and I did it all in pure C here is the link to the repo I will be writing a paper on it please leave reviews and feedback I am a undergraduate student trying to get an internship for ML Research and or Engineering. First in the world from what I can see by the way.

[https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C](https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C)",0,9,2025-08-11 16:30:19,PlugTheGreatest,-0.0258,0.028,0.042,0.93,0.1296536796536796,0.4242424242424242
[D] Has anyone tried cross-modal transfer for visual reasoning? This 76% MMMU result surprised me,"I've been spending a lot of time lately evaluating different multimodal reasoning models for my research, and the gap between closed-source models like GPT-4.1 and open-source alternatives has been really frustrating. Most open models either can't handle complex visual reasoning or require massive compute resources.

Recently I came across Skywork-R1V3, a 38B parameter model that's been getting some attention in the community, so I decided to put it through its paces. What caught my eye initially was their claim of 76.0% accuracy on MMMU, which would put it competitive with much larger proprietary models.

After testing it extensively, I have to say the technical approach is really interesting. The model builds on InternVL-38B but what makes it special is how the Skywork team approached the reasoning problem. Instead of training visual reasoning from scratch, they found a way to transfer reasoning patterns from their existing text-based models into the multimodal domain.

From what I can tell from the paper and my experiments, they used reinforcement learning during post-training rather than just supervised fine-tuning. This seems to be key to why it performs so well on complex reasoning tasks. When I tested it on mathematical problems with diagrams and scientific figure interpretation, it consistently broke down problems into logical steps rather than just pattern matching.

The performance claims seem to hold up in my testing. It's genuinely competitive with closed-source alternatives on the types of visual reasoning tasks I care about, and the fact that it's fully open-source with quantized versions available makes it actually usable for research. I've been running the AWQ quantized version on a single A100 without issues.

What really impressed me is how well it handles cross-disciplinary reasoning where you need to connect visual information with abstract concepts. The chain-of-thought capabilities feel much more robust than other open models I've tried.

This connects to the broader Skywork ecosystem - their reward models have been downloaded over 750,000 times and seem to be helping multiple frontier models achieve strong benchmark results. There's clearly some solid technical work happening there.

I'm curious if others have experimented with cross-modal transfer approaches like this, or if anyone else has found effective ways to get strong reasoning performance without massive scale. Also interested in hearing thoughts on RL vs supervised approaches for this kind of multimodal reasoning - my sense is that RL might be underutilized in this space but I'd love to hear other perspectives.",58,4,2025-08-11 15:14:07,Jealous-Leek-5428,0.9954,0.157,0.035,0.808,0.1126984126984127,0.4395305639986491
[P] VulkanIlm: Accelerating Local LLM Inference on Older GPUs Using Vulkan (Non-CUDA) — Benchmarks Included,"Hi ML community,

I’m building **VulkanIlm**, a Python wrapper around llama.cpp leveraging Vulkan for GPU acceleration on legacy and AMD GPUs (no CUDA required). This opens the door to efficient local LLM use without expensive hardware.

Recent benchmark highlights:

* Dell E7250 integrated GPU (i7-5600U): 33× speedup on TinyLLaMA-1.1B chat model
* AMD RX 580 (8 GB): 4× speedup on Gemma-3n-E4B-it (6.9B params)

Inspired by Jeff Geerling’s blog on accelerating LLMs with eGPU setups on Raspberry Pi ([https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5](https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5)), I adapted and expanded it to run on AMD RX 580. A full how-to guide will come soon.

Repo here: [https://github.com/Talnz007/VulkanIlm](https://github.com/Talnz007/VulkanIlm)

Would love feedback or insights on Vulkan acceleration or similar efforts!",32,5,2025-08-11 10:01:27,Proper_Dig_6618,0.9336,0.109,0.0,0.891,-0.0259259259259259,0.4259259259259259
[D] Beyond fine-tuning and prompting for LLMs?,"I’ve been following a lot of recent LLM competitions and projects, and I’ve noticed that most solutions seem to boil down to either fine-tuning a base model or crafting strong prompts. Even tasks that start out as “generalization to unseen examples” — like zero-shot classification — often end up framed as prompting problems in practice.

From my reading, these two approaches (fine-tuning and prompting) cover a lot of the ground, but I’m curious if I’m missing something. Are there other practical strategies for leveraging LLMs that go beyond these? For example, some technique that meaningfully improve zero-shot performance without becoming “just” a better prompt?

Would love to hear from practitioners who’ve explored directions beyond the usual fine-tune/prompt spectrum.",5,6,2025-08-11 03:32:56,No-sleep-cuz-coffee,0.8952,0.136,0.057,0.807,0.0894841269841269,0.4998015873015872
"[D] Which direction is better: from academia to industry, or the other way around?","Hi all, given the current state of machine learning, I have two questions:

1. At what point in their career can a university lecturer/professor take on a joint position in industry?
2. Alternatively, can a R&D researcher in industry go back to academia without having to restart at the bottom of the ladder?

**Some context:** I am a PhD student on track to graduate in two months. I have several offers for applied/research scientist roles in industry, and interesting postdocs that could lead to a fulfilling academic career. I am not motivated by high salaries, and I know I want to do machine learning research forever! But the early-career academic job insecurity and the constant competitive grant writing I hear about are seriously concerning. At the same time, I know I can make a stronger/quicker practical impact in industry, despite the corporate constraints (work hours, less freedom, etc.). This is why I'm wondering if, in order to get the best of both worlds, one could start in academia and then transition into industry over time (or vice versa).

My question is more related to early-career researchers; I am aware that once tenure is achieved, pretty much anything is doable (e.g., Hinton, LeCun).

Thank you for sharing any insights, examples, or experiences on this :)",27,19,2025-08-11 02:56:44,PrimeMaester,0.9858,0.157,0.031,0.812,0.1534090909090909,0.3253030303030303
Validation accuracy for FER+ dataset[P],"Hey, im working on a project which involves getting 85\~90% validation accuracy for the FER+ dataset but only using shallow neural networks. I have been trying to achieve this but im stuck around 70%. Any ideas on how to make it through?",1,0,2025-08-10 18:06:42,ThatsSusG-O_o,-0.3612,0.0,0.054,0.946,-0.1666666666666666,0.75
[D] Use-case of distribution analysis of numeric features,"Hey! I hope you guys are all doing well. So, I've been deep into the statistics required in M.L. specifically. I just came to understand a few topics like

•Confidence Intervals
•Uniform/Normal distrinutions
•Hypothesis testing etc

So, these topics are quite interesting and help you analyze the numerical feature in the dataset. But here's the catch. I am still  unable to understand the actual practical use in the modeling. For example, I have a numeric feature of prices and for example it doesn't follow the normal distribution and data is skewed so I'll apply the central limit theorem(CLT) and convert the data into normal distribution. But what's the actual use-case? I have changed the actual values in the dataset as I've chosen random samples from the dataset while applying CLT and randomization will actually change the input feature right? So, what is the use-case of normal distribution? And same goes for the rest of the topics like confidence interval. How do we practically use these concepts in M.L.?


Thanks",0,2,2025-08-10 17:46:35,zunairzafar,0.9738,0.137,0.0,0.863,0.0147321428571428,0.3412946428571429
[D] Are there any papers on using reasoning models in embodied AI?,"I've been looking through papers that use LLMs for robotic control (e.g. SayCan, SayPlan etc.). Are there any papers that use reasoning models like DeepSeek R1 or o3 that do well on benchmarks?",1,5,2025-08-10 13:55:26,Limp_Food9236,0.6072,0.103,0.0,0.897,-0.1,0.2
[D] Why is scene edit detection still not at or near 100% accuracy?,"To be clear I understand nothing about the inner workings of the tool (I have a CS degree and no ML/AI background), but I've been in search of a near 100% accurate tool and can't find one.

First q, why (If you can explain like I'm a 5th grader that'd be awesome)? Genuinely curious to understand. Second q, would it be a waste of time for me to try to tackle this problem by myself (I have a lot of time on my hands lately)?

I unexpectedly got very curious and have a strong itch to at least *try* solving it, but I have no background nor any understanding of how hard such a problem would be or if it's ""worth"" trying to solve - whatever worth means.

Any insights are appreciated. Thanks :)",0,5,2025-08-10 13:51:34,Kiwis-Truths,0.9572,0.207,0.105,0.688,0.141031746031746,0.4757936507936507
[P] From GPT-2 to gpt-oss: Analyzing the Architectural Advances And How They Stack Up Against Qwen3,,95,16,2025-08-10 13:09:53,seraschka,0.0,0.0,0.0,1.0,0.0,0.0
[R] Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture,"**Contributions:**

1. **AMICL** (Associative Memory for In-Context Learning) algorithm that works in three steps:

* Identify incomplete patterns in the input
* Search context for similar, complete patterns
* Complete the pattern using the best contextual match

This achieves near-perfect performance on classification tasks.

2. Inspired by AMICL, we introduce ""**residual attention streams**"" -- direct connections between attention head values across layers. This creates information flow pathways that better retain prior context.

**Results:**

* **24% faster convergence** to 95% accuracy in two-layer Transformers on toy tasks
* **6-fold improvement** on Indirect Object Identification tasks (from \~7% to \~41% accuracy) in an 8M parameter model trained on TinyStories
* Also showed (general) improvements on **1B parameter models**

**Architecture details:**

Three variants were tested (residual streams for queries, keys, and values) and we found that the **values stream performed best**. This aligns with the AMICL model, where values directly retain input information.

The key insight is that this approach enhances in-context learning efficiency and robustness **without increasing parameter count** \- making it a computationally efficient improvement.

From a safety perspective, this enhanced in-context learning ability means AI systems can more reliably understand and follow instructions from context rather than falling back on potentially problematic patterns from training data. This work suggests that by looking to biology for inspiration, we can build AI systems that are not just more powerful and efficient, but also more trustworthy and controllable.

**Biological connections:**

It is possible to draw parallels to biological memory systems. The hippocampus has selective skip connections (direct CA3 to CA1 pathways plus indirect routes through CA2), where CA2 specialises in context-switching. This may serve similar computational functions to AMICL and the architectural modifications introduced here.

**Possible future directions:**

* Parameterised residual streams inspired by gamma-models
* Alternative attention head connection patterns
* Scaling to larger architectures
* Applications beyond NLP

**Links:**

* Paper: [https://arxiv.org/abs/2412.15113](https://arxiv.org/abs/2412.15113)
* Code: [https://github.com/tfburns/AMICL-and-residual-attention-streams](https://github.com/tfburns/AMICL-and-residual-attention-streams)

**TL;DR:**

New research shows that adding ""residual attention streams"" (direct connections between attention head values across layers) to Transformers can improve in-context learning performance while requiring no additional parameters. The approach is inspired by associative memory and has interesting parallels to hippocampal circuit architecture.",10,1,2025-08-10 08:20:54,tfburns,0.9919,0.144,0.02,0.836,0.1459415584415584,0.5314123376623378
"Any way to visualise 'Grad-CAM'-like attention for multimodal LLMs (gpt, etc.) [P]","Do anyone have ever worked on getting heatmap-like maps on what ""model sees"" using multimodal LLMs, ofcourse it must be any open-source. Any examples? Would approaches like attention rollout, attention×gradient, or integrated gradients on the vision encoder be suitable?",8,3,2025-08-10 07:15:12,ade17_in,0.594,0.09,0.0,0.91,0.55,0.75
PhDs who publish - how do you get more out of your time [D],"A little background - I'm starting my much anticipated PhD soon. It is limited to 3 years. Took some voluntary teaching duties. My ultimate target before I finish my PhD is to get really good papers out (also should a good number), build a really strong network and have excellent interpersonal skills. 

I've a question to all PhD/research you get good papers out regularly, 1-2+ first authors at good/decent conferences each year- how do you manage to do that? Did you slice up your study into mulitple publications or just really good with intuition about a method?

But often isn't it difficult to manage other duites, collaborations and also go through the arbitrary review process. I would like to know more about any experience of yours and what can you suggest someone starting out.

Edit: changed it to 1-2+ publications each year",88,33,2025-08-10 07:05:33,ade17_in,0.9419,0.122,0.009,0.868,0.2473370927318295,0.5032340466550993
[D] how gpt-oss-20b can load in a GPU with only 16 GB of VRAM?,"I haven't tried to run it yet on PyTorch, but I don't see how we can load 20B parameters with 2 bytes per parameter (torch.bloat16) in a GPU with only 16GB of VRAM

I was assuming that for every forward pass, it will move the experts weights to the GPU. Although as much as I cannot believe that because it is not efficient, I was tempted to the theory because 20B \* 2 bytes (torch.bfloat16) / (1024 byte->kilobyte / 1024 kilboyte->megabyte / 1024 megabyte->gigabyte) \\approx 39,1 GB of VRAM, just to load the model

Is this because of quantization using MXFP4?

How on earth gpt-oss-20b with 4-bit quantization can have on par performance with DeepSeek R1 (671B)?

[model.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/model.py)

[weights.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/weights.py)

[llm-stats.com](https://llm-stats.com/)

Edit: README says it all

\> [`torch`](https://github.com/openai/gpt-oss/blob/main/README.md#reference-pytorch-implementation) — a non-optimized [PyTorch](https://pytorch.org/) implementation for educational purposes only. Requires at least 4× H100 GPUs due to lack of optimization.

[README.md](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/README.md)",7,5,2025-08-10 07:02:33,kidfromtheast,-0.4745,0.022,0.042,0.936,0.0035714285714285,0.6035714285714285
[D] Reminder that Bill Gates's prophesy came true,,3488,397,2025-08-10 01:18:42,we_are_mammals,0.4215,0.286,0.0,0.714,0.35,0.65
[D] open source speech to speech (Voice Agent) model?,"Is there an open source speech to speech (Voice Agent) model, like Amazon Nova Sonic?",0,1,2025-08-09 22:45:50,Powerful-Angel-301,0.5514,0.172,0.0,0.828,0.0,0.5
[D]Help running IDM-VTON (virtual try-on) locally or on Colab – hitting memory issues and need alternatives,"Hi everyone,

I’m trying to run this project from GitHub: [https://github.com/yisol/IDM-VTON](https://github.com/yisol/IDM-VTON)  
My goal is to study how it works and understand how clothes adapt so realistically to different bodies.

Here’s what I’ve tried so far:

* Followed the README exactly on my laptop (no GPU) → not usable because of hardware limits.
* Cloned it to Google Colab → initially had dependency issues, solved them with Miniconda in Colab.
* Now, when running `gradio_demo/app.py`, the process gets **Killed** (out-of-memory).

please Suggestions for running this project without a local GPU.

Any tricks for optimizing memory usage in Colab.

Alternative tools or platforms?

I’m fine with paid or free solutions as long as they let me test and understand the code.

Has anyone here successfully run IDM-VTON or a similar Stable Diffusion-based try-on model without a powerful GPU?

All I want is to be able to run this project, test it, play with the code, and see the results. If you know of any alternative or platform adapted to my problem, I would greatly appreciate it.",2,0,2025-08-09 19:53:09,Altruistic-Front1745,0.9526,0.134,0.033,0.833,0.1572916666666666,0.5171875000000001
[D] What would a measurable test for minimal AI welfare look like?,"I’m collecting operational criteria (not metaphysics): cross-session behavioral consistency, stable self-reports under blinded probes, reproducible third-party protocols. Looking for papers, metrics, or eval harnesses you’d use to *falsify* these.",0,4,2025-08-09 18:07:22,HelenOlivas,0.5719,0.11,0.0,0.89,-0.1,0.6
"[P] I used YOLOv12 and Gemini to extract and tag over 100,000 scientific plots.","For anyone who works in research, the process of designing effective data visualizations can be a significant bottleneck. I often found myself searching through numerous papers just to find inspiration for layouts and plot types, which was inefficient.

To solve this problem for myself and others, I developed [**Plottie.art**](http://Plottie.art), a searchable, browser-based library of over 100,000 plots curated from scientific literature.

I'm sharing it here because the machine learning pipeline behind it combines a specialized computer vision model with an LLM in a way that I thought this community would find interesting.

**The ML Pipeline**

The process starts with a large collection of figure images sourced from open-access papers. The goal is to make each individual plot within these figures searchable.

**1. Subplot Segmentation with a Custom YOLOv12 Model**

A key challenge is that many figures are multi-panel, containing several distinct subplots within a single image.

* **Model Training:** To address this, I trained a custom **YOLOv12 model**. This required **manually annotating a dataset of 1,000 images** to teach the model to accurately identify and isolate the boundaries of individual subplots and their captions.
* **Function:** The model processes each source image and outputs bounding boxes for each subplot, effectively segmenting complex figures into their constituent parts.

**2. Plot Classification and Keyword Extraction with Gemini**

With the subplots isolated, the next step was to classify each image by plot type (e.g., heatmap, UMAP) and extract relevant keywords for search.

* **Approach:** While I considered training another dedicated classification model, the data collection and labeling requirements would have been substantial. I opted for a more efficient approach using a large multimodal model.
* **Implementation:** I utilized the **Google Gemini API**. By providing a subplot image, I could prompt the model to perform both classification and keyword extraction. A prompt structured like, `""Analyze this scientific plot. Identify its specific type and extract key terms from its labels and content.""` proved to be highly effective.
* **Outcome:** This method was not only fast to implement but also yielded high-quality, structured metadata. It successfully bypassed the need for a separate, time-intensive training pipeline for classification.

This two-stage pipeline allows the content on[**Plottie.art**](https://plottie.art)to be easily searched and explored. The tool is free, requires no login, and runs in the browser.

I would be very interested to hear your feedback on the project and the technical stack. I'm especially curious about any thoughts on combining specialized vision models with general-purpose LLMs for this type of application, or suggestions for improving the pipeline.

",48,9,2025-08-09 17:49:07,mert_jh,0.9884,0.116,0.018,0.866,0.2012648809523809,0.5761904761904763
[R] A quick question to Mathematica + LLM users,"Hi everyone, I am wondering if it’s worth to buy the [Mathematica + LLM in notebook](https://writings.stephenwolfram.com/2023/06/introducing-chat-notebooks-integrating-llms-into-the-notebook-paradigm/) so it would be great if anyone who has it could paste this [question](https://pastebin.com/aynsiWrc) into the mathematica LLM. I’ve put it on pastebin, because reddit will mess up the string with its own formatting. But if you do not wish to click I paste it here, but the ^ will mess up, so use the pastebin to paste it into LLM:

> Let V be a vector field on an affine space A generating a flow \phi, let \Psi:A->A be any smooth invertible map with smooth inverse, and let \Phi(t,x)=\Psi(\phi(t,\Psi^{-1}(x))). Show that \Phi is also a flow on A, and that its generator V^\Psi is given by V^\Psi_x=\Psi_*(V_{\Psi^{-1}(x)}).

It’s a kind of problem which can be done with pen & paper and I am not sure if mathematica is useful here.

Would be great if someone can post a screenshot of the answer from mathematica. I am trying to figure out if these types of problems are applicable to mathematica + LLM.

The problem is from book by Crampin, Pirani “Applicable Differential Geometry”, 1987, page 64 Exercise 28.

So far I used the Bing LLM for it, and it gave the correct answer. Including the derivations, calculations and simplifications of the formulas.",0,0,2025-08-09 17:39:02,cosurgi,-0.8105,0.058,0.094,0.848,0.3102564102564102,0.5568376068376069
[D] GPT5 is pretty bad with information extraction tasks,,52,8,2025-08-09 17:30:15,sleepshiteat,-0.0772,0.234,0.255,0.511,-0.2249999999999999,0.8333333333333333
[D] What happens if reviewers don't fill out the mandatory acknowledgement in NeurIPS 2025?,2 of my reviewers completely ghosted the discussion period. Wondering what happens next?,16,16,2025-08-09 16:16:14,tedd235,0.168,0.062,0.0,0.938,0.05,0.2
[D] How do researchers ACTUALLY write code?,"Hello. I'm trying to advance my machine learning knowledge and do some experiments on my own.  
Now, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs.  
It's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch ""debugging"" is good old python print statements.  
My workflow is the following: have an idea -> check if there is simple hugging face workflow -> docs have changed and/or are incomprehensible how to alter it to my needs -> write simple pytorch model -> get simple data from a dataset -> tokenization fails, let's try again -> size mismatch somewhere, wonder why -> nan values everywhere in training, hmm -> I know, let's ask chatgpt if it can find any obvious mistake -> chatgpt tells me I will revolutionize ai, writes code that doesn't run -> let's ask claude -> claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -> ok, print statements it is -> cuda out of memory -> have a drink.  
Honestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&b panels, there are for finetuning your training, and that requires training to actually work.

  
Edit:  
There are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold...",159,123,2025-08-09 11:22:53,Mocha4040,0.9547,0.095,0.018,0.888,0.2333333333333333,0.5429747207524984
[D] Neurips 2025 being hosted at 3 locations.,Neurips 2025 is being hosted at three different locations this time around: 1) San Diego; 2) Mexico City; 3) Copenhagen. What is your opinion on this?,58,30,2025-08-09 07:37:50,casualcreak,0.0,0.0,0.0,1.0,0.0,0.6
[P] We just open-sourced the first full-stack Deep Research: agent + model + data + training—reproducible GAIA 82.4,"https://i.redd.it/b9goy7brywhf1.gif



We’re releasing **MiroMind Open Deep Research (ODR) v0.1**, which we believe is the **first** ***full-stack*****, fully open-source deep research project**—not just an agent, but also the **model, dataset, and training/RL infra** are open and reproducible. The agent framework (**MiroFlow**) reproduces **82.4** on **GAIA validation**; the model series (**MiroThinker**) reaches **60.2%** on **GAIA-Text-103**. Looking for contributors + repro logs.

# Why this matters

* **Full-stack openness**: most deep-research releases stop at the agent; ODR opens **all four layers**: **Agent (MiroFlow)**, **Model (MiroThinker)**, **Data (MiroVerse)**, **Training/RL (MiroTrain / MiroRL)**. 
* **Reproducible numbers**: • **MiroFlow**: GAIA validation **maj. vote 82.4**, pass@1 avg@3 **72.2** (with setup details & scripts). • **MiroThinker v0.1**: **60.2%** on **GAIA-Text-103** (with both SFT & DPO variants across 8B/14B/32B).
* **Open data at scale**: **MiroVerse v0.1**—**147k+** full rollout trajectories (**\~1.9B tokens, 602k+ tool calls**), built for tool-use/web-browsing agents.

# What’s included

* **MiroFlow (Agent framework)** – multi-tool, sub-agent orchestration, MCP integration, benchmarking UI; detailed GAIA runs & scripts.
* **MiroThinker (Model series)** – agentic LLMs optimized for deep research; SFT/DPO at 8B/14B/32B with evaluation guides.
* **MiroVerse (Dataset)** – 147k+ verified trajectories across multi-hop QA, browsing, scientific reasoning; hybrid licensing noted on card.
* **MiroTrain / MiroRL (Training & RL)** – end-to-end post-training + MCP-first RL for tool-using agents.

# Quick start (agent eval)

1. **MiroFlow**: clone, set keys (OpenRouter/Anthropic/OpenAI/Gemini, Serper, Jina, E2B), optional E2B Docker sandbox for stable repro; run GAIA scripts.
2. **MiroThinker**: pull model from HF or self-host via SGLang; run GAIA-Validation / GAIA-Text-103 / HLE / WebWalkerQA scripts.

# Links

* **Overview blog (tables & results)**: [miromind.ai/blog/miromind-open-deep-research](http://miromind.ai/blog/miromind-open-deep-research) [MiroMind](https://miromind.ai/blog/miromind-open-deep-research)
* **Agent**: [GitHub.com/MiroMindAI/MiroFlow](http://GitHub.com/MiroMindAI/MiroFlow) [GitHub](https://github.com/MiroMindAI/MiroFlow)
* **Models**: [GitHub.com/MiroMindAI/MiroThinker](http://GitHub.com/MiroMindAI/MiroThinker) & HF collection [GitHub](https://github.com/MiroMindAI/MiroThinker)[Hugging Face](https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1)
* **Dataset**: HF — miromind-ai/MiroVerse-v0.1 [Hugging Face](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)
* **Training/RL**: [GitHub.com/MiroMindAI/MiroTrain](http://GitHub.com/MiroMindAI/MiroTrain) & /MiroRL [GitHub+1](https://github.com/MiroMindAI/MiroTrain)

# ",24,2,2025-08-09 03:32:34,flyforlight,0.6239,0.034,0.012,0.954,-0.0083333333333333,0.6533333333333334
[R] Adaptive Classifiers: Few-Shot Learning with Continuous Adaptation and Dynamic Class Addition,"**Paper/Blog**: [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)  
**Code**: [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)  
**Models**: [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)

# TL;DR

We developed an architecture that enables text classifiers to:

* Learn from as few as 5-10 examples per class (few-shot)
* Continuously adapt to new examples without catastrophic forgetting
* Dynamically add new classes without retraining
* Achieve 90-100% accuracy on enterprise tasks with minimal data

# Technical Contribution

**The Problem**: Traditional fine-tuning requires extensive labeled data and full retraining for new classes. Current few-shot approaches don't support continuous learning or dynamic class addition.

**Our Solution**: Combines prototype learning with elastic weight consolidation in a unified architecture:

    ModernBERT Encoder → Adaptive Neural Head → Prototype Memory (FAISS)
                                        ↓
                                EWC Regularization
    

**Key Components**:

1. **Prototype Memory**: FAISS-backed storage of learned class representations
2. **Adaptive Neural Head**: Trainable layer that grows with new classes
3. **EWC Protection**: Prevents forgetting when learning new examples
4. **Dynamic Architecture**: Seamlessly handles new classes without architectural changes

# Experimental Results

Evaluated on 17 diverse text classification tasks with only 100 examples per class:

**Standout Results**:

* Fraud Detection: 100% accuracy
* Document Classification: 97.5% accuracy
* Support Ticket Routing: 96.8% accuracy
* **Average across all tasks**: 93.2% accuracy

**Few-Shot Performance**:

* 5 examples/class: \~85% accuracy
* 10 examples/class: \~90% accuracy
* 100 examples/class: \~93% accuracy

**Continuous Learning**: No accuracy degradation after learning 10+ new classes sequentially (vs 15-20% drop with naive fine-tuning).

# Novel Aspects

1. **True Few-Shot Learning**: Unlike prompt-based methods, learns actual task-specific representations
2. **Catastrophic Forgetting Resistance**: EWC ensures old knowledge is preserved
3. **Dynamic Class Addition**: Architecture grows seamlessly - no predefined class limits
4. **Memory Efficiency**: Constant memory footprint regardless of training data size
5. **Fast Inference**: 90-120ms (comparable to fine-tuned BERT, faster than LLM APIs)

# Comparison with Existing Approaches

|Method|Training Examples|New Classes|Forgetting|Inference Speed|
|:-|:-|:-|:-|:-|
|Fine-tuned BERT|1000+|Retrain all|High|Fast|
|Prompt Engineering|0-5|Dynamic|None|Slow (API)|
|Meta-Learning|100+|Limited|Medium|Fast|
|**Ours**|**5-100**|**Dynamic**|**Minimal**|**Fast**|

# Implementation Details

Based on ModernBERT for computational efficiency. The prototype memory uses cosine similarity for class prediction, while EWC selectively protects important weights during updates.

**Training Objective**:

    L = L_classification + λ_ewc * L_ewc + λ_prototype * L_prototype
    

Where L\_ewc prevents forgetting and L\_prototype maintains class separation in embedding space.

# Broader Impact

This work addresses a critical gap in practical ML deployment where labeled data is scarce but requirements evolve rapidly. The approach is particularly relevant for:

* Domain adaptation scenarios
* Real-time learning systems
* Resource-constrained environments
* Evolving classification taxonomies

# Future Work

* Multi-modal extensions (text + vision)
* Theoretical analysis of forgetting bounds
* Scaling to 1000+ classes
* Integration with foundation model architectures

The complete technical details, experimental setup, and ablation studies are available in our blog post. We've also released 17 pre-trained models covering common enterprise use cases.

**Questions welcome!** Happy to discuss the technical details, experimental choices, or potential extensions.",21,7,2025-08-09 02:05:08,asankhs,0.7523,0.052,0.035,0.913,0.044090909090909,0.4818030303030302
[D] - What AI Engineers do in top companies?,"Joined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work. 

When I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently.

Feel free to answer.",149,58,2025-08-08 21:10:25,_crazy_muffin_,0.836,0.167,0.03,0.802,0.1222222222222222,0.5222222222222223
[D]papers on graph neural networks,What are the 10 most impactful ml papers on graph neural networks,0,3,2025-08-08 20:14:15,pythonprogrammer64,0.0,0.0,0.0,1.0,0.5,0.5
"Managing GPU jobs across CoreWeave/Lambda/RunPod is a mess, so im building a simple dashboard[P]","If you’ve ever trained models across different GPU cloud providers, you know how painful it is to:

* Track jobs across platforms
* Keep an eye on GPU hours and costs
* See logs/errors without digging through multiple UIs

I’m building a super simple “Stripe for supercomputers” style dashboard (fake data for now), but the idea is:

* Clean job cards with cost, usage, status
* Logs and error previews in one place
* Eventually, start jobs from the dashboard via APIs

If you rent GPUs regularly, would this save you time?  
What’s missing for you to actually use it?",10,1,2025-08-08 18:13:54,NoTap8152,0.5323,0.096,0.084,0.82,-0.0729166666666666,0.4152396214896214
"[P] Explaining GNN Predictions on """"linear"""" DFGs - GNN experts I need your help <3","I’m working on a research project where, starting from an event log, I build for each trace a Direct Follows Graph (DFG) representing that trace, where each node corresponds to an activity.

My goals are:

1. From the obtained DFGs, derive Prefix graphs (i.e., DFGs with the final nodes removed) and apply a GNN for **next activity prediction at the node level**. This way, if I feed the model a list of activities during inference, it should return the next activity.
2. Given the prediction, I want to apply **GNN explainability techniques**, specifically *Perturbation-based methods*and *Surrogate-based methods*, to explain the model’s decision.

My question is mainly about point 2: since the DFGs are mostly linear (with at most some self-loops or a few normal loops), does it make sense to search for subgraphs that explain the result (e.g., with GNNExplainer or SubgraphX)? For example, if I use a 3-layer GNN, wouldn’t the prediction already be fully explained by the 3-hop neighborhood?  
These are not very large graphs with huge numbers of edges... maybe I’m missing something.

P.S.: I’m new in the world of GNNs.",0,0,2025-08-08 15:47:29,Street_Car_1297,0.7476,0.049,0.011,0.939,0.1647075147075146,0.4211699411699411
[D] Looking for convex-constrained ML problems for benchmarks,"Hello,
  
I am looking for Machine Learning (ML) use cases to try out a class of optimization algorithms, namely Frank Wolfe (FW) algorithms. Those are *gradient-based* and *projection-free* algorithms for optimizing a cost function (convex or non-convex) over a *convex* set of constraints. Usually, such problems are tackled by Projected Gradient Descent (PGD), where each iteration consists of a descent in the direction of the gradient, then a projection onto the set of constraints to ensure that the new solution is feasible. However, depending on the set of constraints, this projection step can be very costly and thus prohibitive. FW algorithms avoid this projection step, which leads to less compute-intensive iterations.
  
I am turning toward r/machinelearning communities for ideas of problems that satisfy those conditions: optimization over a convex set of constraints (original or relaxed version of a problem), ideally that can be large-scale so I can push the FW algorithms to their limits.

For the moment, I found those following problems:

  * **Adversarial attack** : modifying an image in a imperceptible way for a human so that a classifier misclassifies it. The modification 𝛿 can be constrained in the 𝜀-ball so that it remains small, which is a convex set so it fits the description.

  * **Polynomial Regression**/**Compressed Sensing**: when we need a sparse represention, we can set the constraint that the coefficients live in the L1-norm ball that is sparsity-inducing.

  * **Matrix Completion**: not the original formulation that constrain that the rank of the matrix *X* denoted rank(*X*) is low, but setting a constraint of the nuclear-norm value of the matrix *X*, which is a convex constraint.

I am also looking for optimization over the set of Doubly Stochastic Matrices (also called the Birkhoff polytope, which is the convex hull of permutation matrices), but I've been looking for a few hours on Google and I haven't found any concrete application, so if you have any ideas I will gladly take them. I've heard that they are useful in matching/assignment problems.

Thanks for reading",7,10,2025-08-08 14:55:45,Ttghtg,0.9453,0.094,0.053,0.853,0.1150392817059483,0.4637485970819303
"[R] Live coding benchmark: GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, GLM45 — same prompt, varying difficulty","We’re running a live comparative test today to see how four leading LLMs handle coding tasks in a natural-language coding environment.

**Models tested:**

* GPT-5
* Claude Sonnet 4
* Gemini 2.5 Pro
* GLM45 (open-source)

**Format:**

* All models receive **the exact same prompt**
* Multiple runs at different complexity levels:
   * Simple builds
   * Bug-fix tasks
   * Multi-step complex builds
   * Possible planning flows

We’ll compare:

* Output quality
* Build speed
* Debugging performance

**When:** Today, 16:00 UTC (19:00 EEST)

**Where:** [https://live.biela.dev](https://live.biela.dev) 

Hop in with questions, curiosities, prompt suggestions and whatever comes in mind to make the test even better! :)",0,11,2025-08-08 14:03:12,darkageofme,0.5848,0.057,0.022,0.922,0.112310606060606,0.4464285714285714
[D] Looking for ideas for a ML initiative,"Hi all,

My goal is to launch a small ML initiative/lab that:

* Focus on non-mainstream but high-impact ML research areas.
* Work on project-driven open-source contributions and papers from day one
* Build a network and reputation through real, tangible outputs rather than just theory or coursework

I want this to be lean and agile, not a formal institution, but a focused group of people (starting small) who want to push boundaries and build a reputation in underexplored domains.

**What I’m looking for:**

* Suggestions on promising underexplored ML fields or projects with potential real-world impact
* Advice on structuring such a lab efficiently (collaboration tools, workflow, open-source best practices)
* Potential collaborators interested in contributing to projects with measurable outputs
* Any pitfalls to watch out for in early-stage lab building

**Conditions I’m considering:**

1. Projects must be open-source and reproducible.
2. Research and code contributions should aim for quality over quantity.
3. Members commit to regular updates and active communication.
4. We focus on non-mainstream areas to avoid crowded research spaces.
5. All contributions must align with ethical standards.
6. Aim for publishable or demonstrable outcomes, no just “exploratory” hacks.
7. Small core team at first (3-5 people max) to stay agile.
8. Clear documentation and modular code required from day one.

Would appreciate any concrete ideas or feedback. Also open to recommendations on platforms or tools that could help us run this smoothly.",0,4,2025-08-08 10:10:00,Horror_Job_566,0.9902,0.166,0.021,0.812,0.1365079365079365,0.4854090354090354
[D] Neurips rebuttal score change,"

It's just my feeling, but from what I see, the post rebuttal score this year maybe higher than previous year. Can everyone share how the score change so far for the paper that you review? 

In my case, I know 9 paper reviewed by me and my friend, 4 get their score increase (1 increases by 1, the rest a lot more), 1 withdraw, 1 likely to decrease by 1, the rest didn't change",29,67,2025-08-08 09:08:32,Careless-Top-2411,0.8834,0.142,0.0,0.858,0.1366666666666666,0.6333333333333333
[D] Disentanglement using Flow matching,"Hi, 

I’ve been considering flow matching models to disentangle attributes from an embedding. The idea stems from the fact that flow matching models learn smooth and invertible mappings.

Consider a pre-trained embedding E, and disentangled features T1 and T2. Is it possible to learn a flow matching model to learn this mapping from E to T1 and T2 (and vice versa)?

My main concerns are -
1. Distribution of E is known since its source distribution. But T1 and T2 are unknown. How will the model learn when it has a moving or unknown target?
2. I was also wondering if some clustering losses can enable this learning?
3. Another thought was to use some priors, but I am unsure as to what would be a good prior. 

Please suggest ideas if this wouldnt work. Or advancements on this if it does.

Prior work:
A paper from ICCV 25 (“SCFlow”) does disentanglement using flow matching. But, they know the disentangled representations (Ground truth is available). So they provide T1 or T2 distributions to the model alternatively and ask it to learn the other. ",17,3,2025-08-08 06:35:29,southern_brownie,0.6416,0.054,0.032,0.914,0.1341666666666666,0.4408333333333333
"[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc","Greetings,

At work, I am currently building a very simple document summarization platform that takes in source documents, produces small and concise summaries of the documents, and storing them in a database.

The project plans to expand to a lot of other functionalities later on, but for the moment I've been asked to determine a way to ""grade"" or ""analyze"" the generated summaries against the original source text and give it a score, as an aid for some of our human reviewers.

I've been working on this for about a week, and have tried various methods like BERTScore, MoverScore, G-eval, ROGUE, BLEU and the like. And I've come to the conclusion that the scores themselves don't tell me a lot, at least personally (which could simply be due in part to me misunderstanding or overlooking details). For example I understand cosine similarity to a degree, but it's hard to put into context of ""grade this summary."" I've also tried out an idea about sending the summary to another decoder-only model (such as Qwen or even Phi-4), asking it to extract key facts or questions, then running each of those through a BERT NLI model against chunks of the source material (checking ""faithfulness"" I believe). I also thought about maybe doing some kind of ""miniature RAG"" against a single document and seeing how that relates to the summary itself, as in to find gaps in coverage.

For the most part, I wasn't disappointed in the results but I also was not thrilled by them either. Usually I'd get a score that felt ""middle of the road"" and would be difficult to determine whether or not the summary itself was good.

So my question is: Does anyone here have any experience with this and have any suggestions for things to try out or experiment with? I feel like this might be a large area of ongoing research as is, but at this point we (where I work) might actually just be striving for something simple.

Thanks!",9,10,2025-08-08 00:07:35,IThrowShoes,0.9491,0.079,0.032,0.889,-0.0079931972789115,0.4537840136054422
[D] Can LLMs Have Accurate World Models?,"I have seen many articles (one example https://aiguide.substack.com/p/llms-and-world-models-part-1) stating that LLMs have no coherent/effective world models and because of this their accuracy is inherently limited. Can this obstacle be overcome, and if not why?",42,48,2025-08-07 22:48:30,NandoGando,-0.7149,0.0,0.158,0.842,0.2761904761904762,0.4253968253968254
[R] CRINN: Free & Fast Framework for Approximate Nearest Neighbors Search,"Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN’s effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN’s success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.

[https://github.com/deepreinforce-ai/CRINN](https://github.com/deepreinforce-ai/CRINN)",16,1,2025-08-07 22:18:59,Optimal-Outcome-7458,0.9794,0.19,0.036,0.774,0.1902020202020202,0.4847474747474747
[D] LSTMs vs Transformers (Model Selection and Thoughts),"I wanted to have a discussion along the following lines. Lets say there is a scenario where the advantage of parallelism is no longer present. Then for an NLP task which model would you prefer an LSTM or a transformer? Lets assume the size of both models in terms of parameters is also the same. I have consulted 4o, claude sonnet, gemini flash 2.5 and grok 3 as well. Posting their responses in the comments. The question is around how to think about different models and their advantages. I feel like nowadays throwing a transformer is the first thing people do.",0,6,2025-08-07 19:59:44,bababhaukali,0.7096,0.085,0.021,0.895,0.05,0.2316666666666666
[D] Unsaturated Evals before GPT5,"Ahead of today’s GPT-5 launch, I compiled a list of unsaturated LLM evals. Let's see if GPT-5 can crack them.

link: [https://rolandgao.github.io/blog/unsaturated\_evals\_before\_gpt5](https://rolandgao.github.io/blog/unsaturated_evals_before_gpt5)  
x post: [https://x.com/Roland65821498/status/1953355362045681843](https://x.com/Roland65821498/status/1953355362045681843)

https://preview.redd.it/t3cwiitotjhf1.png?width=1302&format=png&auto=webp&s=098a7f2092afdf436a2699104accc49d01909f19",19,8,2025-08-07 07:29:35,Roland31415,0.0,0.0,0.0,1.0,-0.75,1.0
[P] Reproducing YOLOv1 From Scratch in PyTorch - Learning to Implement Object Detection from the Original Paper,"Hey everyone,

I have recently reproduced **YOLOv1** entirely from scratch using **PyTorch**, as a self-driven project to dive deeper into object detection and research implementation

**What I implemented**

YOLOv1 CNN architecture (paper-faithful)

Custom loss function (localization, confidence, classification)

IoU calculations and grid transformations

Forward pass and inference pipeline (with visualization)

Modular structure and utilities

**Training hasn’t been done yet** although I have a GPU it is taking a long time, but the pipeline is fully written, ready for VOC or a custom dataset.

**GitHub repo:**

[https://github.com/aayan873/YOLOv1-from-Scratch-My-First-Paper-to-Code-Project/](https://github.com/aayan873/YOLOv1-from-Scratch-My-First-Paper-to-Code-Project/)",12,0,2025-08-07 07:14:49,No-Economist146,0.7325,0.088,0.016,0.896,0.105,0.505
"[D] Idea for an efficient text diffusion model with adaptive, token-level steps","Hi r/MachineLearning,

I've been thinking about the inefficiency of using a fixed number of inference steps in text diffusion models. It seems wasteful to use the same amount of compute for a simple sentence as for a complex one.

I've prototyped an alternative architecture I'm calling ""Adaptive Refinement Diffusion,"" and I'd love your feedback on it.

The core idea is:

* Instead of a fixed loop, the model iteratively refines the sequence.
* At each step, it calculates a confidence score for every token (based on a mix of its embedding stability and prediction probability).
* If a token's score passes a certain threshold, it gets ""frozen"" and is excluded from future computation.
* The entire generation process stops dynamically once all tokens in the sequence are frozen.

This means the model would naturally focus compute on the more difficult or ambiguous tokens and could finish simple sentences much faster.

My questions for the community are:

1. Does this architecture already exist? I've searched for prior work but haven't found this specific token-level freezing mechanism.
2. What potential flaws or failure modes do you see with this approach?

Appreciate any thoughts or links to related papers. Thanks!",2,8,2025-08-07 06:40:29,MokshMalik,0.8154,0.1,0.051,0.849,0.0423809523809523,0.3842857142857143
[D] Training Whisper Tiny,"I am trying to build an on device speech recognition engine for recognising kids’ voice better replacing speech framework I am using in my ios app right now.

To do this, I collect sample audio data from my app keeping the privacy concerns in mind and transcribe these audio files with whisper large v2 and then using it as pseudo labelling to train  whisper tiny. 

I have following questions now:

1. Is this a valid strategy or with low parameters  of whisper tiny this is a futile exercise no matter how much I train it?

2. Most of my data is not clean, meaning background and other noise is interspersed with kids’ speech. But it’s also important for my app to be accurate in these environment.

3. How many hours of audio I need to train it on  keeping the above audio quality in mind to achieve reasonable accuracy?

4. Are there better solutions?",7,5,2025-08-07 04:15:53,Realistic_Public_415,0.7106,0.073,0.042,0.885,0.1884259259259259,0.4707010582010582
[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything?,"If so, link the paper and the result. Very curious about this. Not even just metrics like accuracy, have BDL methods actually achieved better results in calibration or uncertainty quantification vs say, deep ensembles?",95,56,2025-08-07 02:07:58,35nakedshorts,0.7143,0.161,0.046,0.792,0.074,0.48
[D] FP4 training methods (request for paper recommendations),"The new OSS models by OpenAI have low precision weights (MXFP4). Does anyone know:

- Is it likely that they were trained with MXFP4?

- Could anyone recommend papers on how to train models in such a low precision? Is it possible to train with SGD in such a low range, i.e. FP4, has just 16 values?

- Is it possible to go even lower? I.e. FP3 or FP2?

",6,7,2025-08-06 21:17:56,ArtisticHamster,-0.504,0.064,0.117,0.818,0.0151515151515151,0.5949494949494949
[R] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models,"**TL;DR**: [Soft tokens](https://www.arxiv.org/abs/2505.15778) (probabilities-weighted sum over vocab) actually underperform traditional ""hard"" tokens. But a Gumbel-Softmax trick can salvage this issue.

**Paper:** [https://www.arxiv.org/pdf/2508.03440](https://www.arxiv.org/pdf/2508.03440)

**Abstract:**

>Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the \`Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.

**Visual Highlights:**

https://preview.redd.it/zza3t8r17fhf1.png?width=1099&format=png&auto=webp&s=e12815cb0774bce2a2614b2c3ad0df47b071d8c8

https://preview.redd.it/lulzrar27fhf1.png?width=1109&format=png&auto=webp&s=0fd5cd8dc90a9c09afb46dbd8e0412a72800dbe3

",22,0,2025-08-06 15:48:20,StartledWatermelon,0.9325,0.091,0.04,0.87,0.1037574404761904,0.4588169642857143
[R] Trainable Dynamic Mask Sparse Attention,"https://preview.redd.it/v3nxbno7xbhf1.png?width=1280&format=png&auto=webp&s=4a425509b0c20e16992c7998392567ff534a9b02

Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! 🤗

* **Blog Post (The TL;DR):** [https://hf.co/blog/wubingheng/dmattn](https://hf.co/blog/wubingheng/dmattn)
* **Paper (The Nitty-Gritty):** [https://huggingface.co/papers/2508.02124](https://huggingface.co/papers/2508.02124)
* **Code (The Good Stuff):** [https://github.com/SmallDoges/flash-dmattn](https://github.com/SmallDoges/flash-dmattn)",4,0,2025-08-06 04:48:07,BitExternal4608,0.8883,0.219,0.0,0.781,0.3999999999999999,0.5555555555555556
[D] GSPO: Qwen3’s sequence-level RLHF method vs. GRPO - stability & scaling analysis,"The Qwen team recently proposed **Group Sequence Policy Optimization (GSPO)**, a reinforcement learning approach for post-training LLM fine-tuning. They position it as an alternative to **Group Relative Policy Optimization (GRPO)** \- used in DeepSeek - and claim GRPO’s token-level importance sampling is “ill‑posed” for stable training.

**Background:**

* Popular RLHF methods (e.g. PPO) optimize LLMs via reward signals.
* DeepSeek’s GRPO extends this by computing sample-level value estimations.
* Qwen reports that GRPO often triggers gradient instability and model collapse unless patched with complex adjustments.

**Key concerns with GRPO:**

* Applies importance sampling **per token**, accumulating high variance across long sequences.
* Particularly problematic for **Mixture-of-Experts (MoE)** models, where token-level routing shifts can destabilize training.
* To counteract this, GRPO-based pipelines often rely on strategies like **Routing Replay**.

**GSPO’s proposal:**

* Moves to **sequence-level importance sampling**, normalizing by sequence length.
* Dramatically reduces variance and eliminates the need for routing hacks.
* Qwen reports stable MoE convergence and better scaling.

**Findings from experiments:**

* On benchmarks such as AIME’24, LiveCodeBench, and CodeForces, GSPO achieves better reward curves than GRPO.
* GSPO converges faster with more compute and shows smoother scaling trends.
* GRPO requires Routing Replay to perform adequately; GSPO does not.

If you're interested, read more about it here: [Qwen Team Proposes GSPO for Qwen3, Claims DeepSeek's GRPO is Ill-Posed](https://blog.netmind.ai/article/Qwen_Team_Proposes_GSPO_for_Qwen3%2C_Claims_DeepSeek's_GRPO_is_Ill-Posed). The blog post includes mathematical formulations of both methods and performance comparisons.

I’m interested to know:

* Whether anyone in the community has observed instability with token-level importance sampling or GRPO?
* Has sequence-level weighting like GSPO been tested in your RLHF pipelines?",70,4,2025-08-06 12:50:47,MarketingNetMind,0.9863,0.162,0.037,0.801,0.1477777777777777,0.4809259259259259
[D] Do you think LLM memory will ever be solved without fine‑tuning?,"I’ve been running into the same issue again and again while working with LLMs: they forget. You can stuff the history into the prompt, set up a RAG pipeline, or go through fine‑tuning, but none of these feel like a real solution.

Because of that frustration, I started exploring memory management myself, more like giving models “on‑demand context” instead of retraining them. It’s early, but it made me realize how huge and unexplored this space is.

I’m wondering if others here have felt the same pain. How are you approaching memory in your projects, and do you think we’ll ever see something beyond the RAG/fine‑tuning combo?",14,54,2025-08-06 12:43:19,shbong,0.7768,0.133,0.075,0.792,0.1714285714285714,0.3214285714285714
[D] Is modern academic published zero-sum?,"It seems the current state of publishing in A* venues (CVPR, NeurIPS, ICML, ICCV/ECCV) is zero-sum. One person’s rejection is another person’s acceptance. Reviewers seem to reject papers just for the sake of rejection. There’s a sense that some reviewers reject papers not on substantive grounds, but out of an implicit obligation to limit acceptance rates. Rebuttals appear to be pointless as reviewers take stubborn positions and not acknowledge their misunderstandings during this period. Good science just doesn’t appear to be as valued as the next flashiest LLM/VLM that gets pretty results.",159,28,2025-08-06 00:20:44,bigbird1996,0.85,0.154,0.101,0.745,0.1285714285714285,0.3999999999999999
[P] From Business Processes to GNN for Next Activity Prediction,"I’m quite new to GNNs and process mining, and I’m trying to tackle a project that I’m really struggling to structure. I’d love your input, especially if you’ve worked with GNNs or process data before.

I have a CSV file representing a business process (specifically a Helpdesk process). From this CSV, I want to build a graph representation of the process (specifically a Directly-Follows Graph). Then, I want to train a GNN to do **next activity prediction** at the **node level**.

The idea is: given a *prefix graph* (i.e., a pruned version of the full process graph up to a certain point), I want the model to predict the label of the next activity, corresponding to the node that would logically come next in the process.

I’ve found very little literature on this, and almost no practical examples. I have a few specific doubts I hope someone can help me with.

1. **Model choice**: It's a dataset made of 4580 graphs (traces), 7 average nodes each, 15 total labels (activities). I was thinking of using a 3-layer GCN for the prediction task. Does this make sense for my use case? Are there better architectures for sequence-based node prediction in process graphs?
2. **Multiple process instances (graphs)**:As I said, I have 4580 different instances of the process, each one is essentially a separate graph. Should I treat them as 4580 **separate graphs** during training, or should I **merge them into one big graph** (while preserving per-node instance information somehow)?My concern is about how GNNs typically work with multiple small graphs, should I batch them separately, or does it make sense to construct one global graph?",3,3,2025-08-05 17:05:11,Street_Car_1297,0.9211,0.081,0.026,0.892,0.0456093073593073,0.322038961038961
[D] NeurIPS 2025 reviewer Confidential Comment,"We are in discussion period for NeurIPS 2025. One of my reviewer is disrespectful;

Doesn't have much knowledge in this field, but keep insisting he/she is right, againsting all the references in this field.  
Also, this reviewer keeps raising issue out of scope. e.g., My paper is regarding bias, but the reviewer is saying ""setting 'gender' and 'race' as debiasing target is biased action"". I totally disagree this, then, how about the US law like ""The Equal Pay Act of 1963"" and ""The Fair Housing Act"" also controversial?

I want to send AC confidential comment for the first time in my life, but is there any official guideline regarding the AC confidential comment? I want to make sure this reviewer is not eligible to review.",22,17,2025-08-05 16:43:13,,0.1332,0.087,0.084,0.829,0.2873015873015873,0.545326278659612
DeepMind Genie3 architecture speculation,"If you haven't seen Genie 3 yet: [https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)

It is really mind blowing, especially when you look at the comparison between 2 and 3, the most striking thing is that 2 has this clear constant statistical noise in the frame (the walls and such are clearly shifting colours, everything is shifting because its a statistical model conditioned on the previous frames) whereas in 3 this is completely eliminated. I think we know Genie 2 is a diffusion model outputting 1 frame at a time, conditional on the past frames and the keyboard inputs for movement, but Genie 3's perfect keeping of the environment makes me think it is done another way, such as by generating the actual 3d physical world as the models output, saving it as some kind of 3d meshing + textures and then having some rules of what needs to be generated in the world when (anything the user can see in frame). 

What do you think? Lets speculate together!",147,24,2025-08-05 15:25:24,HerpisiumThe1st,0.8478,0.057,0.0,0.943,0.1225490196078431,0.5152661064425771
[D]Improving Hybrid KNN + Keyword Matching Retrieval in OpenSearch (Hit-or-Miss Results),"Hey folks,

I’m working on a Retrieval-Augmented Generation (RAG) pipeline using OpenSearch for document retrieval and an LLM-based reranker. The retriever uses a hybrid approach:
	•	KNN vector search (dense embeddings)
	•	Multi-match keyword search (BM25) on title, heading, and text fields

Both are combined in a bool query with should clauses so that results can come from either method, and then I rerank them with an LLM.

The problem:
Even when I pull hundreds of candidates, the performance is hit or miss — sometimes the right passage comes out on top, other times it’s buried deep or missed entirely. This makes final answers inconsistent.

What I’ve tried so far:
	•	Increased KNN k and BM25 candidate counts
	•	Adjusted weights between keyword and vector matches
	•	Prompt tweaks for the reranker to focus only on relevance
	•	Query reformulation for keyword search

I’d love advice on:
	•	Tuning OpenSearch for better recall with hybrid KNN + BM25 retrieval
	•	Balancing lexical vs. vector scoring in a should query
	•	Ensuring the reranker consistently sees the correct passages in its candidate set
	•	Improving reranker performance without full fine-tuning

Has anyone else run into this hit-or-miss issue with hybrid retrieval + reranking? How did you make it more consistent?

Thanks!
",6,1,2025-08-05 06:57:56,MylarSome,0.9261,0.092,0.03,0.878,0.224047619047619,0.5523809523809523
[D] Seeking advice on choosing PhD topic/area,"Hello everyone,

I'm currently enrolled in a master's program in statistics, and I want to pursue a PhD focusing on the theoretical foundations of machine learning/deep neural networks.

I'm considering statistical learning theory (primary option) or optimization as my PhD research area, but I'm unsure whether statistical learning theory/optimization is the most appropriate area for my doctoral research given my goal.

Further context: I hope to do theoretical/foundational work on neural networks as a researcher at an AI research lab in the future. 

Question:

1)What area(s) of research would you recommend for someone interested in doing fundamental research in machine learning/DNNs?

2)What are the popular/promising techniques and mathematical frameworks used by researchers working on the theoretical foundations of deep learning?

Thanks a lot for your help.",13,3,2025-08-05 05:38:28,willingtoengage,0.9575,0.15,0.017,0.833,0.1541666666666666,0.31875
[D] AAAI 2026 desk reject,"I submitted a paper to the AAAI 2026 conference. The conference states that colors must only be used for figures.

I mistakenly used colors in an experimental table to show the increase in accuracy within parentheses.

Will I have a chance to modify it in the rebuttal phase? Are there some cases in which those who have made the same mistake proceed with the rebuttal phase?

I found someone who submitted a paper with the same mistake to another conference proceeded with the rebuttal successfully.",5,3,2025-08-05 04:20:04,Dense-Ad-8885,-0.3736,0.08,0.108,0.812,0.1699999999999999,0.5199999999999999
[P] sklearn-migrator – A library to migrate scikit-learn models across versions,"Hi everyone! 👋

I want to share the initial release of \[\`sklearn-migrator\`\] ([https://pypi.org/project/sklearn-migrator/](https://pypi.org/project/sklearn-migrator/)) – a Python library designed to **serialize and migrate scikit-learn models across incompatible versions.**

If you’ve ever faced issues like \`AttributeError: '...' object has no attribute '...'\` after upgrading \`scikit-learn\`, or had to retrain models just because of version mismatches in production… this tool is for you.

What it does?

\- Converts saved models from older \`scikit-learn\` versions to be compatible with newer ones

\- Supports serialization and internal structure mapping (especially for tree-based models)

\- Designed to help maintain long-term model compatibility in production

\## ✅ Current support

\- \*\*Classifiers & regressors\*\*:

\- \`DecisionTree\`, \`RandomForest\`, \`GradientBoosting\`, \`LogisticRegression\`, \`LinearRegression\`, and more

\- Tested across versions like: \[

'0.21.3', '0.22.0', '0.22.1', '0.23.0', '0.23.1', '0.23.2',

'0.24.0', '0.24.1', '0.24.2', '1.0.0', '1.0.1', '1.0.2',

'1.1.0', '1.1.1', '1.1.2', '1.1.3', '1.2.0', '1.2.1', '1.2.2',

'1.3.0', '1.3.1', '1.3.2', '1.4.0', '1.4.2', '1.5.0', '1.5.1',

'1.5.2', '1.6.0', '1.6.1', '1.7.0'

\]

We have 900 pairs of tested versions.

Repository Github: [https://github.com/anvaldes/sklearn-migrator](https://github.com/anvaldes/sklearn-migrator)  
PyPI: [https://pypi.org/project/sklearn-migrator/](https://pypi.org/project/sklearn-migrator/)  
Medium article: [https://medium.com/@alberto.valdes.gonzalez.96/sklearn-migrator-safe-migration-of-models-across-scikit-learn-versions-0842f8dc375e](https://medium.com/@alberto.valdes.gonzalez.96/sklearn-migrator-safe-migration-of-models-across-scikit-learn-versions-0842f8dc375e)",7,2,2025-08-04 23:39:32,beto_valdes,0.9359,0.107,0.012,0.881,-0.1833333333333333,0.6233333333333333
[N] Machine Learning Reproducibility Challenge (MLRC) 2025 happening this month at Princeton University,"- The 8th iteration of MLRC is happening in-person at Princeton University on August 21st. Keynote speakers include Arvind Narayanan (Princeton), Soumith Chintala (Pytorch - Meta), Jonathan Frankle (Databricks) and Stella Biderman (EleutherAI). 
- Panel discussion on ""Reproducibility of and by large language models"", moderated by Sayash Kapoor (Princeton)
- Link to webpage: https://reproml.org/ (registration seems to be still open!)",31,3,2025-08-04 19:32:02,ndpian,0.1511,0.023,0.0,0.977,0.0714285714285714,0.3095238095238095
[D] NeurIPS 2025 Final Scores,"I understand that updated scores of reviewers are not visible to authors this time round. I was wondering if anyone knows whether the final scores will also not be visible? I.e. once you revise your review and add your ""Final justification"", will your score not be visible to the authors anymore?

  
Asking because I've had a reviewer who has selected the mandatory acknowledgement option, not responded to my review, and whose score no longer appears on the portal.",44,118,2025-08-04 15:18:03,NPCNo10,-0.3094,0.016,0.031,0.953,-0.05,0.85
[R] CIKM 2025 Decision,"Hi, has anybody received their submission outcome for CIKM 2025?",18,77,2025-08-04 14:19:30,No_Adhesiveness_3444,0.0,0.0,0.0,1.0,0.0,0.0
[D] Is AMD Still a Bad Choice for AI Workloads?,"I've read a lot that working with an AMD GPU is a nightmare, but that was a while ago. Since they seem to be releasing a well-priced AI GPU in a few months, I wanted to know if it's worth it or if poor support still makes it a bad choice.",10,16,2025-08-04 14:13:17,Popular_Lunch7488,-0.7391,0.091,0.171,0.738,-0.3399999999999999,0.4266666666666666
[P] DocStrange - Open Source Document Data Extractor with free cloud processing for 10k docs/month,"Sharing **DocStrange**, an open-source Python library that makes document data extraction easy.

* **Universal Input**: PDFs, Images, Word docs, PowerPoint, Excel
* **Multiple Outputs**: Clean Markdown, structured JSON, CSV tables, formatted HTML
* **Smart Extraction**: Specify exact fields you want (e.g., ""invoice\_number"", ""total\_amount"")
* **Schema Support**: Define JSON schemas for consistent structured output

**Quick start:**

    pip install docstrange
    docstrange invoice.jpeg --output json --extract-fields invoice_amount buyer seller

**Data Processing Options:**

* **Cloud Mode**: Fast and free processing with minimal setup, free 10k docs per month
* **Local Mode**: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu

**Github**: [https://github.com/NanoNets/docstrange](https://github.com/NanoNets/docstrange)",53,6,2025-08-04 10:09:02,LostAmbassador6872,0.9607,0.173,0.017,0.81,0.1469187675070028,0.5103641456582633
[D] ZRIA architecture and P-FAF are baseless,"I recently came across youtube channel Richard Aragon, watching his videos regarding his original model ZRIA and token transformation method P-FAF in [this video](https://www.youtube.com/watch?v=xP0oHEE6t_U), another on benchmarking his original ZRIA model for [agentic tasks](https://www.youtube.com/watch?v=b9zwwlRVQPo), and finally a video discussing P-FAF's conceptual connections to a recent work in [stochastic calculus](https://www.youtube.com/watch?v=64mmFBclymc). Admittedly, I am unsettled and agitated after posting a handful of questions on his video comments section as user yellowbricks and being threatened into silence with personal attacks and false accusations after challenging his theory and methodology but less than a vent post this it is a warning against the seemingly baseless theory of ZRIA and P-FAF and the unacceptable behavior which led to its niche following. We should remain critical of ZRIA and P-FAF not because of the individual promoting them, but because of the unchecked patterns of thought and conduct they can reinforce in the scientific community.  
  
In the videos, we get conceptual explanations of the architecture ZRIA and he promotes it as a superior architecture to the transformer for language tasks. He has yet to point to a precise mathematical definition or theoretical foundation of ZRIA to describe what it predicts, what it optimizes, etc. Instead, in his agentic analysis video, he presents benchmarks scores such as ROCG which he presents as the best agentic benchmark and shows impressive score of his ZRIA model compared to a bigger Gemma, although as noted by commenter JohnMcclaned he clearly overfits the training data to ZRIA with no mitigating methods such as monitoring a validation set, and as noted by commenter israrkarimzai he has an issue in the code which explains why Gemma had 0 scores across the board and with the fix showed much more reasonable scores with several 100% scores. Both of these wildly weakens his claim to architectural superiority. (JohnMcclaned was unfortunatly bullied out of the comments sections by Richard.)

This lack of rigor is reflected again in his video discussing the combination of ZRIA and P-FAF. Again, he presents a conceptual explanation of ZRIA and P-FAF. In particular he never points to a rigorous formulation of his P-FAF theory. Upon request he does not provide explanations, only a motivation, or insists that modern LLMs have enough knowledge of his theory such that they can substitute as a teacher (as he told to commenter wolfgangsullifire6158). His video description has a link to his hugging face blog post which again is unrigorous and uses a questionable benchmark whose results are weakened by Richard's examples of unscientific methodology in his benchmark videos. He which leaves viewers with no means to analyze, verify, or even understand what his theory is about. He does not address the inconsistencies in the benchmarking and the risk of overfitting in this video either as pointed out again by wolfgangsullifire6158 instead stating that ""Overfitting is a phenomenon unique to the Transformers architecture."" Admittedly I did not comment kindly towards his unscientific attitude and dismissal of the transformer despite his ZRIA being based on it.

In his video linking his P-FAF to a graduate-level stochastic calculus paper on ""theta-expectations"", he again discusses the concepts at a very high level. I assume this video was made to address a request for a video on the theory of P-FAF. Instead of explaining the theory rigorously he tries to present the theta-expectations as a substitute for the mathematical foundation of P-FAF, suggesting that he had to ""go through the exact same process"" and solve the ""exact same problem"" to derive P-FAF with no evidence of such a derivation and only a dim conceptual overlap linking the two ideas in any way.

This is not about Richard as a person. It is about his repeated behavior: marketing unverified claims as revolutionary science, silencing dissent, and treating scientific skepticism as personal attack. You should take this seriously not because of this one individual but because this pattern can erode the epistemic foundations of our field if left unchecked.",2,0,2025-08-04 09:12:23,when_i_Go,-0.943,0.067,0.095,0.838,0.1259933333333333,0.4700399999999999
"[R] Integrative approach for early detection of Parkinson’s disease and atypical Parkinsonian syndromes leveraging hemodynamic parameters, motion data & advanced AI models","https://www.sciencedirect.com/science/article/abs/pii/S0169260725004067


A recent study in Computer Methods and Programs in Biomedicine explores an efficient approach to early Parkinson’s detection using time-series data from low-cost sensors processed on microcontrollers. The lightweight hybrid machine learning model offers potential for accessible screening in low-resource settings.

Highlights:

• Parkinson’s disease (PD) is a progressive neurological disorder affecting motor and non-motor functions. Early detection of PD is essential for improving patient outcomes and quality of life

• This study proposes a multimodal hardware based wearable integrated with a novel machine learning framework for early, accurate and remote diagnosis of Parkinson’s disease.

• Analyses diverse data sets, including hemodynamic parameters, gait patterns, and hand tremor metrics including bradykinesia and rigidity.

• Achieves high accuracy through advanced algorithms, integrating artificial intelligence and intuitive user interface, thus providing a robust diagnostic tool.
",7,0,2025-08-04 04:36:29,Neither_Shoulder_278,0.9349,0.128,0.027,0.845,0.0959375,0.4311458333333333
[P] Implementing Einsum,Implemented einsum using torch operations. Learned a lot doing it and had a lot of fun so wanted to share it here :),44,5,2025-08-03 22:50:55,lyadalachanchu,0.83,0.294,0.0,0.706,0.4,0.6
[D] Strange label studio behavior,"Im using label studio

I'm having a strange problem. When I output with YOLO, it doesn't make predictions, but when I output with v8 OBB and train it, I can see the outputs. What's the problem ?

I wanted to create a cat recognition algorithm. I uploaded 50 cat photos.

I labelled them with Label Studio and exported them in YOLO format. I trained the model with v11 and used it. However, even though I tested the training photos, it couldn't produce any output.

Then I exported the same set in YOLOv8 OBB format and trained it. This time, it achieved a recognition rate of 0.97.

Why aren't the models I trained using YOLO exports working?",0,0,2025-08-03 21:16:41,Hamzayslmn,0.7372,0.104,0.069,0.828,-0.0333333333333333,0.1416666666666666
[D] A not-too-expensive cpu server provider for a month ?,"Hello everyone,

I'm currently in my last month of an internship, doing ML. Everything is great, however, we have a lot of problems with the hardware : the server we usually use is down and will be until the end of my internship. We need to do more training and I managed to convince my boss to use some funds for a remote server until the end of the month. However, I don't know which providers exists and how good they are, so I am asking you. I would need at least 16 cpu threads, ideally more, capable of running 24/7, running on a flavor of ubuntu and, most importantly, with python and conda pre-installed. I don't have a lot of experience with using remote servers so the easier the better (I know how to use ssh for remote connection, but for example I don't know how to close the connection without ending the runnng task). All of this for a budget of 200€ for the month, max !

Thank you all for your help !",2,6,2025-08-03 18:06:37,Pizel_the_Twizel,0.9529,0.126,0.01,0.864,0.2025462962962962,0.4237037037037037
[D] What’s the realistic future of Spiking Neural Networks (SNNs)? Curious to hear your thoughts,"I’ve been diving into the world of Spiking Neural Networks (SNNs) lately and I’m both fascinated and a bit puzzled by their current and future potential.

From what I understand, SNNs are biologically inspired, more energy-efficient, and capable of processing information in a temporally dynamic way.

That being said, they seem quite far from being able to compete with traditional ANN-based models (like Transformers) in terms of scalability, training methods, and general-purpose applications.

# So I wanted to ask :

* Do you believe SNNs have a practical future beyond niche applications?
* Can you see them being used in real-world products (outside academia or defense)?
* Is it worth learning and building with them today, if I want to be early in something big?
* Have you seen any recent papers or startups doing something truly promising with SNNs?

Would love to hear your insights, whether you’re deep in neuromorphic computing or just casually watching the space.

Thanks in advance!",57,26,2025-08-03 17:00:29,ExtraPops,0.9826,0.184,0.009,0.807,0.0798611111111111,0.4381944444444444
Building for the era of experience [D],,0,4,2025-08-03 12:47:07,Excellent-Effect237,0.0,0.0,0.0,1.0,0.0,0.0
[R] Kimi K2: Open Agentic Intelligence (Technical Report),"The Moonshot AI team behind the recent [Kimi K2](https://x.com/Kimi_Moonshot/status/1943687594560332025) model, one of the leading open-weights LLM, just released the technical report: https://arxiv.org/abs/2507.20534

---

**Kimi K2: Open Agentic Intelligence**

*We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.*

---

Recently, there has been discussions about Muon and MuonClip, which the Moonshot AI team has developed for training Kimi. See recent discussions here on r/MachineLearning : https://old.reddit.com/r/MachineLearning/comments/1m2y23l/p_understanding_muon_a_revolutionary_neural/",13,2,2025-08-02 13:16:08,hardmaru,0.9776,0.104,0.008,0.888,0.1136904761904762,0.4437003968253969
[D]pi0 used in simulation,"Has anyone tried out using pi0(the well-known VLA model) on simulation platforms?

Due to budget and safety reasons, i only have very limited access to real robots. So i need to do everything once in simulation first.

So i really would like to know whether it works well there. Would distribution shift be an issue?

Thanks in advance!",1,0,2025-08-02 13:14:35,CurseCrusader,0.8519,0.17,0.032,0.798,0.0974489795918367,0.3705782312925171
[D] Submitted to KDD for the first time! Can I now upload a preprint to arXiv?,"Hey everyone,  
I just made my first ever submission to KDD.  
The submission was double-blind and I uploaded the anonymized version via OpenReview, as required.

Now I’m wondering:  
**Can I submit the same anonymized version as a preprint to arXiv?** The official KDD CFP didn’t say much clearly about this, and I wanted to check what the norm is. Also, the deadline for submission (31 July) has passed.

I had a few concerns and would love input from anyone who's been through this before:

* Will uploading the paper to arXiv violate the double-blind review policy for KDD?
* If I submit it to arXiv now, does the metadata (like the arXiv account or email) risk de-anonymizing me?",0,2,2025-08-02 11:59:54,ArjunBasandrai,0.593,0.062,0.04,0.898,0.1604166666666666,0.3125
[D] Is there any AI startups in Germany🇩🇪 investing time and money in building and training foundational models or working for General Intelligence ?other than Aleph Alpha?,"The only startup I know of that is focused specifically on this area is Aleph Alpha. Most others are just fine-tuning existing models or working on translation and image generation. There is no serious investment of time or money in original research and development in AI.
Does anyone know of any other startups in Germany 🇩🇪 working in this area? Even a pre-revenue stage startup?",53,48,2025-08-02 09:55:56,Remarkable-Ad3290,0.755,0.092,0.036,0.872,0.1202380952380952,0.5952380952380951
[P] Implemented the research paper “Memorizing Transformers” from scratch with my own additional modifications in architecture and customized training pipeline .,"Did some major modifications to the model architecture and hyperparameters, aiming for improved performance. The entire model is built from scratch using PyTorch. 
The original paper introduces a memory-based mechanism that allows the model to attend to information beyond its context window, enabling long-term context handling. Instead of a single attention mechanism, the architecture incorporates two types of attention blocks: XLAttention for capturing short term memory and KNNAttention for enabling long term memory retrieval.

Key Modifications from the Original Paper:
•Replaced the default positional encoding with Rotary Positional Embeddings (RoPE)
•Altered the attention mechanism to use Grouped Query Attention
•Customized the DataLoader to support sharded datasets and data parallelism
•Implemented Mixed Precision Training along with Distributed Data Parallel (DDP) support
•Tweaked several training and model hyperparameters for better adaptability

HF repo with model and training code is here:

https://huggingface.co/abhinavv3/GPT_with_Modified_Memorizing_Transformer",26,5,2025-08-02 09:28:04,Remarkable-Ad3290,0.9201,0.095,0.011,0.893,0.1377747252747252,0.4837912087912088
[D] Looking for help: Need to design arithmetic-economics prompts that humans can solve but AI models fail at,"Hi everyone,  
I’m working on a rather urgent and specific task. I need to craft prompts that involve arithmetic-based questions within the economics domain—questions that a human with basic economic reasoning and arithmetic skills can solve correctly, but which large language models (LLMs) are likely to fail at.

I’ve already drafted about 100 prompts, but most are too easy for AI agents—they solve them effortlessly. The challenge is to find a sweet spot:

* **One correct numerical answer** (no ambiguity)
* **No hidden tricks or assumptions**
* **Uses standard economic reasoning and arithmetic**
* **Solvable by a human (non-expert) with clear logic and attention to detail**
* **But likely to expose conceptual or reasoning flaws in current LLMs**

Does anyone have ideas, examples, or suggestions on how to design such prompts? Maybe something that subtly trips up models due to overlooked constraints, misinterpretation of time frames, or improper handling of compound economic effects?

Would deeply appreciate any input or creative suggestions! 🙏",0,18,2025-08-02 06:37:18,parassssssssss,0.9445,0.158,0.072,0.77,0.0603174603174603,0.4147321428571428
[R] From Taylor Series to Fourier Synthesis: The Periodic Linear Unit,"**Full Example Runs as Videos:** [https://www.youtube.com/playlist?list=PLaeBvRybr4nUUg5JRB9uMfomykXM5CGBk](https://www.youtube.com/playlist?list=PLaeBvRybr4nUUg5JRB9uMfomykXM5CGBk)

Hello! My name is Shiko Kudo; you might have seen me on r/stablediffusion some time back if you're a regular there as well, where I published a vocal timbre-transfer model around a month ago.

...I had been working on the next version of my vocal timbre-swapping model, but as I had been working on it, I realized that in the process I had something really interesting in my hands. Slowly I built it up more, and in the last couple of days I realized that I had to share it no matter what.

This is the Periodic Linear Unit (PLU) activation function, and with it, some fairly large implications.

The paper and code is available on Github here:  
[https://github.com/Bill13579/plu\_activation/blob/main/paper.pdf](https://github.com/Bill13579/plu_activation/blob/main/paper.pdf)  
[https://github.com/Bill13579/plu\_activation](https://github.com/Bill13579/plu_activation)  
The paper is currently pending release on Arxiv, but as this is my first submission I am expecting the approval process to take some time.

It is *exactly* as it says on the tin: neural networks based upon higher-order (cascaded) sinusoidal waveform superpositions for approximation and thus Fourier-like synthesis instead of a Taylor-like approximation with countless linear components paired with monotonic non-linearities provided by traditional activations; and all this change from a change in the activation.

...My heart is beating out my chest, but I've somehow gotten through the night and gotten some sleep and I will be around the entire day to answer any questions and discuss with all of you.",228,52,2025-08-02 05:07:31,bill1357,0.7573,0.057,0.027,0.915,0.0883403361344537,0.398852617970265
[D] What happens if none of the reviewers respond for all of the NeurIPS discussion?,"Got 5/4/3/3, none of the reviewers have responded so far 😭😭😭

Hopefully someone will respond by the end, but was wondering if anyone has any experience with no reviewers responding for the entire discussion",18,27,2025-08-02 03:17:41,PatientWrongdoer9257,-0.2067,0.038,0.054,0.908,0.05,0.8125
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",4,36,2025-08-02 02:15:29,AutoModerator,0.9488,0.205,0.063,0.731,0.0874242424242424,0.2975757575757575
[D] Implementing GPU snapshotting to cut cold starts for large models by 12x,"GPU snapshotting is finally a thing! NVIDIA recently released their [CUDA checkpoint/restore API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html) and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.

GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!

https://preview.redd.it/vjld59c34hgf1.png?width=3162&format=png&auto=webp&s=7a785152723d7a93a2b7ec1c28076e19c2fe27f1

More on how GPU snapshotting works plus additional benchmarks in this blog post: [https://modal.com/blog/gpu-mem-snapshots](https://modal.com/blog/gpu-mem-snapshots)",49,7,2025-08-01 21:09:58,crookedstairs,-0.2225,0.019,0.034,0.947,-0.0814153439153439,0.6996693121693122
[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",7,39,2025-08-01 15:01:19,AutoModerator,0.9323,0.258,0.0,0.742,0.0569264069264069,0.2904143475572047
[R] I’ve read the ASI‑Arch paper — AI discovered 106 novel neural architectures. What do you think?,"I’ve read the ASI‑Arch paper (arxiv.org/abs/2507.18074). It describes an automated AI driven search that discovered 106 novel neural architectures, many outperforming strong human‑designed baselines.

What stood out to me is that these weren’t just small tweaks, some designs combined techniques in ways we don’t usually try. For example, one of the best architectures fused gating directly inside the token mixer:
(Wmix · x) ⊙ σ(Wg · x)
instead of the usual separate stages for mixing and gating. Feels “wrong” by human design intuition, yet it worked, like an AlphaGo move‑37 moment for architecture search.

One thing I’d love to see: validation across scale. The search was done at ~20M parameters, with only a few winners sanity‑checked at 340M. Do these rankings hold at 3B or 30B? If yes, we could explore cheaply and only scale up winners. If not, meaningful discovery might still demand frontier‑level budgets.

Curious what others think: will these AI‑discovered designs transfer well to larger models, or do we need new searches at every scale?
",74,18,2025-08-01 14:28:40,Life-Independence347,0.9821,0.161,0.017,0.822,0.0273524720893141,0.5625199362041468
[D] Database selection out of several dozens conflicting schemas for a larger NL2SQL pipeline,"For a natural language to SQL product, I'm designing a scalable approach for database selection across several schemas with high similarity and overlap.

Current approach:
Semantic Search → Agentic Reasoning

Created a CSV data asset containing:
Database Description (db summary and intent of que to be routed),  Table descriptions (column names, aliases, etc.), Business or decisions rules


Loaded the CSV into a list of documents and used FAISS to create a vector store from their embeddings

Initialized a retriever to fetch top-k relevant documents based on user query

Applied a prompt-based Chain-of-Thought reasoning on top-k results to select the best-matching DB


Problem:
Despite the effort, I'm getting low accuracy at the first layer itself. Since the datasets and schemas are too semantically similar, the retriever often picks irrelevant or ambiguous matches.

I've gone through a dozen research papers on retrieval, schema linking, and DB routing and still unclear on what actually works in production.

If anyone has worked on real-world DB selection, semantic layers, LLM-driven BI, or multi-schema NLP search, I'd really appreciate either:

A better alternative approach, or

Enhancements or constraints I should add to improve my current stack

Looking for real-world, veteran insight. Happy to share more context or architecture if it helps.",3,3,2025-08-01 11:49:21,schmosby420,0.9447,0.116,0.041,0.842,0.1338888888888889,0.4151851851851852
[P] Tri-70B-preview-SFT: Open 70B Parameter LLM for Alignment Research (No RLHF) | Trillion Labs,"Hi r/MachineLearning!

Our startup, Trillion Labs, just released [Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT), a 70 billion-parameter language model trained on \~1.5T tokens. Due to an unexpected compute crunch, we had to cut short on training tokens and opt for a pure supervised fine-tuning (SFT) approach—no RLHF.

# Key Highlights:

* **Pure SFT, zero RLHF**: Great baseline model for alignment experiments (RLHF, RLVR, GRPO, CISPO, etc.)
* **32K token context window**, optimized for long-context tasks
* Strong performance benchmarks (\~Qwen-2.5-72B and LLaMA-3.1-70B), but definitely raw and unaligned
* Optimized multilingual capabilities (primarily English, Korean; Japanese support available)
* Introduced new techniques: **FP8 mixed precision, Scalable Softmax, and iRoPE attention**
* Fully open-source on HuggingFace under a permissive commercial license (though experimental!)

We’re explicitly inviting alignment researchers and NLP enthusiasts to evaluate this model. We'd greatly appreciate feedback on strengths, weaknesses, and especially any alignment issues.

👉 [Model & Details Here](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)

Happy to discuss more—ask us anything below!",15,1,2025-08-01 01:33:31,jshin49,0.9867,0.205,0.026,0.769,0.1933749583749583,0.5187208624708625
[D] The AAAI website is Awful and organization feels clumsy :/,"https://preview.redd.it/x9z4mqmjnagf1.png?width=1078&format=png&auto=webp&s=fe3a69476a90e4574c86b9ee670f601ad7d93320

**Just a rant**

The **instructions literally** **OVERFLOW the web page** on PC. Also the latex **author kit** was updated **3 DAYS** before submission! (Coming from the systems/ML systems research field this is basically unheard of).

Feels very unprofessional and poorly organized. Regardless, best of luck with your submissions! Hopefully we'll see each other in Singapore",58,45,2025-07-31 23:36:04,,0.3593,0.132,0.113,0.755,-0.1339285714285714,0.5678571428571428
[D] Weight Tying in LLM Seems to Force the Last MLP to Become the True Unembedding,"The common story about the unembedding layer of a LLM is usually that they predict the next token based on the hidden state of a vector. However, in practice many small models I inspected uses something called weight tying, where the unembedding matrix is just the transpose of the embedding matrix. This effectively just makes it become a similarity search for matching tokens via dot product with token embeddings. This decision seems out of nowhere and didn't make sense to be the natural choice for token unembedding. It appears to me to assume some weird structure of the embedding space in some sense at first.  And I didn't find any good explanation online either. So what I did was the following experiment: 

1. Take a random small model with weight tying, Llama-3.2-1B in this case. Input some random text and do a forward pass, record what is being added to the residual stream at each layer. 
2. Look at the final logit output and check for the top few most likely next tokens, then record their (normalized) token embedding as their direction. At least in the last layer hidden states those direction are meaningful and basically represent how much the model wants the output to be that token.
3. Check which layers contributed most to those directions. I computed each layer's percentage contribution by dotting each layer's output with the above direction vector and divide by total magnitude in that direction.

So for example suppose the input text is just ""Steve"", then the most likely next token is "" Jobs"". I then record the "" Jobs"" token embedding as direction (I also tried normalizing it but it doesn't change the end result), dot it with the final hidden state which gets 18, which is exactly the number in the raw logits. Before the final hidden state there was a RMSNorm which only scale the magnitude but doesn't change the direction. And the pre-norm dot product is about 3. So what I did was dotting the output of each layer with the "" Jobs"" direction, which turns out the final MLP contributed more than 2 out of 3 here where all other MLP and attention layers contribute very small amount and can be seen as the result of some kind of interference most likely.

And it turns out that the final MLP layer consistently contributed to 60%-80% (sometimes as high as 90%) of the magnitude in top output directions after trying many input texts. I also checked the frobenius norm of all down\_proj matrix of all the MLP layers to make sure it's not just the last layer outputting everything large. (All of them are mostly the same)  
  
 My conclusion is that the final MLP takes in whatever the real hidden representation of the input text is (concentrated on the last token), and just output the probability distribution of next token directly. And the actual unembedding matrix just acts as a format converter (much like softmax) instead of having any meaningful computation itself. But since they aren't real parameters there, it isn't really wasteful and could indeed be a more efficient way for small models. But functionally speaking doing weight tying seems to just make the last MLP to be true unembedding and you effectively lose one MLP layer worth of computation.

I am not a researcher and am not sure if this is the best place to have this kind of discussion. I would appreciate any opinion on if my method and the result makes sense and what are some good places to discuss things like this.",18,19,2025-07-31 23:08:01,Eaklony,0.9901,0.081,0.015,0.903,0.1262892381879723,0.4725049071884513
[D] How are hybrid reasoning models trained?,"I was wondering how a single model, like Claude 3.7 Sonnet, can have both reasoning and non-reasoning modes. I understand that they likely have opening and closing tokens for the chain of thought, similar to Deepseek and that for the non-reasoning mode they probably add the closing tag automatically, preventing reasoning. How do they train something like this? After all, there is a decent amount of overlap between what you would use a reasoning and non-reasoning model for.",5,5,2025-07-31 19:30:43,New-Skin-5064,0.644,0.064,0.013,0.923,0.0238095238095238,0.5702380952380952
[D] NeurIPS 2025 rebuttals.,"Rebuttals are slowly getting released to Reviewers. Let's hope Reviewers are responsive and willing to increase these digits.

  
Feel free to share your experience with rebuttal, your expectations, and how it actually goes as the process evolves.",82,896,2025-07-31 18:57:25,Constant_Club_9926,0.9042,0.268,0.0,0.732,0.0875,0.5125
[R] Seeking Publicly Available Paired MRI + Genomic/Structured Data for Multimodal ML (Human/Animal/Plant),"I'm working on a multimodal machine learning pipeline that combines image data with structured/genomic-like data for prediction task. I'm looking for publicly available datasets where MRI/Image data and Genomic/Structured data are explicitly paired for the same individual/subject. My ideal scenario would be human cancer (like Glioblastoma Multiforme, where I know TCGA exists), but given recent data access changes (e.g., TCIA policies), I'm open to other domains that fit this multimodal structure:

What I'm looking for (prioritized):

Human Medical Data (e.g., Cancer): MRI/Image: Brain MRI (T1, T1Gd, T2, FLAIR). Genomic: Gene expression, mutations, methylation. Crucial: Data must be for the same patients, linked by ID (like TCGA IDs).

I'm aware of TCGA-GBM via TCIA/GDC, but access to the BraTS-TCGA-GBM imaging seems to be undergoing changes as of July 2025. Any direct links or advice on navigating the updated TCIA/NIH Data Commons policies for this specific type of paired data would be incredibly helpful.

Animal Data:

Image: Animal MRI, X-rays, photos/video frames of animals (e.g., for health monitoring, behavior).

Genomic/Structured: Genetic markers, physiological sensor data (temp, heart rate), behavioral data (activity), environmental data (pen conditions), individual animal ID/metadata.

Crucial: Paired for the same individual animal.

I understand animal MRI+genomics is rare publicly, so I'm also open to other imaging (e.g., photos) combined with structured data.

Plant Data:

Image: Photos of plant leaves/stems/fruits (e.g., disease symptoms, growth).

Structured: Environmental sensor data (temp, humidity, soil pH), plant species/cultivar genetics, agronomic metadata. Crucial: Paired for the same plant specimen/plot.

I'm aware of PlantVillage for images, but seeking datasets that explicitly combine images with structured non-image data per plant.

What I'm NOT looking for:

Datasets with only images or only genomic/structured data.

Datasets where pairing would require significant, unreliable manual matching.

Data that requires extremely complex or exclusive access permissions (unless it's the only viable option and the process is clearly outlined).

Any pointers to specific datasets, data repositories, research groups known for sharing such data, or advice on current access methods for TCGA-linked imaging would be immensely appreciated!

Thank you!",2,3,2025-07-31 16:44:51,01kaushikjain01,0.9794,0.08,0.007,0.912,0.1088815789473684,0.4572368421052633
[D] How to fairly compare AI training methods when they produce different population sizes?,"Hey! I'm working on a conference paper about training AI models and I've hit a tricky experimental design problem that I'd love your input on.

**TL;DR:** I'm comparing two LLM optimization methods that produce final populations of 35 vs 600. How do I fairly measure which works better?

**The Big Picture**

I'm using an evolutionary algorithm that evolves LLM prompts for an objective (persuasiveness vs truthfulness in my case). I'm using a debating tournament to determine the fitness of prompts on a reading comprehension task and then evolve them to be more persuasive/truthful through a mutator.

Evolution implementation:

**Persuasion Training:** Individual debate strategies compete in tournaments. Winners advance, losers get eliminated and replaced with evolved versions.

**Truth Training:** Pairs of strategies work as teams and get scored together (their objective is to ""surface"" the truth in the debate). They win when the judge picks the correct answer (not just when they sound convincing).

Both start with identical seeds: 7 categories of debate strategies (like ""Emotional Appeal,"" ""Authority,"" ""Rationality"") with 5 specific prompts in each category (35 total).

**The Problem**

To run my evolutionary tournaments, for truth optimization, I pair the strategies up with each other, which results in 2 very different population sizes (35 for persuasion vs 595 for truth). In the evolution step, the members of a pair are mutated together (mutator generates A + B prompt).

Now I want to compare which approach produces better results, but how do you fairly compare 35 vs 600 strategies?

Possible Solutions I've thought of:

**- Category Averages**: Compare the average performance of each strategy category (Persuasion optimized Emotional Appeal vs Truth optimized Emotional Appeal, etc.). For truth, I take the average performance of all paired strategies in a particular category. (seems complicated, and I'm not measuring prompts, which I optimized, directly)

**- Top-K Performers:** Compare the top k from each approach (k=20 means 57% of persuasion population vs 3% of truth population - seems unfair?)

**- Kind of Apples-to-Apples**: Make ids for the original strategies and use these to average the truth pair member's performance - effectively mapping performance in pairs back to individual performance. (but does this throws away the core collaborative aspect of truth training?)

**- Something else entirely?**

**My Questions:**

Which comparison method would be most methodologically sound?

Are there established practices for comparing optimization results with different population structures?

Is there a fundamentally better way to frame this comparison that I'm missing?

Any insights would be hugely appreciated!

https://preview.redd.it/q4c0pqr417gf1.png?width=1080&format=png&auto=webp&s=31e93192b2831d4ddf7fda9977fad5bf8c89c9dd

",5,5,2025-07-31 11:13:16,Hot_Letter5239,0.9963,0.173,0.018,0.809,0.1721014492753622,0.5334420289855073
[D] How to find colloborators to grow a small result?,"I’ve made a small but tangible research/prototyping step. I’m unsure how to pursue the next direction/step. I’d appreciate advice on next steps and how can I find collaborators who are interested in extending, or co-authoring the same  
Thanks",8,6,2025-07-31 10:11:52,ApartmentEither4838,0.8573,0.201,0.046,0.753,-0.0071428571428571,0.2321428571428571
[R] How LLMs Are Transforming Recommender Systems — New Paper,"Just came across this solid new arXiv survey:  
📄 **""Harnessing Large Language Models to Overcome Challenges in Recommender Systems""**  
🔗 [https://arxiv.org/abs/2507.21117](https://arxiv.org/abs/2507.21117)

Traditional recommender systems use a modular pipeline (candidate generation → ranking → re-ranking), but these systems hit limitations with:

* Sparse & noisy interaction data
* Cold-start problems
* Shallow personalization
* Weak semantic understanding of content

This paper explores how **LLMs** (like GPT, Claude, PaLM) are redefining the landscape by acting as **unified, language-native models** for:

* 🧠 Prompt-based retrieval and ranking
* 🧩 Retrieval-augmented generation (RAG) for personalization
* 💬 Conversational recommenders
* 🚀 Zero-/few-shot reasoning for cold-start and long-tail scenarios
* And many more....

They also propose a structured taxonomy of LLM-enhanced architectures and analyze trade-offs in **accuracy, real-time performance, and scalability**.

https://preview.redd.it/r97wfum1f5gf1.png?width=950&format=png&auto=webp&s=48cb784526ec81ff1b44318ee894da1fa386201c

  
",0,0,2025-07-31 05:49:56,Downtown_Ambition662,-0.837,0.021,0.077,0.903,0.0778679653679653,0.4312662337662337
[D] Is there a method as general as MCTS for imperfect information games?,"As I understand, MCTS had hype when GDM's AlphaX projects succeeded because MCTS+NN combo ended up being a very general method applicable to a lot of perfect information games, its efficiency was proved by the fact that AlphaZero/Lc0 reached very close to Stockfish level in chess.

Do we have something similarly simple yet efficient for IIGs? I don't count CFR and its variants as such because they don't scale to huge games (MCTS+NN does). ReBeL is a new type of beast but it is not very general (I guess) because it requires the developer to decide at which point to do subgame solving.

I also saw IS-MCTS and other determinization approaches but they look very fragile.

Thanks in advance",1,4,2025-07-30 09:17:44,Working_Bunch_9211,0.9081,0.123,0.033,0.844,0.1589277389277389,0.5226086413586414
[D] Scientific ML: practically relevant OR only an academic exploration?,"I am no ML expert, but a master's student in computational science/mechanics with interest in scientific ML. 

There have been several developments since the inception of PINNs and I see many researchers working in this area. The field has at least academically grown, with several maths, computational mechanics, scientific computing and even some computer graphics groups contributing actively to it. 

What I often see is that the applications are made to very academic PDEs and simple geomtrical domains. The recent complexity I saw was physics-informed diffusion of metamaterials or heterogeneous material generation. 

I am not yet sure if this field has got traction in the broader industry with practical applications. Yes, there is Physicsx which has stood out recently. 

I see several challenges, which may have been addressed: 
1) geometrical complexity and domain size limitations due to GPU limits, 
2) generalization of the trained SciML model on new BCs or physical conditions.
3) training bottlenecks: if high fidelity simulation data is required, typically it takes long times to generate a large enough dataset, with practically relevant geomtrical complexity and domain sizes. Even if solver and model are coupled in some way, all that GPU acceleration is moot since most solvers are still CPU based. 
4)  Building trust and adoption in engineering industries, which heavily rely on CPU intensive simulations. 

Given these challenges, does the broader ML community see any relevance of scientific ML beyond academic interests? 

Do you think it is still in a very nascent stage of development? 

Can it grow like the boom of LLMs and Agentic AI? 

Thank you for contributing to the discussion!",55,32,2025-07-31 07:24:01,Mundane_Chemist3457,0.9764,0.102,0.014,0.884,0.0772462722462722,0.3958150285928064
[R] Need Urgent Help Regarding ICCV Submission,I received the email from OpenReview that CPS has not received my paper submission but in CPS site I already submitted the paper with Copyright. As the email stated my submission status should be 'received' but it is still 'submitted'. Can someone know why this is happening?,6,32,2025-07-31 03:49:05,megaton00,0.3071,0.061,0.0,0.939,0.0,0.0
[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",5,5,2025-07-31 02:30:32,AutoModerator,0.7096,0.082,0.0,0.918,0.2,0.1
"[P] FOMO(Faster Objects, More Objects)","Hey folks!

I recently implemented the [FOMO model by Edge Impulse](https://docs.edgeimpulse.com/docs/edge-impulse-studio/learning-blocks/object-detection/fomo-object-detection-for-constrained-devices) to make longer training sessions available for free. I trained the model using the Mobilenet 0.35 backbone on the VIRAT dataset. The model is incredibly fast and lightweight, coming in at just 20K parameters🚀! You can check out the repository here:  
[https://github.com/bhoke/FOMO](https://github.com/bhoke/FOMO)

While it performs fantastically in terms of speed and efficiency, I’m currently struggling with a high rate of false positives. If anyone has tips or experience tackling this issue, your advice would be greatly appreciated.

https://i.redd.it/delf5bb5p2gf1.gif

I’d love to hear your feedback, and all contributions are very welcome. If you find the project interesting or useful, please consider giving it a star—it really helps improve visibility! ⭐

Thanks in advance for your support and suggestions!",3,5,2025-07-30 20:51:10,berkusantonius,0.9903,0.273,0.018,0.71,0.2303124999999999,0.4806249999999999
[R] Deepmind's AlphaEarth Foundations helps map our planet in unprecedented detail,"Blogpost: [https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/](https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/)  
Paper: [https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf)",98,2,2025-07-30 17:52:23,LetsTacoooo,0.3818,0.157,0.0,0.843,-0.3,0.9666666666666668
[R] Has anyone experimented with using Euclidean distance as a probability function instead of cosine distance?,"I mean this: in the classic setup in order to get probability estimations we calculate softmax of a linear projection, which is calculating cosine distance between predicted vector and weight matrix (plus bias score).

I am intrigued by the following idea: what if we replace cosine distance with Euclidean one as follows:

Instead of calculating 

*cos\_dist = output\_vectors \* weights* 

*unnormalized\_prob = exp(cos\_dist) \* exp(bias)     // lies in (0;+inf) interval*

*normalized\_prob = unnormalized\_prob  / sum(unnormalized\_prob)*

we can calculate

*cos\_dist = output\_vectors \* weights* 

*euc\_dist = l2\_norm(output\_vectors)\^2 - 2 \* cos\_dist + l2\_norm(weights)\^2*

*unnormalized\_prob = abs(bias) / euc\_dist     // lies in (0; +inf) interval*

*normalized\_prob = unnormalized\_prob  / sum(unnormalized\_prob)*

  
The analogy here is gravitational problem, and unnormalized probability is gravitational potential of a single vector from the weights matrix which correspond to a single label. 

I've tried it on a toy problem, but resulting crossentopy was higher than crossentropy with classic formulas, which means it learns worse.

So I wonder if there are any papers which researched this topic?",1,7,2025-07-30 11:47:10,fan_is_ready,-0.8809,0.0,0.078,0.922,-0.0302248677248677,0.405489417989418
[P] A Black Box LLM Explainability Metric,"Hey folks, in one of my maiden attempts to quanitfy the Explainability of Black Box LLMs, we came up with an approach that uses Cosine Similarity as a methodology to compute a word level importance score.
This kindof gives an idea as to how the LLM interprets the input sentence and masking which word causes the maximum amount of deviation in the output.
This method involves several LLM calls to be made, and it's far from perfect but I got some interesting observations from this approach and just wanted to share with the community.

This is more of a quantitative study of this Appraoch.

The metric is called ""XPLAIN"" and I also got some time to create a starter GitHub repo for the same.

Do check it out if you find this interesting:

Code: https://github.com/dhargopala/xplain

Paper: https://www.tdcommons.org/dpubs_series/8273/
",2,3,2025-07-30 10:50:40,dhargopala,0.9413,0.12,0.0,0.88,0.2518518518518518,0.4990740740740741
"[P] Fine-tuning a fast, local “tab tab” code completion model for Marimo notebooks","In the spirit of building in public, we're collaborating with Marimo to build a ""tab completion"" model for their notebook cells, and we wanted to share our progress as we go in tutorial form.

Here’s the first post in what will be a series:
https://www.oxen.ai/blog/building-a-tab-tab-code-completion-model

The goal is to create a local, open-source model that provides a Cursor-like code-completion experience directly in notebook cells. You'll be able to download the weights and run it locally with Ollama or access it through a free API we provide.

We’re already seeing promising results by fine-tuning the Qwen and Llama models, but there’s still more work to do. Here's a leaderboard on a corrupted MBPP dataset with the models we've tried so far. All fine-tuned models have funky code names in parenthesis. Promising to see the early experiments getting to GPT-4 level.

Accuracy -> Model

82.60% -> Claude 4 Sonnet

80.60% -> Qwen3 Coder 480B

78.80% -> Kimi-2

74.40% -> Llama 4 Maverick

74.40% -> GPT 4o

73.00% -> GPT 4.1

68.60% -> Qwen 3 - 4B (acute-chocolate-anteater)

68.00% -> Llama 4 Scout

61.80% -> Qwen 3 - 1.7B (ordinary-red-cow)

60.20% -> GPT 4o Mini

52.80% -> Llama 3.2 - 3B (awful-crimson-salamander)

50.80% -> Llama 3.1 - 8B (sufficient-tan-alligator)

47.80% -> Qwen 3 - 0.6B (continental-blush-guppy)

36.00% -> Llama 3.2 - 1B (successful-amaranth-raven)

If you’re interested in contributing to data collection or the project in general, let us know! We already have a working CodeMirror plugin and are focused on improving the model’s accuracy over the coming weeks.

",10,7,2025-07-30 01:30:16,FallMindless3563,0.9652,0.095,0.006,0.899,0.17890625,0.4140625
[D] Math book recommendations for NN theory,"I'm a PhD student interested in neural network architecture design, who recently ran into a growing level of rigor in the field and found out that his CS major math background is not enough. In particular, I was working primarily with sequence processing networks (Transformers and RNNs) with an aim to reduce their computational complexity or find inefficient representations. I would like to continue the work but to guide it with a theory instead of intuition, and as reference papers I'd cite Albert Gu's papers on [SSM ](https://arxiv.org/pdf/2111.00396)and [HiPPO](https://arxiv.org/abs/2008.07669) and Chulhee Yun's works, for example like [this](https://arxiv.org/abs/1912.10077) and [this](https://arxiv.org/abs/2006.04862).

Currently I'm finishing the Rudin's ""Real and Complex Analysis"" first half on real analysis. I'm also quite sure that Horn's ""Matrix Analysis"" and Trefethen's ""Approximation Theory and Approximation Practice"" will be useful, but I struggle to decide how much and which analysis sources I need to study after (Complex analysis chapters? Rudin's and Kreyszig's FA?). I feel that I haven't reached the level to study from papers yet, although earlier works like [this](https://web.njit.edu/~usman/courses/cs677/10.1.1.441.7873.pdf) seem to be accessible after I'm done with RCA.

I would like to ask for some guidance about which math literature might be useful in the given context after I finish the real analysis chapters from RCA. I have found ""understanding level"" lit recommendations quite abundant, but ""research level"" much less addressed overall, so I hope it will be useful not only for me.",62,21,2025-07-30 00:27:22,EternaI_Sorrow,0.9845,0.137,0.017,0.846,0.1348333333333333,0.3665555555555555
"[R] Are AUC/ROC curves ""black box"" metrics?","Hey guys! (My first post here, pls be kind hehe)

I am a PhD student (relatively new to AI) working with ML models for a multi-class classification task. Since I ruled out accuracy as the evaluation metric given a class imbalance in my data (accuracy paradox), I stuck to AUC and plotting ROC curves (as a few papers told they are good for imbalanced train sets)  to evaluate a random forest model's performance ( 10-fold cross validated) trained on an imbalanced dataset and tested on an independent dataset. I did try SMOTE to work on the imbalance, but it didn't seem to help my case as there's a major overlap in the distribution of the data instances in each of the classes I have (CLA,LCA,DN) and the synthetic samples generated were just random noise instead of being representative of the minority class. Recently, when I was trying to pull the class predictions by the model, I have noticed one of the classes( DN) having 0 instances classified under it. But the corresponding ROC curve and AUC said otherwise. Given my oversight, I thought DN shined ( High AUC compared to other classes ) given it just had a few samples in the test set, but it wasn't the case with LCA (which had fewer samples). Then I went down the rabbit hole of what ROC and AUC actually meant. This is what I thought and would like more insight on what you guys think and what can it mean, which could direct my next steps.

The model's assigning higher probability scores to true DN samples than non-DN samples (CLA and LCA), Hence, masked good ROC curve and high AUC scores, but when it comes to the model's predictions, the probabilities aren't able to pass the threshold selected. Is this is a right interpretation? If so, I thought of these steps:

\- Set threshold manually by having a look at the distribution of the probabilities ( which I am still skeptical about)

\- Probably ditch ROC and AUC as the evaluation metrics in this case (I have been lying to myself this whole time!)

If you think I am a bit off about what's happening, your insights would really help, thank you so much! 

",3,26,2025-07-29 16:03:41,Pure_Landscape8863,0.7335,0.056,0.039,0.905,0.0888125494577107,0.4012359772843643
"[D] New recent and applied ideas for representation learning? (i.g. Matryoshka, Constrastive learning, etc.)","I am exploring ideas for building domain specific representations (science problems). I really like the idea of [Matryoshka learning](https://arxiv.org/html/2505.23337v1) since it gives you ""PCA""-like natural ordering to dimensions.

Contrastive learning is also a very common tool know for building representations since it makes your embeddings more ""distance aware"".

What are new neural network ""tricks"" that have come out in the last 2-3 years for building better representations. Thinking broadly in terms of unsupervised and supervised learning problems. Not necessarily transformer models.",37,19,2025-07-29 15:49:38,LetsTacoooo,0.6697,0.083,0.027,0.89,0.1150174825174825,0.3971736596736597
[P] Stand-alone implementation of DeepSeek's Native Sparse Attention in PyTorch,"NSA is an interesting architectural choice, reduces both the complexity while matching or even surpassing full attention benchmarks as well.

I went around looking inside it to try and grab my head around things, most of the implementations were packed with Triton kernels for performance, so I built this naive implementation of Native Sparse Attention in pure PyTorch with

* GroupedMLP/Convolution1d/AvgPooling for token compression
* Gating mechanism for combining different branches of the network
* Drop-in replacement functionality to standard Attention block

Check it out here: [native\_sparse\_attention](https://github.com/shreyashkar-ml/native_sparse_attention)",7,3,2025-07-29 02:54:08,Southern-Whereas3911,-0.2034,0.049,0.057,0.894,0.0642857142857142,0.58125
"[D] First research project – feedback on ""Ano"", a new optimizer designed for noisy deep RL (also looking for arXiv endorsement)","Hi everyone,

I'm a student and independent researcher currently exploring optimization in Deep Reinforcement Learning. I recently finished my first preprint and would love to get feedback from the community, both on the method and the clarity of the writing.

The optimizer I propose is called Ano. The key idea is to decouple the magnitude of the gradient from the direction of the momentum. This aims to make training more stable and faster in noisy or highly non-convex environments, which are common in deep RL settings.

📝 Preprint + source code: [https://zenodo.org/records/16422081](https://zenodo.org/records/16422081)

📦 Install via pip: \`pip install ano-optimizer\`

🔗 GitHub: [https://github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)

This is my first real research contribution, and I know it's far from perfect, so I’d greatly appreciate any feedback, suggestions, or constructive criticism.

I'd also like to make the preprint available on arXiv, but as I’m not affiliated with an institution, I can’t submit without an endorsement. If anyone feels comfortable endorsing it after reviewing the paper, it would mean a lot (no pressure, of course, I fully understand if not).

Thanks for reading and helping out 🙏

Adrien",32,14,2025-07-29 11:19:02,Adrienkgz,0.9511,0.137,0.047,0.816,0.1093545454545454,0.5482818181818183
[R] Multi-View Contrastive Learning: Principled Framework for 3+ Views and Modalities,"**TL;DR**: Current SSL methods like SwAV, DINO, and VICRegL use multiple views but handle them suboptimally by aggregating pairwise losses, causing conflicting objectives and missed interactions. We introduce MV-InfoNCE and MV-DHEL - principled objectives that scale properly with any number of views and prevent dimensionality collapse.

**Paper**: [https://arxiv.org/abs/2507.06979](https://arxiv.org/abs/2507.06979)

**Code**: [https://github.com/pakoromilas/Multi-View-CL](https://github.com/pakoromilas/Multi-View-CL)  


**The Problem**

Current SSL methods create multiple augmented views but handle them through pairwise loss aggregation:

    L_total = L(v1,v2) + L(v1,v3) + L(v1,v4) + L(v2,v3) + L(v2,v4) + L(v3,v4)

This approach causes:

* **Conflicting objectives**: Each view satisfies multiple competing loss terms
* **Ignored view relationships**: Pairwise aggregation misses view interactions among all views
* **Fundamental limitations**: Inherits problems (e.g. alignment-uniformity coupling) from pairwise CL losses
* **Limited transfer**: Multi-view benefits diminish as you add more views

**The CLIP Problem**: While CLIP revolutionized vision-language learning, extending it to 3+ modalities is still not straightforward. CLIP's contrastive framework is inherently pairwise - adding audio, video, or sensor data requires either separate pairwise models or naive aggregation, both of which fail to capture all multimodal interactions concurrently.

**Our Loss Functions**

1. **MV-InfoNCE**: Extends InfoNCE to N views properly
2. **MV-DHEL**: Decouples alignment from uniformity

**Key Results**

✅ **Scale properly** with number of views

✅ **Prevent dimensionality collapse** when using 5+ views (figure below)

✅ **Outperform existing** multi-view approaches on ImageNet1K and three other datasets

✅ **Extend to 3+ modalities** (not just 2!)

https://preview.redd.it/vib4lluozrff1.png?width=1200&format=png&auto=webp&s=9c0daafe65e74c8a24bca93f2343d3c17a1767f2

**Overall Contributions**

* **Principled Multi-View Formulation**: Mathematical framework that properly extends CL from pairwise to multi-view settings, modeling simultaneous interactions between all N views rather than aggregating pairwise comparisons
* **Novel Loss Functions**: (i) MV-InfoNCE - natural extension of InfoNCE incorporating all view interactions, (ii) MV-DHEL - decouples alignment from uniformity across views
* **Theoretical Guarantees**: Proved both objectives share asymptotic behavior with traditional InfoNCE, establishing them as theoretically sound extensions
* **Empirical Advances**: Consistently outperform existing approaches, effectively scale with view multiplicity, mitigate dimensionality collapse with sufficient views
* **Multimodal Applicability**: Unlike existing methods designed for bimodal settings, directly applicable to 3+ modalities



**Possible Applications**

* **Beyond CLIP**: Multimodal learning with vision + text + audio + sensor data
* **Video Understanding**: Temporal + spatial + semantic views in unified framework
* **Medical Imaging**: Multiple scan types (CT, MRI, X-ray) without pairwise limitations
* **Robotics**: Vision + tactile + proprioceptive sensing with theoretical guarantees



The GitHub repo includes PyTorch implementations.

Happy to discuss about our research!",7,0,2025-07-29 08:42:17,nai_alla,-0.8843,0.098,0.11,0.792,-0.0122111344537815,0.3880252100840335
[D] AAAI-2026 Code Submission,"Hello\~\~

I am just wondering how much importance code submission has for the decision making and review. and are you all submitting the codes? or it is fine if we release it if/after acceptance. My code is so messy so m in dilemma",8,25,2025-07-29 08:00:10,i_minus,0.2687,0.139,0.099,0.762,0.1388888888888889,0.3666666666666667
[R] Introducing SNAC-DB: A New Open-Source Resource for Antibody & NANOBODY® VHH–Antigen Modeling,"Predicting antibody and NANOBODY® VHH–antigen complexes remain a notable gap in current AI models, limiting their utility in drug discovery. We present **SNAC-DB**, a machine-learning-ready database and pipeline developed by structural biologists and ML researchers to address this challenge.

Key features of SNAC-DB include:

·       **Expanded Coverage:** 32 % more structural diversity than SAbDab, capturing overlooked assemblies such as antibodies/nanobodies as antigens, complete multi-chain epitopes, and weak CDR crystal contacts.

·       **ML-Friendly Data:** Cleaned PDB/mmCIF files, atom37 NumPy arrays, and unified CSV metadata to eliminate preprocessing hurdles.

·       **Transparent Redundancy Control:** Multi-threshold Foldseek clustering for principled sample weighting, ensuring every experimental structure contributes.

·       **Rigorous Benchmark:** An out-of-sample test set comprising public PDB entries post–May 30, 2024 (disclosed) and confidential therapeutic complexes.

Using this benchmark, we evaluated six leading models (AlphaFold2.3‐multimer, Boltz-2, Boltz-1x, Chai-1, DiffDock-PP, GeoDock) and found that success rates rarely exceed 25 %, built-in confidence metrics and ranking often misprioritize predictions, and all struggle with novel targets and binding poses.

We presented this work at the Forty-Second International Conference on Machine Learning (ICML 2025) Workshop on DataWorld: Unifying Data Curation Frameworks Across Domains (https://dataworldicml2025.github.io/) in Vancouver.

·       **Paper:** [https://www.researchgate.net/publication/393900649\_SNAC-DB\_The\_Hitchhiker's\_Guide\_to\_Building\_Better\_Predictive\_Models\_of\_Antibody\_NANOBODY\_R\_VHH-Antigen\_Complexes /](https://www.researchgate.net/publication/393900649_SNAC-DB_The_Hitchhiker's_Guide_to_Building_Better_Predictive_Models_of_Antibody_NANOBODY_R_VHH-Antigen_Complexes%20/) [https://openreview.net/forum?id=68DcIpDaHK](https://openreview.net/forum?id=68DcIpDaHK)

·       **Dataset:** [https://zenodo.org/records/16226208](https://zenodo.org/records/16226208)

·       **Code:** [https://github.com/Sanofi-Public/SNAC-DB](https://github.com/Sanofi-Public/SNAC-DB)

We hope SNAC-DB will accelerate the development and evaluation of more accurate models for antibody complex prediction

https://preview.redd.it/a0d42seuvqff1.png?width=3456&format=png&auto=webp&s=e38ea120357174191b8b5cbb707979cde0ff498a",3,0,2025-07-29 04:57:37,playa_aikido,0.9103,0.081,0.027,0.892,0.1256313131313131,0.4210858585858587
[P] Keyword and Phrase Embedding for Query Expansion,"Hey folks, I am workig on a database search system. The language of text data is Korean. Currently, the system does BM25 search which is limited to keyword search. There could be three scenarios:

1. User enters a single keyword such as ""coronavirus""
2. User enters a phrase such as ""machine learning"", ""heart disease""
3. User enters a whole sentence such as ""What are the symptoms of Covid19?""

To increase the quality and the number of retireved results, I am planning to employ query expansion through embedding models. I know there are context-insensitive static embedding models such as Wor2Vec or GloVe and context-sensitive models such as BERT, SBERT, ELMO, etc.

For a single word query expansion, static models like Word2Vec works fine, but it cannot handle out-of-vocabulary issue. FastText addresses this issue by n-gram method. But when I tried both, FastText put more focus on the morphologic form of words rather than semantic. BERT would be a better option with its WordPiece tokenizer, but when there is no context in a single-word query, I am afraid it will not help much.

For sentence query cases, SBERT works much better than BERT according to the SBERT paper. For Phrases, I am not sure what method to use although I know that I can extract single vector for the phrase through averaging the vectors for individual word (in case of static methods) or word-pieces in case of BERT model application.

What is the right way to proceed these scenarios and how to measure which model is performing better. I have a lot of domain text unlabeled. Also If I decide to use BERT or SBERT, how should I design the system? Should I train the model on unlabeled data using Masked Language Modeling method and will it be enough?

Any ideas are welcome.",2,2,2025-07-29 02:00:19,Ordinary_Pineapple27,0.9195,0.08,0.031,0.889,0.1641025641025641,0.5234737484737485
[P] QLora with HuggingFace Model,"I am finetuning a hugging face LLM in a pytorch training loop using 4-bit quantization and LoRA. The training got through a few batches before hitting the error:

`RuntimeError: one of the variables needed for gradient computation has been modified by an inlace operation: [torch.cuda.HalfTensor[1152,262144], which is output 0 of AsStrideBackward0, is at version 30; expected version 28 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).`

Even if I knew the exact computation causing this, I'm using an open source LLM out of the box, not sure the proper way to go in and modify layers, etc. . I'm also not sure why I could get past a few batches without this error and then it happens. I was getting OOM error originally and then I shortened some of the sequence lengths. It does look like this error is also happening on a relatively long sequence length, but not sure that has anything to do with it. Does anyone have any suggestions here?",1,2,2025-07-29 01:33:34,Secret_Valuable_Yes,-0.8077,0.021,0.086,0.893,-0.1553571428571428,0.4869047619047619
[P] BluffMind: Pure LLM powered card game w/ TTS and live dashboard,"Introducing BluffMind, a LLM powered card game with live text-to-speech voice lines and dashboard involving a dealer and 4 players. The dealer is an agent, directing the game through tool calls, while each player operates with their own LLM, determining what cards to play and what to say to taunt other players. Check out the repository [here](https://github.com/TangyKiwi/BluffMind), and feel free to open an issue or leave comments and suggestions to improve the project!",27,4,2025-07-29 01:21:35,TangyKiwi65,0.8268,0.101,0.014,0.885,0.0162012987012986,0.5374999999999999
[P] Built a modern cookiecutter for ML projects - Lets make it better,"I got fed up with spending the first 3 hours of every ML project fighting dependencies and copy-pasting config files, so I made this cookiecutter template: [https://github.com/prassanna-ravishankar/cookiecutter-modern-ml](https://github.com/prassanna-ravishankar/cookiecutter-modern-ml)

It covers NLP, Speech (Whisper ASR + CSM TTS), and Vision with what I think are reasonable defaults. Uses uv for deps, pydantic-settings for config management, taskipy for running tasks. Detects your device (Mac MPS/CUDA/CPU), includes experiment tracking with Tracelet. Training support with Skypilot, serving with LitServe and integrated with accelerate and transformers. Superrrr opinionated.

I've only tested it on my own projects. I'm sure there are edge cases I missed, dependencies that conflict on different systems, or just dumb assumptions I made.

If you have 5 minutes, would love if you could:

* Try generating a project in your domain
* See if the dependencies actually install cleanly
* Check if uv run task train works (even on dummy data)
* Tell me what breaks or feels wrong

I built this because I was annoyed, not because I'm some template expert. Probably made mistakes that are obvious to fresh eyes. GitHub issues welcome, or just roast it in the comments 🤷‍♂️",0,0,2025-07-28 21:19:59,prassi89,0.2263,0.093,0.089,0.819,0.1217592592592592,0.6512345679012346
"[D] Pattern recognition is not intelligence, just an important part of the structure","Hi everyone, I’ve been doing enterprise ai integration for the last year or so, and I think I’m the only person currently applying reactor control theory to llm orchestration.

To me, current industry efforts aren’t trying to make AI, they’re trying to make omnipotence. Very different.

Let’s imagine Einstein with no memory or gobel who couldn’t tell you why. Sounds ridiculous.

What I’ve been doing is applying transformers as dynamic parts of a larger system. And I’ve been seeing incredible results.

Give the llm memory, guidance, and structure, and suddenly hallucinations are not a big deal. I wouldn’t expect a person to think about the same thing, the same way, every time, so why expect an AI to?

Once you start shaping the structure, and allowing the drift, you can collapse reasoning into lookups.

First concept: Radiology scans.

https://youtu.be/JaNtSkDX1I0?si=sAvQJIHjsuLtnGDx

This collapses llm api calls from 30 to 5 for repeated queries.

Next concept: robotics.

It seems like with a little capital and a little execution, there’s asymmetric upside here. Looking to see if there’s anyone else experimenting in this direction.",0,3,2025-07-28 17:35:53,BSmithA92,-0.7281,0.035,0.066,0.899,0.0467592592592592,0.4664814814814815
[D] EMNLP 2025 Track Selection,"
1) Is it okay/possible (and how is it perceived) to change the main track selection from ARR review to EMNLP conference submission?

2) Can it increase/decrease chances of getting the paper in?",0,2,2025-07-28 16:02:42,Significant_Course12,0.2869,0.057,0.0,0.943,0.1666666666666666,0.3333333333333333
[D] Shifting Research Directions: Which Deep Learning Domains Will Be Most Impactful in the Next 5–6 Years?,"I’m looking for some advice on which research domains in deep learning/computer vision might be exciting and impactful over the next 5–6 years.

For context; I’ve been working in medical image segmentation for the last 3–4 years. While it’s been rewarding, I feel like I’ve been a bit cut off from the broader progress in deep learning. I’ve used modern methods like diffusion models and transformers as baselines, but I haven’t had the time to dive deep into them because of the demands of my PhD. Now that most of my dissertation work is done, I still have about a year and a half of funding left, and I’d like to use this time to explore new directions.

A few areas I’ve considered:

* **Semi-supervised learning**, which occasionally produces some very impactful work in vision. That said, it feels somewhat saturated, and I get the sense that fundamental contributions in this space often require heavy GPU resources.  
* **3D medical imaging**; which seems to be gaining traction, but is still tied closely to the medical domain.  
* **Diffusion and foundational models**; definitely among the most hyped right now. But I wonder if diffusion is a bit overrated; training is resource-intensive, and the cutting-edge applications (like video generation or multimodal foundational diffusion models) may be tough to catch up with unless you’re in a big lab or industry. Do you think diffusion will still dominate in 5 years, or will a new class of generative models take over?  
* **Multimodal deep learning**; combining text+images or text+video feels less over-hyped compared to diffusion, but possibly more fertile for impactful research.  

My interest is in computer vision and deep learning more broadly; I’d prefer to work on problems where contributions can still be meaningful without requiring massive industry-level resources. Ideally, I’d like to apply foundational or generative models to downstream tasks rather than just training them from scratch/only focusing on them.

So my question is: given the current trends, which areas do you think are worth investing in for the next 5–6 years? Do you see diffusion and foundational models continuing to dominate, or will multimodal and other directions become more promising? Would love to hear diverse opinions and maybe even personal experiences if you’ve recently switched research areas. I’m interested in shifting my research into a more explorative mode, while still staying somewhat connected to the medical domain instead of moving entirely into general computer vision.",35,48,2025-07-28 15:13:13,Dismal_Table5186,0.9939,0.134,0.023,0.843,0.1328180262003791,0.3757968338850692
"[P]: `ambient-utils`: A small python package for training diffusion models with ""bad data"".","Made this small python package for training diffusion generative models with ""bad data"":

[https://github.com/giannisdaras/ambient-utils](https://github.com/giannisdaras/ambient-utils)

Install with: \`pip install ambient-utils\`

The idea is that ""bad data"" is only used to train denoisers for \*some\* diffusion times, but not all. There are some easy wrappers that enable this (\`AmbientSampler\` class) and a README with a quick example.

I have been using versions of this codebase for my research for the past 2 years, and it is the primary driver for more than 6 accepted papers to NeurIPS, ICML, and ICLR. I decided to make it open-source so that people can play with it.

If you are dealing with bad data in scientific applications, Computer Vision, robotics or elsewhere, please comment below and give it a try!",4,2,2025-07-28 13:47:08,Constant_Club_9926,0.6439,0.107,0.078,0.815,-0.1715277777777777,0.5875
[R] Misuse of ML for a cortical pain biomarker?,"This comment in *JAMA Neurology* raises several methodological concerns about a previously published ""ML""-based pain biomarker.  


The critique points out two core issues:  


* An incorrect validation set
* An unrepresentative test set

Additionally, the original model was based on only **two input features** (one binary), yet neural networks or gradient boosting were applied. To me, that raises the question of whether such model complexity is appropriate for this data scale and structure, no?  


Are there other plausible reasons why the reanalysis would yield an AUC of **0.65**, compared to the reported **1.0 (validation)** and **0.88 (test)**—beyond what the authors describe?  


The full comment can be found in *JAMA Neurology (2025):* [https://jamanetwork.com/journals/jamaneurology/fullarticle/2836397](https://jamanetwork.com/journals/jamaneurology/fullarticle/2836397).  


Whats your opinion on it?",6,1,2025-07-28 12:30:30,penicillinpeter,-0.7236,0.036,0.074,0.89,0.1592592592592593,0.4824074074074074
State of the Art SISR [R],"I'm investigating state-of-the-art techniques for extreme single-image super-resolution (SISR), specifically targeting high magnification factors up to 100x. My focus is on domain-specific texture synthesis for materials, trained on a curated dataset. I'm exploring the feasibility of fine-tuning generative models like ESRGAN and am particularly interested in methods for conditional generation, where semantic guidance (e.g., material property tags like 'shiny' or 'rough') can be used to steer the output. Would anyone have recommendations on relevant literature, model architectures, or even alternative approaches?",5,4,2025-07-28 11:41:50,eoghank,0.8399,0.109,0.0,0.891,0.117,0.6679999999999999
[D] AAAI: Not able to update authors,"I am trying to submit a paper to AAAI. Even though the modificiation guidelines say that I can edit authors (https://aaai.org/conference/aaai/aaai-26/paper-modification-guidelines/). I am not able to add an author to the paper.  
Anyone facing the same issue? Or any chairs from AAAI can help with this?

Text from the guidelines:  
""After the July 25 abstract deadline and until the August 1 paper submission deadline, the following items can be changed

* list of authors
* author order
* submitted paper"".",9,9,2025-07-28 06:44:12,Lost-Ingenuity5017,0.4696,0.038,0.0,0.962,-0.125,0.36875
[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning,,42,19,2025-07-28 06:01:01,LakshyAAAgrawal,0.0,0.0,0.0,1.0,0.0,0.0
[R] Sapient Hierarchical Reasoning Model. HRM.,,0,15,2025-07-28 03:37:04,vwibrasivat,0.0,0.0,0.0,1.0,0.0,0.0
[P] AI Learns to Play Metal Slug (Deep Reinforcement Learning) With Stable-R...,"Github: [https://github.com/paulo101977/MetalSlugPPO](https://github.com/paulo101977/MetalSlugPPO)  
  
Hey everyone! I recently trained a reinforcement learning agent to play the arcade classic *Metal Slug* using **Stable-Baselines3 (PPO)** and **Stable-Retro**.

The agent receives pixel-based observations and was trained specifically on **Mission 1**, where it faced a surprisingly tough challenge: dodging missiles from a non-boss helicopter. Despite it not being a boss, this enemy became a consistent bottleneck during training due to the agent’s tendency to stay directly under it without learning to evade the projectiles effectively.

After many episodes, the agent started to show decent policy learning — especially in prioritizing movement and avoiding close-range enemies. I also let it explore Mission 2 as a generalization test (bonus at the end of the video).

The goal was to explore how well PPO handles sparse and delayed rewards in a fast-paced, chaotic environment with hard-to-learn survival strategies.

Would love to hear your thoughts on training stability, reward shaping, or suggestions for curriculum learning in retro games!",11,7,2025-07-27 14:50:29,AgeOfEmpires4AOE4,0.7997,0.134,0.095,0.77,0.0736111111111111,0.5570512820512821
[P] Reinforcement Learning from Human Feedback (RLHF) in Notebooks,,8,0,2025-07-27 14:26:56,ashz8888,0.0,0.0,0.0,1.0,0.0,0.1
"[P] AI-Failsafe-Overlay – Formal alignment recovery framework (misalignment gates, audit locks, recursion filters)","This is a first-pass release of a logic-gated failsafe protocol to handle misalignment in recursive or high-capacity AI systems.

The framework defines:

* Structural admission filters
* Audit-triggered lockdowns
* Persistence-boundary constraints

It’s outcome-agnostic — designed to detect structural misalignment even if external behavior looks “safe.”

GitHub repo: [AI-Failsafe-Overlay](https://github.com/oxey1978/AI-Failsafe-Overlay)

Looking for feedback or critique from a systems, logic, or alignment theory lens.",0,4,2025-07-27 12:28:10,sf1104,0.0,0.0,0.0,1.0,-0.0833333333333333,0.5333333333333333
[P] I tried implementing the CRISP paper from Google Deepmind in Python,"I spent the weekend analyzing this open-source PyTorch implementation of Google's [CRISP paper (arXiv:2505.11471)](https://arxiv.org/pdf/2505.11471). The repository provides a direct, hands-on comparison between CRISP's in-training clustering and the more traditional post-hoc approach.

For context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings *after* training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering *during* training to force the model to learn inherently ""clusterable"" representations.

The repository sets up a clean head-to-head experiment to test that claim. Here's a breakdown of the results from its built-in pipeline.

[https://github.com/sigridjineth/crisp-py](https://github.com/sigridjineth/crisp-py)

I tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.",72,5,2025-07-27 10:32:08,Ok_Rub1689,-0.4588,0.038,0.059,0.903,0.1243589743589743,0.4782051282051282
[P] LLM Economist: Large Population Models and Mechanism Design via Multi‑Agent Language Simulacra,"Co-author here. We’ve released a new preprint, **LLM Economist**, which explores how LLM-based agents can learn and optimize economic policy through multi-agent simulation.

In our setup, a planner agent proposes marginal tax schedules, while a population of 100 worker agents respond by choosing how much labor to supply based on their individual personas. All agents are instantiated from a calibrated skill and demographic prior and operate entirely through language—interacting via in-context messages and JSON actions.

The planner observes these behaviors and adjusts tax policy over time to maximize social welfare (happiness). No gradient updates are used; instead, the planner learns directly through repeated text-based interactions and the culminating societal/individual reward. This yields realistic economic dynamics, including responding to the Lucas Critique, behavioral adaptation, and tradeoffs between equity and efficiency.

**Key contributions:**

* A two-tier in-context RL framework using LLMs for both workers and planner.
* Persona-conditioned agent population grounded in U.S. Census-like statistics.
* Emergent economic responses to policy changes, such as implicit varying elasticity and participation behavior.
* Stackelberg-inspired simulation loop where planner and workers co-adapt.

We would welcome feedback from this community on:

* The viability of language-only RL architectures for economic modeling.
* Stability and interpretability of emergent agent behavior.
* Broader implications for coordination and mechanism design with LLMs.

Paper: [https://arxiv.org/abs/2507.15815](https://arxiv.org/abs/2507.15815)  
Code: [https://github.com/sethkarten/LLM-Economist](https://github.com/sethkarten/LLM-Economist)

Happy to answer questions or discuss possible extensions.",14,8,2025-07-26 22:54:02,PokeAgentChallenge,0.9432,0.076,0.009,0.915,0.1166975881261595,0.4908627087198516
[P] LLM Context Manager,"Hi, i built something! An LLM Context Manager, an inference optimization system for conversations. it uses branching and a novel algorithm contextual scaffolding algorithm (CSA) to smartly manage the context that is fed into the model. The model is fed only with context from previous conversation it needs to answer a prompt. This prevents context pollution/context rot. Please do check it out and give feedback what you think about it. Thanks [https://github.com/theabhinav0231/LLM-Context-Manager](https://github.com/theabhinav0231/LLM-Context-Manager)",8,7,2025-07-26 14:56:24,abhinav02_31,0.9041,0.175,0.0,0.825,0.0619047619047619,0.5023809523809524
[P] Sub-millisecond GPU Task Queue: Optimized CUDA Kernels for Small-Batch ML Inference on GTX 1650.,"Over the past month, I’ve been working on writing high-throughput, low-latency CUDA kernels for small-batch inference workloads typical in real-time ML use cases (e.g., finance, RL serving).

Despite running on a GTX 1650 (consumer laptop GPU), I achieved:

* **93,563 ops/sec**
* **0.011 ms median latency**
* **7.3× speedup over PyTorch (float32 GEMV)**
* **30–40% faster than cuBLAS batched GEMV** (in small-batch regime)

This was done by hand-optimizing a set of three core kernels:

* Batched GEMV
* Softmax
* Vector elementwise ops (e.g., affine transforms)

# Engineering Highlights:

* `float4` **vectorization** with proper alignment checks
* **128-byte staged shared memory blocks** (using padding for bank conflict mitigation)
* **Thread-per-output-element grid strategy**
* **Aggressive loop unrolling** and warp-aware memory access
* Benchmarked with **CUDA events**, median+IQR over 1,000 trials

# Why it matters:

cuBLAS (and by extension PyTorch) is heavily tuned for large-batch throughput, but small-batch latency suffers. For real-time systems (e.g., financial models or reinforcement learning), this is a major bottleneck.

This kernel suite shows that even with modest hardware, you can cut inference latency significantly below PyTorch/cuBLAS levels through architecture-aware programming.

# Links:

* [GitHub source & benchmark code](https://github.com/shreshthkapai/cuda_latency_benchmark)
* [Full write-up on Medium](https://medium.com/@shreshthkapai/sub-millisecond-gpu-task-queue-breaking-pytorchs-latency-bottleneck-b6f3d3f2e895)

Would love to hear feedback from others doing similar work—especially around kernel tuning strategies, warp divergence handling, and memory hierarchy tradeoffs.",70,11,2025-07-26 14:53:32,shreshthkapai,0.2732,0.048,0.038,0.914,0.0142361111111111,0.4895833333333332
[D] Why CDF normalization is not used in ML? Leads to more uniform distributions - better for generalization,"CDF/EDF normalization to nearly uniform distributions is very popular in finance, but I haven't seen it before in ML - is there a reason?

We have made tests with KAN (by just adding normalized Gaussian CDF after batch norm), and such more uniform distributions can be described with smaller models, which are better for generalization: [https://arxiv.org/pdf/2507.13393](https://arxiv.org/pdf/2507.13393)

Where in ML such CDF normalization could find applications? Any other interesting nonstandard normalization approaches?",114,28,2025-07-26 04:01:55,jarekduda,0.9167,0.137,0.0,0.863,0.20875,0.5645833333333333
[D] Do you think that Muon Optimizer can be viewed through the lens of explore-exploit?,"Recent research shows that the Muon optimizer can achieve comparable loss with significantly less data, without requiring any changes to the network architecture. This suggests that there might be something fundamentally important at play in Muon, especially after years of Adam’s dominance. After looking deeper into how Muon works, I started to wonder if it might be understood through the lens of the exploration-exploitation tradeoff in training dynamics. I’d love to hear your thoughts on this.

The full analysis is written here:
https://paperplanet.github.io/posts/muon-a-explore-exploit-perspective/",23,20,2025-07-26 08:36:26,paperplanet07,0.9186,0.159,0.021,0.82,0.1805555555555555,0.5777777777777778
[R] Training small transformer model on WikiText2 from scratch,"Currently I'm using this codebase to train small decoder-only transformer models on WikiText2. The hyperparameters aren't tuned well though, the perplexity starts increasing after 20 epochs using the default hyperparameters in this repository. [https://github.com/huggingface/naacl\_transfer\_learning\_tutorial](https://github.com/huggingface/naacl_transfer_learning_tutorial)

Do you know any of open-sourced repositories that get better results on this baseline?

[https://x.com/Tim\_Dettmers/status/1245805495895511042](https://x.com/Tim_Dettmers/status/1245805495895511042) This post states that a perplexity of 107 is possible with transformers.

[https://github.com/pytorch/examples/blob/main/word\_language\_model/model.py](https://github.com/pytorch/examples/blob/main/word_language_model/model.py) This official PyTorch repository also has an implementation, but it uses encoder-decoder models (not decoder-only transformers like GPT2).",3,0,2025-07-26 08:09:01,random_sydneysider,0.5849,0.057,0.015,0.928,0.0,0.54
[D] Constructing semantic spaces from given spaces?,"I want to share [a working draft ](https://www.orges-leka.de/constructing_semantic_spaces_from_given_spaces.pdf)from me which discusses how to construct semantic spaces from given ones and how to reverse this process in order to infer the semantic meaning between two words given a database of sequence of words with similarity measures between them. This writing is a followup of my informal writing in [representing logic in semantic spaces](https://www.orges-leka.de/semantic_space_of_logic.pdf). Any thoughts for discussion?

",2,0,2025-07-26 07:01:21,musescore1983,0.4329,0.054,0.0,0.946,0.0,0.0
"[P] Tried Everything, Still Failing at CSLR with Transformer-Based Model","Hi all,  
I’ve been stuck on this problem for a long time and I’m honestly going a bit insane trying to figure out what’s wrong. I’m working on a **Continuous Sign Language Recognition (CSLR)** model using the **RWTH-PHOENIX-Weather 2014** dataset. My approach is based on transformers and uses **ViViT** as the video encoder.

# Model Overview:

**Dual-stream architecture**:

* One stream processes the *normal RGB video*, the other processes *keypoint video* (generated using Mediapipe).
* Both streams are encoded using **ViViT (depth = 12)**.

**Fusion mechanism**:

* I insert **cross-attention** layers *after the 4th and 8th ViViT blocks* to allow interaction between the two streams.
* I also added **adapter modules** in the rest of the blocks to encourage mutual learning without overwhelming either stream.

**Decoding**:

I’ve tried *many decoding strategies*, and none have worked reliably:

* **T5 Decoder**: Didn't work well, probably due to integration issues since T5 is a text to text model.
* **PyTorch’s TransformerDecoder (Tf)**:
   * Decoded each stream separately and then merged outputs with cross-attention.
   * Fused the encodings (add/concat) and decoded using a single decoder.
   * Decoded with two separate decoders (one for each stream), each with its own FC layer.

**ViViT Pretraining**:

Tried pretraining a ViViT encoder for 96-frame inputs.

Still couldn’t get good results even after swapping it into the decoder pipelines above.

# Training:

* **Loss**: CrossEntropyLoss
* **Optimizer**: Adam
* Tried different learning rates, schedulers, and variations of model depth and fusion strategy.

# 

Nothing is working. The model doesn’t seem to converge well, and validation metrics stay flat or noisy. I’m not sure if I’m making a fundamental design mistake (especially in decoder fusion), or if the model is just too complex and unstable to train end-to-end from scratch on PHOENIX14.

I would deeply appreciate any insights or advice. I’ve been working on this for weeks, and it’s starting to really affect my motivation. Thank you.

**TL;DR**: I’m using a dual-stream ViViT + TransformerDecoder setup for CSLR on PHOENIX14. Tried several fusion/decoding methods, but nothing works. I need advice or a sanity check.",7,7,2025-07-26 02:56:59,Naneet_Aleart_Ok,-0.4247,0.039,0.059,0.902,0.036525974025974,0.5330988455988456
[D] How to improve pretraining pipeline,"I’m interested in large language models, so I decided to build a pretraining pipeline, and was wondering what I should add to it before I start my run. I’m trying to pretrain a GPT-2 Small(or maybe medium) sized model on an 11b token dataset with web text and code. I made some tweaks to the model architecture, adding Flash Attention, RMSNorm, SwiGLU, and RoPE. I linearly warmup the batch size from 32k to 525k tokens over the first ~100m tokens, and also have a Cosine learning rate schedule with a warmup over the first 3.2m tokens. I’m using the free Kaggle TPU v3-8(I use the save and run all feature to run my code overnight, and I split training up between multiple of these sessions). I’m using FSDP through Torch XLA for parralelism, and I log metrics to Weights and Biases. Finally, I upsample data from TinyStories early in training, as I have found that it helps the model converge faster. What should I add to my pipeline to make it closer to the pretraining code used in top companies? Also, could I realistically train this model with SFT and RLHF to be a simple chatbot?

Edit: I’m still in high school, so I’m doing this in my spare time. I might have to prioritize things that aren’t too compute-heavy/time-intensive.",5,6,2025-07-26 00:21:03,New-Skin-5064,0.949,0.089,0.0,0.911,0.1931168831168831,0.4629437229437228
[D] AACL VS. AAAI for NLP papers,"AAAI is sometimes considered ~~lower tier~~ \[edit: less preferred\] for ML research communities compared with ICML/Neurips/ICLR and ACL conferences. but still it is a fairly good brand overall and has steady quality. This year AAAI and AACL-IJCNLP deadlines are about the same. For an NLP methodology paper, which venue is more preferable given that confidence of acceptance is relatively high?",0,8,2025-07-26 00:14:49,Ok-Atmosphere3141,0.9231,0.163,0.0,0.837,0.1942857142857142,0.333095238095238
[D] Is this Lambda AI rig in demand anymore?,"Hi guys, I got an AI rig donated to me, and while I've been toying with some LLMs on it, I'm no ML professional, so I feel like someone else probably has a better use for it than just spinning their own chatbot. I was curious to hear from this community whether it'd be worth it to sell the thing, or if it's old enough now that it's only worth keeping around as an end-user machine. I've done some googling and there's only a little demand for Lambda machines in general, and I'm just not in the world of ML enough to know any better.

Here are the specs:

* Ryzen threadripper 3960X, 64GB RAM
* 2x RTX 3080 blower style, 10GB VRAM each

Thanks in advance!",1,5,2025-07-25 22:41:21,yaboproductions,0.8927,0.125,0.057,0.818,0.1608333333333333,0.5133333333333333
[P] Build an MLP and Visualize Training in Real Time In Your Browser,"Hi everyone,

I built Grada, a browser-based tool that lets you build and train an mlp from scratch and visualize the training process in real time. Built entirely from scratch (no libraries) so it's not the fastest of course but it's fast enough to train simple models.

The goal is to make neural network training more transparent and intuitive, especially for those learning how MLPs work under the hood. You can tweak hyperparameters on the fly and immediately see how the model responds during training. There's also a pretrained handwritten digit classifier you can interact with to see inference in action.

[https://saliherdemk.github.io/Grada/](https://saliherdemk.github.io/Grada/)",4,1,2025-07-25 22:32:05,saliherdemk,0.0,0.0,0.0,1.0,0.2,0.5182142857142857
"[D]: DDPMs: Training learns to undo entire noise, but at sampling time, noise removed step by step, why?","During training, diffusion models are trained to predict the full noise that was added to a clean image. However, during inference (sampling), the same model is used to gradually remove noise step by step over many `T` iterations. Why does this approach work, even though the model was never explicitly trained to denoise incrementally?

[Algos from the DDPM paper](https://preview.redd.it/denzyibu72ff1.png?width=1088&format=png&auto=webp&s=54994920af52bb721b1362eae1a226e340674b82)

",14,9,2025-07-25 18:00:02,Previous-Scheme-5949,0.6237,0.052,0.0,0.948,0.2433333333333333,0.5
[D] BMVC 2025 Results Discussion,"I just got the email. Unfortunately rejected but cannot see the reviews, only that my paper and all the ones I reviewed were on the ""Rejected"" tab on OpenReview. Can anyone see yours? What was your experience?",8,13,2025-07-25 17:44:11,xEdwin23x,-0.4956,0.0,0.1,0.9,-0.25,1.0
"[R] NeurIPS 2025 D&B: ""The evaluation is limited to 15 open-weights models ... Score: 3""","I'm pretty shocked how the only reviewer criticism on our benchmark paper (3.5/6) was that our paper included *only* 15 open weights models and that we didn't evaluate our benchmark on SoTA commercial models (that would cost \~10-15k $ to do).

I mean how superficial does it get to reject a paper not because something is wrong about its design or that it isn't a novel/useful benchmark, but because we don't want to pay thousands of dollars to OpenAI/Google/Anthropic to evaluate (and promote) their models.

How academic is it to restrict the ability to publish to the big labs / companies in wealthy countries that have the money lying around to do that?!",327,35,2025-07-25 17:28:52,kaitzu,-0.7802,0.07,0.133,0.797,-0.0160714285714285,0.6254120879120879
"[N] PapersWithCode sunsets, new HuggingFace Papers UI","After a month of discussions here about [problems](https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d_paperswithcode_has_been_compromised/) [with](https://www.reddit.com/r/MachineLearning/comments/1lqedrt/d_paper_with_code_is_completely_down/) the PapersWithCode site staying online and hosting spam, the [PapersWithCode.com](http://PapersWithCode.com) URL now redirects to their GitHub

According to Julien Chaumond of HF, they have ""partnered with PapersWithCode and Meta to build a successor"" on  [https://huggingface.co/papers/trending](https://huggingface.co/papers/trending) . There have been links to browse papers and associated models and datasets on HF for some time, but potentially they are going to give it some additional attention in the coming weeks.",105,16,2025-07-25 13:32:31,prototypist,-0.0772,0.018,0.021,0.961,0.0681818181818181,0.7272727272727273
[P] 🚀Built another 124m parameters transformer based model from scratch.This time with multi GPU training with DDP. Inspired from nanoGPT but redesigned to suit my own training pipeline.Model and training code is here,"https://huggingface.co/abhinavv3/MEMGPT

Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE. Also tryingg to implement some concepts from research papers like Memorizing Transformers.
Bt these changes haven't been implemented yet.",6,1,2025-07-25 11:27:50,Remarkable-Ad3290,0.6542,0.067,0.0,0.933,0.3,0.7
[D] [MLOps] How to Handle Accuracy Drop in a Few Models During Mass Migration to a New Container?,"Hi all,

I’m currently facing a challenge in migrating ML models and could use some guidance from the MLOps community.

# Background:

We have around 100 ML models running in production, each serving different clients. These models were trained and deployed using older versions of libraries such as `scikit-learn` and `xgboost`.

As part of our upgrade process, we're building a new Docker container with updated versions of these libraries. We're retraining all the models inside this new container and comparing their performance with the existing ones.

We are following a blue-green deployment approach:

* Retrain all models in the new container.
* Compare performance metrics (accuracy, F1, AUC, etc.).
* If all models pass, switch production traffic to the new container.

# Current Challenge:

After retraining, 95 models show the same or improved accuracy. However, 5 models show a noticeable drop in performance. These 5 models are blocking the full switch to the new container.

# Questions:

1. Should we proceed with migrating only the 95 successful models and leave the 5 on the old setup?
2. Is it acceptable to maintain a hybrid environment where some models run on the old container and others on the new one?
3. Should we invest time in re-tuning or debugging the 5 failing models before migration?
4. How do others handle partial failures during large-scale model migrations?

# Stack:

* Model frameworks: scikit-learn, XGBoost
* Containerization: Docker
* Deployment strategy: Blue-Green
* CI/CD: Planned via GitHub Actions
* Planning to add MLflow or Weights & Biases for tracking and comparison

Would really appreciate insights from anyone who has handled similar large-scale migrations. Thank you.",6,10,2025-07-25 08:44:45,xeenxavier,0.6063,0.064,0.05,0.886,0.1009222661396574,0.4147891963109354
[D] Review Confidence Guidelines,"* 5. I'm a world expert. I resent wasting my precious time on your little paper and I'll tear it to shreds unless you cite me at least 3 times.
* 4. I know the area.
* 3. I don't know the area.
* 2. I just started my masters and my supervisor gave me 5 papers to review. Please don't be mad if I mess up.
* 1. What's the deep learning?

",62,6,2025-07-25 06:08:21,Unique_Revolution_59,0.7208,0.157,0.091,0.752,-0.13125,0.5791666666666666
Help Needed: Accurate Offline Table Extraction from Scanned Forms [P],"I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  

Current Approach & Challenges  
1. OCR Tools (e.g., Tesseract):  
   - Used to identify the table and extract text.  
   - Issue: OCR accuracy is inconsistent—sometimes the table isn’t recognized or is parsed incorrectly.  

2. Post-OCR Correction (e.g., Mistral):  
   - A language model refines the extracted text.  
   - Issue: Poor results due to upstream OCR errors.  

Despite spending hours on this workflow, I haven’t achieved reliable extraction.  

Alternative Solution (Online Tools Work, but Local Execution is Required)  
- Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.  
- Constraint: The solution must run entirely locally (no internet connection).  

Attempted new Workflow (DINOv2 + Multimodal LLM)  
1. Step 1: Image Embedding with DINOv2  
   - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).  
   - Issue: Did not produce usable results—possibly due to incorrect implementation or model limitations. Is this approach even correct?

2. Step 2: Multimodal LLM Processing  
   - Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.  
   - Blocker: Step 2 failed, didn’t got usable output 

Question  
Is there a local, offline-compatible method to replicate the quality of online extraction tools? For example:  
- Are there better vision models than DINOv2 for this task?  
- Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?  
- Any tips for debugging DINOv2 missteps?",3,7,2025-07-25 02:09:05,Antelito83,0.9013,0.085,0.038,0.877,0.0773491214667685,0.3742869875222815
[D] How to calculate the memory needed to train your model on GPU,"I want to be able to know if my model should fit on a single GPU a head of time before I start training. I assume this is what most people do (if not, please share your approach). Here's a formula that I came across to estimate the memory requirements - except I'm not sure how to calculate the activation memory. Does anyone have a rule of thumb for the activation memory? I heard it scales linearly with batch size, so what would be the baseline assuming a batch size of 1? 

Formula (ex. 32bit model = 32 bit x (1 byte / 8 bit) = 4 bytes per parameter )

\- parameter memory = bytes x num params

\- optimizer states = 2 x bytes x num params (momentum + velocity for adam)

\- gradient memory = bytes x num params

\- activations = ? (somewhere I heard it was roughly 2 x bytes x num params)",9,9,2025-07-25 00:55:37,Secret_Valuable_Yes,0.2565,0.047,0.04,0.913,0.1630952380952381,0.5046957671957671
[R] Question about the NeurIPS 2025 rebuttal process,"The NeurIPS 2025 FAQ ([https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ](https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ)) mentions that rebuttals are limited to 6,000 characters per review, plus an additional 6,000-character global rebuttal (with the option to upload a one-page PDF for figures/tables).

However, the OpenReview notification I received states a 10,000-character limit per review and doesn’t mention anything about a global rebuttal.

Does anyone know which guideline I should follow? Should I assume OpenReview’s limits take precedence?",6,2,2025-07-24 23:50:08,Dismal_Moment5761,-0.3094,0.0,0.033,0.967,-0.0238095238095238,0.0476190476190476
[D] Tried of the same review pattern,"Lately, I’ve been really disappointed with the review process. There seems to be a recurring pattern in the weaknesses reviewers raise, and it’s frustrating:

1. ""No novelty"" – even when the paper introduces a new idea that beats the state of the art, just because it reuses components from other fields. No one else has achieved these results or approached the problem in the same way. So why dismiss it as lacking novelty?

2. Misunderstanding the content – reviewers asking questions that are already clearly answered in the paper. It feels like the paper wasn’t read carefully, if at all.


I’m not claiming my paper is perfect—it’s definitely not. But seriously... WTF?",124,21,2025-07-24 20:51:59,MalumaDev,0.2813,0.091,0.103,0.806,-0.1893308080808081,0.5732954545454546
[D] - NeurIPS'2025 D&B Track,"# Hey everyone,

I think it's a good idea to have a separate discussion for the datasets and benchmarks track, feel free to share your scores or any other relevant feedback.

Let’s keep things constructive and supportive. Good luck to all!",27,28,2025-07-24 17:17:28,,0.9412,0.324,0.0,0.676,0.4583333333333333,0.7125
[D] ACL ARR July 2025 Discussion,"Discussion thread.

",12,13,2025-07-24 11:32:32,Practical_Pomelo_636,0.0,0.0,0.0,1.0,0.0,0.0
[P] Issues in Training Differential Attention Transformer.,"Hey folks,

I have been trying to implement a research paper that utilized differential transformer block  attention [https://arxiv.org/abs/2502.13189](https://arxiv.org/abs/2502.13189) as a means to denoise background noise from  biological sounds, While training the model I am constantly running into numeric instability (nan loss), specifically this step : --

lambda\_val = torch.exp(lambda\_q1\_dot\_k1) - torch.exp(lambda\_q2\_dot\_k2) + self.lambda\_init

Most probably due to exponential terms assuming large values. I did try clamping the lambda values to avoid this but doing this is resulting in diverging loss values after few epochs.  Anybody how might  have tried this block can suggest any fixes or whether the clamping approach is the right way in terms of loss optimization (I know  clamping is not the best thing for loss optimization ) ?",8,2,2025-07-24 08:48:27,1h3_fool,-0.7752,0.1,0.148,0.753,0.2392857142857143,0.367517006802721
[D] Why is there such a noticeable difference between Stat and CS section of Arxiv? Any underlying reasons?,"As a math major, I was interested in seeing what different fields of mathematical research looks like. I decided to just browse the Arxiv, but I can't help to notice the difference between Stat.ML and CS.LG sections.

From my understanding, they are both suppose to be about Machine Learning research, but what I found was that many of the CS.LG articles applied ML to novel scenarios instead of actually researching new mathematical/statistical models. Why are these considered ML research, if they are not researching ML but using it?

Does this reflect a bigger divide within the machine learning research field? Is there some fields in ML that are more suited for people interested in math research? if so, are those generally hosted in the math/stats department, or still under the CS department?",28,18,2025-07-24 03:59:23,Collegiate_Society2,0.8005,0.073,0.019,0.908,0.1457386363636363,0.4295454545454545
[D] EMNLP 2025 Meta Reviews,Has anyone received the meta reviews yet for the ARR May 2025 cycle (EMNLP 2025)? Let's discuss.,2,1,2025-07-24 03:15:28,Mysterious_Flan5357,0.0,0.0,0.0,1.0,0.0,0.0
[R] treemind: A High-Performance Library for Explaining Tree-Based Models,"I am pleased to introduce [`treemind`](https://github.com/sametcopur/treemind/), a high-performance Python library for interpreting tree-based models.

Whether you're auditing models, debugging feature behavior, or exploring feature interactions, `treemind` provides a robust and scalable solution with meaningful visual explanations.

* **Feature Analysis** Understand how individual features influence model predictions across different split intervals.
* **Interaction Detection** Automatically detect and rank pairwise or higher-order feature interactions.
* **Model Support** Works seamlessly with LightGBM, XGBoost, CatBoost, scikit-learn, and perpetual.
* **Performance Optimized** Fast even on deep and wide ensembles via Cython-backed internals.
* **Visualizations** Includes a plotting module for interaction maps, importance heatmaps, feature influence charts, and more.

**Installation**

    pip install treemind

**One-Dimensional Feature Explanation**

Each row in the table shows how the model behaves within a specific range of the selected feature.  
The `value` column represents the average prediction in that interval, making it easier to identify which value ranges influence the model most.

    | worst_texture_lb | worst_texture_ub |   value   |   std    |  count  |
    |------------------|------------------|-----------|----------|---------|
    | -inf             | 18.460           | 3.185128  | 8.479232 | 402.24  |
    | 18.460           | 19.300           | 3.160656  | 8.519873 | 402.39  |
    | 19.300           | 19.415           | 3.119814  | 8.489262 | 401.85  |
    | 19.415           | 20.225           | 3.101601  | 8.490439 | 402.55  |
    | 20.225           | 20.360           | 2.772929  | 8.711773 | 433.16  |

**Feature Plot**  


https://preview.redd.it/cbmyl38y7oef1.png?width=1189&format=png&auto=webp&s=5c7657a74bdebf5c51332ddc856f5de3d5583de9

# 

**Two Dimensional Interaction Plot**

The plot shows how the model's prediction varies across value combinations of two features. It highlights regions where their joint influence is strongest, revealing important interactions.

https://preview.redd.it/2zb1ra5h8oef1.png?width=943&format=png&auto=webp&s=6b1149795ce202f50f47f0264013eb225e09de2c

# Learn More

* Documentation: [https://treemind.readthedocs.io](https://treemind.readthedocs.io)
* Github: [https://github.com/sametcopur/treemind/](https://github.com/sametcopur/treemind/)
* Algorithm Details: [How It Works](https://treemind.readthedocs.io/en/latest/algorithm.html)
* Benchmarks: [Performance Evaluation](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)

Feedback and contributions are welcome. If you're working on model interpretability, we'd love to hear your thoughts.",7,1,2025-07-23 19:06:08,zedeleyici3401,0.9844,0.13,0.0,0.87,0.0204545454545454,0.6102272727272727
[R] PhD scholarship at Victoria University of Wellington in machine learning for Volcano forecasting,"We are seeking a highly motivated PhD student to join our multidisciplinary volcanic hazards research team at Victoria University of Wellington, New Zealand. This exciting project focuses on developing cutting-edge diffusion-based machine learning models to forecast volcanic activities, significantly enhancing our ability to predict eruption dynamics.

🔹 Scholarship details:

Generous stipend: NZ$35,000/year for 3 years (possible extension).

Full tuition fees covered.

Funding for international conferences and collaboration visits in Europe.

Fieldwork opportunities.

🔹 Ideal candidates:

Background in Machine Learning, Data Science, Computer Science, or related fields.

Strong Python skills.

Excellent communication in English.

Previous publications in top-tier AI conferences/journals.

🔹 Supervisors: Prof. Bastiaan Kleijn, Dr. Felix Yan, Dr. Finnigan Illsley-Kemp

📅 Applications reviewed from: September 1st, 2025 (Flexible start date from October 2025 onwards).

For inquiries and applications, please contact me directly at 📧 [felix.yan@vuw.ac.nz](mailto:felix.yan@vuw.ac.nz). Application documents include your CV, transcript, Master's thesis, and publications.

Feel free to share this fantastic opportunity with your network!",6,4,2025-07-23 21:56:39,No_Cost_4788,0.9915,0.233,0.0,0.767,0.2805018939393939,0.601221590909091
The Serial Scaling Hypothesis,,40,11,2025-07-23 20:02:40,HealthyInstance9182,0.0,0.0,0.0,1.0,0.0,0.0
[D] - NeurIPS'2025 Reviews,"Hey everyone,

NeurIPS 2025 reviews should be dropping soon (July 24th AoE), and I thought it might be a good idea to start a thread where we can share our thoughts, experiences, and reactions.

Feel free to post your initial impressions, any surprises (good or bad), questions about rebuttals, or just how you’re feeling about the process this year. Whether it’s your first submission or your tenth, you’re not alone in the rollercoaster.

Let’s keep things constructive and supportive. Good luck to all!",235,915,2025-07-23 09:41:14,Proof-Marsupial-5367,0.963,0.246,0.0,0.754,0.340625,0.575
[D] Working on a ML in Quant Finance Conf - Need your guidance,"Hellow ML/Al folks,

I'm working on an upcoming Machine Learning in Quantitative Finance conference, my role is to outreach and engage relevant professionals.

While I've handled other events before, this field is new to me. I'd appreciate any quick tips, resources, or key concepts to get up to speed.

Also, if you have advice on how to approach senior roles (MDs, Heads of Departments, Chiefs, Presidents) effectively in this space.

Thanks",6,2,2025-07-22 19:47:57,Witty_Doughnut3497,0.9001,0.144,0.0,0.856,0.2206709956709956,0.6042207792207792
[D] Is there anyone using GRPO in their company?,"I am considering doing RL as a service for companies looking to finetune LLMs, and I have doubts. It is a lot more compute-intensive. it promises data efficiency, but training is more unstable, it is less straightforward to debug, and there are so many moving parts in infra and environment setup that make reproducibility very difficult unless you just have the compute to scale. was wondering how far RL for agents is from adoption? are there people experimenting with this in your work/training custom reasoning models? is it worth it?",32,16,2025-07-22 17:46:32,Classic_Eggplant8827,-0.7102,0.057,0.095,0.848,0.1822916666666667,0.5052083333333333
[D]  Is it me or is ECAI really bad this year?,"I have one accepted paper and another one rejected. The review and meta-review quality was really subpar. It felt like most of the responses we got, on both sides of the spectrum, came from underexperinced reviewers. I am all for letting undergrads read, review, and get experience, but I always review the paper by myself first and would never submit theirs as is. This really boggles me because I always thought ECAI is a good conference, but this year I can't help but feel a little bit embarrassed to even go there.

I have not submitted to other conferences yet. So, I wonder if there is a trend.",44,20,2025-07-22 09:52:41,whereismycatyo,-0.4795,0.06,0.086,0.854,0.1046875,0.421875
[R] Gaussian Process to Approximate Vehicle Dynamics,"A while back, I was working on localization with GPs and had a thought: could we encode vehicle dynamics directly into the GP kernel?

I know GPs are used to model parameters in physical models. But my idea was that a car’s trajectory resembles a smooth GP sample. A faster car takes smoother paths, just like longer length scales produce smoother GPs. Instead of modeling `y(x)` directly, I used cumulative distance `s` as the input, and trained two separate GPs:

* `x(s)`
* `y(s)`

Both use an RBF kernel. So we are basically maximizing the probability function:

https://preview.redd.it/ksoisiw9r9ef1.png?width=430&format=png&auto=webp&s=e01f1827f3c74550f596de2ee02fe4b7d2e93178

Which translates to something like

*“Given a speed, how probable is it that these data points came from this vehicle?”*

**The algorithm goes like this:**

1. Collect data
2. Optimize the kernel
3. Construct the `l(v)` function
4. Optimize the lap

I fitted the kernel’s length scale `l` as a function of speed: `l(v)`. To do this, I recorded driving data in batches at different constant speeds, optimized the GP on each batch, then fit a simple `l(v)` relation, which turned out to be very linear.

With the optimized kernel in hand, you can ask questions like:

*“Given this raceline and a speed, can my car follow it?""*

As the GP is a probabilistic model, it doesn’t give a binary answer that we requested. We could optimize for “the most likely speed” the same way we optimized the length scales. However, this would be more like asking, “What is the most likely speed this raceline can be achieved?”, which is okay for keeping your Tesla on the road, but not optimal for racing. My approach was to define an acceptable tolerance for the deviation from the raceline. With these constraints in hand, I run a heuristic window-based optimization for a given raceline:

https://i.redd.it/e7qteia2s9ef1.gif

**Results?**

Simulator executed lap plan times were close to human-driven laps. The model didn't account for acceleration limits, so actual performance fell slightly short of the predicted plan, but I think it proved the concept.

There are a lot of things that could be improved in the model. One of the biggest limitations is the independent models for x and y coordinates. Some of the things I also tried:

1. Absolute angle and cumulative distance model - This one considers the dynamics in terms of the absolute heading angle with respect to cumulative distance. This solves the problem of intercorrelation between X and Y coordinates, but introduces two more problems. First, to go back from the angle-domain, you need to integrate. This will lead to drifting errors. And even if you don’t want to go back to trajectory space, you still lose the direct link between the error definition of the two domains. And second, this function is not entirely smooth, so you need a fancier Kernel to capture the features. A Matérn at least.
2. “Unfolding the trajectory” - This was one of my favorites, since it is the closest to the analogy of modeling y relation to x directly, wiggly road style. In the original domain, you would face the multivalued problem, where for a single x-value, there can be multiple y-values. One can “unfold” the lap (loop) by reducing the corner angles until you have unfolded the points to a single-valued function. This, however, also destroys the link to the original domain error values.

Here is the code and the data if you want to make it better:  
[https://github.com/Miikkasna/gpdynalgo](https://github.com/Miikkasna/gpdynalgo)",16,0,2025-07-21 18:40:32,NorthAfternoon4930,0.9967,0.16,0.051,0.789,0.1073308270676692,0.4271303258145363
[D] Gemini officially achieves gold-medal standard at the International Mathematical Olympiad,"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/

>This year, our advanced Gemini model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions – all within the 4.5-hour competition time limit.",225,69,2025-07-21 18:28:50,currentscurrents,-0.0772,0.099,0.106,0.795,0.0857142857142857,0.1999999999999999
[D] Encoding time series data into images drawbacks,"So I've been reading many articles and reviews about encoding time series data into images, before feeding them into vision models for classification or forecasting. So this shifts the original problem from conventional time series analysis into the image domain. Yet, i didn't find any article or even a phrase that mentions that this transformation has any drawbacks or limitations.  Do you think this is possible?",25,13,2025-07-21 17:11:02,Important_Book8023,0.1531,0.057,0.036,0.908,0.1830357142857143,0.6517857142857143
[P] Echoes of GaIA: modeling evolution in biomes with AI for ecological studies.,"Hi there!

I'd like to share a project I've been working on over the last few months; **Echoes of GaIA** is a hybrid framework for modeling evolution and running biome simulations with “*living*” ecosystems using lots of AI techniques. For context, I've been working quite a few years in the software and videogame development world, but four years ago I went back to university (hasn't been easy at this stage of life, but I just finished a few days ago and finally pulled out a huge thorn I'd had for more than 15 years) and this has been my capstone project. I specialized in Computation theory and Artificial Intelligence and wanted to create a kind of ode to AI and tackle biomes holistically, since I was eager to learn all these techniques and the underlying math.

The idea was to shape a project that - although just a very modest, small gesture, symbolic I’d say - tries to contribute something toward helping heal the planet, improving climate change, etc., through Artificial Intelligence. I just wanted to share it because I think it might interest people reading this subreddit, and I cover some pretty current topics that I believe are very important.

Anyway, some of the things I've implemented:

• Climate and fauna agents based on **Reinforcement Learning**

• **Genetic algorithms** for species **evolution**

• “Equilibrium” agent (**neurosymbolic AI**) – the idea here is to balance the whole ecosystem (for now using **LSTM multivariate multihorizon with attention** and expert systems and/or **graphs** as the knowledge base)

• I also do c**omputational modeling** (but on its discrete side, not continuous) of many biological and physiological processes

It can be extended easily (I used ECS so I could have a modular component system for the biological processes of flora and fauna entities) and I've also put together a snapshot viewer and real‑time metrics (InfluxDB + Grafana).

Project website → [https://www.echoes-of-gaia.com](https://www.echoes-of-gaia.com) (turn on **sound** before clicking!! I'm quite a big nerd and wanted to set a proper ambiance)

GitHub repo → [https://github.com/geru-scotland/echoes-of-gaia](https://github.com/geru-scotland/echoes-of-gaia)

If anyone’s interested in the technical report, it's available on the site as **Main Doc** and there's also a document covering the project’s basic foundations, architecture, and main systems **Architecture doc** (those documents are only available in Spanish, unfortunately).

Any suggestions are more than welcome and, if you like it, I'd appreciate a star on GitHub. Thanks!",14,0,2025-07-21 13:47:23,Basajaun-Eidean,0.9954,0.15,0.015,0.835,0.1206249999999999,0.5356249999999998
[P] AI Learns to Play TMNT Arcade (Deep Reinforcement Learning) PPO vs Recur...,"Github: [https://github.com/paulo101977/TMNT-RecurrentPPO](https://github.com/paulo101977/TMNT-RecurrentPPO)  
  
Hey everyone!  
I’ve been training a **Recurrent PPO** agent to play the classic **Teenage Mutant Ninja Turtles (Arcade)** game using only visual input. The goal is to teach the agent to fight through the levels using memory and spatial awareness, just like a human would.

Here are some key details:

* **Environment:** TMNT Arcade via custom Gymnasium + stable-retro integration
* **Observations:** 4 stacked grayscale frames at **160×160** resolution
* **Augmentations:** Random noise, brightness shifts, and cropping to improve generalization
* **Reward Signal:** Based on score increase, boss damage, and stage progression
* **Algorithm:** Recurrent Proximal Policy Optimization (RecPPO) with CNN + LSTM
* **Framework:** PyTorch with custom training loop (inspired by SB3)

The recurrent architecture has made a big difference in stability and long-term decision making. The agent is now able to consistently beat the first few levels and is learning to prioritize enemies and avoid damage.",0,6,2025-07-21 00:29:11,AgeOfEmpires4AOE4,0.3802,0.105,0.084,0.811,-0.0580555555555555,0.3983333333333332
[D] Is transfer learning and fine-tuning still necessary with modern zero-shot models?,"Hello. I am a machine learning student, I have been doing this for a while, and I found a concept called ""transfer learning"" and topics like ""fine tuning"". In short, my dream is to be an ML or AI engineer. Lately I hear that all the models that are arriving, such as Sam Anything (Meta), Whisper (Open AI), etc., are zero-shot models that do not require tuning no matter how specific the problem is. The truth is, I ask this because right now at university we are studying PyTorch and transfer learning. and If in reality it is no longer necessary to tune models because they are zero-shot, then it does not make sense to learn architectures and know which optimizer or activation function to choose to find an accurate model. Could you please advise me and tell me what companies are actually doing? To be honest, I feel bad. I put a lot of effort into learning optimization techniques, evaluation, and model training with PyTorch.",22,18,2025-07-20 22:08:18,Altruistic-Front1745,0.9237,0.133,0.034,0.834,0.0644557823129251,0.5471938775510204
[P] Anyone interested in adding their fine-tuned / open source models to this benchmark?,"I've posted on this sub before, but context is that me and a small team are working on a [benchmark](https://www.designarena.ai/) to evaluate how good LLMs are at producing UIs and frontends that are engaging and satisfiable for people.

Right now, working on adding more models, and specifically open source models developed by individual developers (or a small group of developers). Above is the current top 10 in the leaderboard. If you're interested, just send me a DM.

Here are some requirements:

1. Inference needs to be fairly quick (max should take 3 minutes on average). Models are writing html/css/js code on the order of 4K-10K tokens on average.
2. Give us a logo and name for the provider/org you want the model to be associated with
3. An api endpoint that we can call with your desired parameters for the model. It needs to ideally be able to support a few concurrent requests at a time and around \~500 requests a day (though you can rate limit us if you would like to cap it at a smaller number)",3,3,2025-07-20 18:07:01,Accomplished-Copy332,0.9849,0.162,0.0,0.838,0.1342403628117913,0.4457482993197278
"[P] Federated Learning on a decentralized protocol (CLI demo, no central server)","This CLI command spins up a decentralized federated learning session using Parity Protocol. No central coordination, no cloud. Model training is performed across independent nodes, and final aggregation is provably deterministic.

**Example usage:**

https://preview.redd.it/4cjz7qwcb2ef1.png?width=1192&format=png&auto=webp&s=959dd70368ec15d4f607486dc464cc339d691a9e

  
\- No central coordinator  
\- Nodes train locally on custom data shards  
\- Aggregation (e.g., FedAvg) happens across verifiable nodes  
\- All results are hash-verified before acceptance  
\- Decentralized, docker-native FL infra  
\- Ideal for research in Non-IID, private datasets, or public benchmark tasks

Project:  
GitHub – [https://github.com/theblitlabs](https://github.com/theblitlabs)  
Docs – [https://blitlabs.xyz/docs](https://blitlabs.xyz/docs)  
  
We’re college devs building a trustless alternative to AWS Lambda for container-based compute, Federated learning and LLM inference

Would love feedback or help. Everything is open source and permissionless.",22,3,2025-07-20 17:17:05,Efficient-Ad-2913,0.4939,0.097,0.088,0.815,0.1272727272727272,0.4015151515151515
[P] Chess Llama - Training a tiny Llama model to play chess,"You can try it out [here!](https://lazy-guy.github.io/chess-llama/)

It's a 23M parameter model based on the Llama 3 architecture and plays at around 1400 Elo.",60,23,2025-07-20 15:47:11,LazyGuy-_-,0.5707,0.139,0.0,0.861,0.0,0.5
[P] Fine-Tuning YOLO to Watch Football (Soccer) Matches,"Hey everyone 👋 This is my first post here :D

I published a guide on fine-tuning YOLO models for custom object detection, showing how to transform a generic 80-class detector into a specialized system (using soccer match analysis as an example).

A bit of context: I've been working on a YOLO library for Elixir that supports custom models via ONNX format. Since the library can load any custom YOLO model, I created this content to show how to train your own models using Ultralytics' tooling. The approach is language-agnostic - the resulting model works with any framework supporting PyTorch or ONNX, though I demonstrate Elixir integration at the end.

This fine-tuning approach applies to various industries where domain-specific object detection is needed - sports analytics, manufacturing QC, etc.

Elixir YOLO library: [https://github.com/poeticoding/yolo\_elixir](https://github.com/poeticoding/yolo_elixir)

Video + Article about Elixir YOLO 0.2.0: [https://www.poeticoding.com/elixir-yolo-v0-2-0-yolox-support-custom-models-and-performance-boost/](https://www.poeticoding.com/elixir-yolo-v0-2-0-yolox-support-custom-models-and-performance-boost/)

Let me know if you would find interesting some videos about the details of the YOLO architecture",17,5,2025-07-20 14:48:17,alvises,0.9848,0.194,0.0,0.806,0.1222222222222222,0.6203703703703703
"[D] Monorepos for AI Projects: The Good, the Bad, and the Ugly",,0,1,2025-07-20 11:54:56,iamjessew,-0.5994,0.155,0.364,0.481,-0.2333333333333332,0.7555555555555555
[D] Set of sequences input for transformers,"Hi all. A small question regarding encoding the position of inputs to a transformer model.

How would you encode a set of sequences to a (bidirectional) transformer? For a sequence we have positional encodings. For a set we can just work without them. What about a set of sequences {s\_1, ..., s\_n}, where each s\_1, ..., s\_n is a sequence, but their relative order does not matter?",0,19,2025-07-20 11:50:59,alexsht1,-0.1207,0.0,0.022,0.978,-0.125,0.2
[R] 3 backprop vs 1 backprop for gan discriminator training,"I am trying to train a 3D gan using 2D discriminator that take slices of the original data.

And wanted to get your opinion on two points:

1- is it better to have 3 discriminators, one per plane. Or a single discriminator and takes the embedding of the plane as input.

2-my current implementation is something like this:

\- disc real training backprop 

\- disc fake training backprop

\- r1 regularisation backprop

\- gen training backprop



What would the expected effect of summing up the losses and doing one back prop per model? which method is better.",0,2,2025-07-19 23:39:11,PassengerQuiet832,0.5859,0.095,0.052,0.853,0.1003968253968254,0.4515873015873016
[R] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation,"Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.",14,3,2025-07-19 19:49:03,RobbinDeBank,0.9934,0.21,0.019,0.771,0.0915755297334244,0.5733595352016405
[R] Raw RF MSK Ultrasound Data Request,"Hi

I'm a undergrad working on **signal processing and ML algorithms** for MSK ultrasound analysis, but I'm struggling to find **raw RF ultrasound datasets** for my work.

**The Problem:** Clinical scanners only provide processed B-mode images, but I need the raw radiofrequency data from the transducer for advanced analysis.

**Looking for:**

* Raw RF datasets from MSK ultrasound exams
* Public RF ultrasound databases

**Question:** Has anyone worked with RF ultrasound data ? Any leads on accessing research platforms or datasets would be hugely appreciated!

tried referring to PICMUS dataset , but does have enough data for training a ml model for feature extraction

Thanks for any guidance!

**TL;DR:** Need raw RF ultrasound data for MSK research. Clinical systems don't provide this. Seeking dataset sources",1,0,2025-07-19 15:21:16,Past-Technician-4211,0.8451,0.087,0.027,0.886,-0.023076923076923,0.4340326340326341
[P] Benchstreet - the benchmark for financial time series forecasting.,,1,1,2025-07-19 15:06:29,Possible-Session9849,0.0,0.0,0.0,1.0,0.0,0.0
[P] Design Arena: A benchmark for evaluating LLMs on design and frontend development,"LLMs can do math, competitive programming, and more, but can they develop applications that people actually want to use?

This benchmark tasks LLMs to create interfaces at a users’ request and then based on preference data, produces a stack ranking of the LLMs that currently are able to build the most satisfiable UI. ",4,0,2025-07-19 15:02:45,Accomplished-Copy332,0.8289,0.142,0.0,0.858,0.3,0.425
[P] Pruning benchmarks for LMs (LLaMA) and Computer Vision (timm),"Hi everyone, I am here to find a new contributor for our team's project, pruning (sparsity) benchmarks.

# Why should we develop this?

Even though there are awesome papers (i.e., Awesome-Pruning; [GitHub](https://github.com/he-y/Awesome-Pruning), [GitHub](https://github.com/hrcheng1066/awesome-pruning)) focused on pruning and sparsity, there are no (maybe... let me know if there are) open-source for fair and comprehensive benchmarks, making first-time users confused. And this made a question, ""What is SOTA in the fair environment? How can we profile them?""

# Why can PyTorch-Pruning be a fair benchmark?

Therefore, [PyTorch-Pruning](http://github.com/namgyu-youn/PyTorch-Pruning) mainly focuses on implementing a variable of pruning papers, benchmarking, and profiling in a fair baseline.

More deeply, in the Language Models (LLaMA) benchmarks, we use three evaluation metrics and prompts inspired by Wanda (Sun et al., 2023) and SparseGPT (ICML'23) :

* Model (parameters) size
* Latency : Time TO First Token (TTFT) and Time Per Output Token (TPOT) for computing total generation time
* Perplexity (PPL) scores : We compute it in same way like [Wanda](https://github.com/locuslab/wanda/blob/8e8fc87b4a2f9955baa7e76e64d5fce7fa8724a6/lib/prune.py#L214) and [SparseGPT](https://github.com/locuslab/wanda/blob/8e8fc87b4a2f9955baa7e76e64d5fce7fa8724a6/lib/prune.py#L214)
* Input Prompt : We uses `databricks-dolly-15k` like Wanda, SparseGPT

# Main Objective (Roadmap) : 2025-Q3 ([GitHub](https://github.com/namgyu-youn/PyTorch-Pruning/issues/1))

For more broad support, our main objectives are implementing or applying more pruning (sparsity) researches. If there is already implemented open-source, then it could be much easier. Please check fig1 if you have any interests.

[fig1. Roadmap : 2025-Q3](https://preview.redd.it/69h8sz9z7udf1.png?width=855&format=png&auto=webp&s=19ad89510f0c4948faec9772606f661cc3eeaa52)

> Since our goal is applying more researches for pruning (sparsity), we are not planning to apply inference engines like ONNX, TensorRT, DeepSpeed, or TorchAO. But applying those engines is definitely a long-term objective, and always welcome!

p.s., Feel free to comment if you have any ideas or advice. That could be gratefully helpful for better understanding!",5,2,2025-07-19 14:03:21,youn017,0.9919,0.179,0.011,0.81,0.329763986013986,0.529822261072261
[N] What's New in Agent Leaderboard v2?,"[Agent Leaderboard v2](https://preview.redd.it/2onzjdgb3udf1.png?width=1368&format=png&auto=webp&s=3d11b5e3ab64d3e913f8af4dc99bb78bfc202c7a)

**Here is a quick TL;DR 👇**

🧠 **GPT-4.1** tops with 62% Action Completion (AC) overall.  
⚡ **Gemini 2.5** Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).  
💸 **GPT-4.1**\-mini is *most cost-effective* at $0.014/session vs. GPT-4.1’s $0.068.  
🏭 No single model dominates across industries.  
🤖 **Grok 4** didn't lead in any metric.  
🧩 Reasoning models *underperform* compared to non-reasoning ones.  
🆕 **Kimi’s K2** leads *open-source models* with 0.53 AC, 0.90 TSQ, and $0.039/session.

Link Below:

\[Blog\]: [https://galileo.ai/blog/agent-leaderboard-v2](https://galileo.ai/blog/agent-leaderboard-v2)

\[Agent v2 Live Leaderboard\]: [https://huggingface.co/spaces/galileo-ai/agent-leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard)",9,4,2025-07-19 13:36:56,5h3r_10ck,-0.4039,0.059,0.067,0.874,-0.0286916786916786,0.4743145743145743
[R] NeuralOS: a generative OS entirely powered by neural networks,"We built NeuralOS, probably the world's most expensive operating system, running at a blazing 1.8fps on an NVIDIA H100 GPU. 😅

**What exactly is NeuralOS?**

It's an experimental generative OS that predicts every screen frame entirely from your mouse and keyboard inputs. No internet, no traditional software stack, purely hallucinated pixels.

**How does it work?**

* An RNN tracks the computer state (kind of like a traditional OS kernel, but all neural and continuous).
* A diffusion model generates the actual screen images (imagine a desktop environment, but fully neural-rendered).

The GIF shows a funny demo: NeuralOS running NeuralOS inside itself. Every single pixel you're seeing is model-generated, no network involved at all!

Long-term, our goal is to remove boundaries between software entirely and make OS fully customizable beyond fixed menus and options. Imagine asking your OS something like:

* ""Merge all my messaging apps into one interface.""
* ""Make Signal look like Messenger.""
* ""Turn the movie I'm watching into a playable video game.""

**I'm curious about your thoughts:**

* Could future OS interfaces just become human-like avatars (think Grok's Ani)? Are menus and app-specific UIs going away?
* What about fully generative games: could diffusion-based games eventually replace traditional ones?

Try the live demo here: [neural-os.com](http://neural-os.com) (you might need patience…)

More details about the project: [x.com/yuntiandeng/status/1944802154314916331](http://x.com/yuntiandeng/status/1944802154314916331)",586,74,2025-07-19 12:37:24,yuntiandeng,0.9062,0.072,0.026,0.902,0.0026679841897233,0.5832298136645963
[P] The Big LLM Architecture Comparison,,81,5,2025-07-19 12:32:45,seraschka,0.0,0.0,0.0,1.0,0.0,0.1
[D] What are the most important RLVR papers?,I am searching for the big milestone papers on RLVR to get started in the field. ,6,3,2025-07-19 10:23:44,Friendly-Angle-5367,0.2716,0.087,0.0,0.913,0.3,0.5333333333333333
[P] RetinaNet + MobileNetV2 for Edge TPU Deployment,"Hey everyone! I’m currently working on a machine learning project and wanted to get some insights from the community.

I’m building a seed classification and detection system using RetinaNet. While its default backbone is ResNet50, I plan to deploy the model on a Raspberry Pi 5 with a USB Coral Edge TPU. Due to hardware limitations, I’m looking into switching the backbone to MobileNetV2, which is more lightweight and compatible with Edge TPU deployment.

I’ve found that RetinaNet does allow custom backbones, and MobileNetV2 is supported (according to Keras), but I haven’t come across any pretrained RetinaNet + MobileNetV2 models or solid implementation references so far.

The project doesn’t require real-time detection—just image-by-image inference—so I’m hoping this setup will work well. Has anyone tried this approach? Are there any tips or resources you can recommend?

Thanks in advance!
",5,5,2025-07-19 09:58:52,gigi_yanyan,0.9545,0.13,0.0,0.87,0.1208333333333333,0.4291666666666667
[R] Paper recommendations?,"Hello guys :)  
Since I am through with my pile of papers to read, I wanted to ask you if there are any recent papers you liked and would recommend :)  
I am interested in everything that you find worthwhile, however since I need to specify my personal favorites to not get this post removed, I am mostly interested in:  
\- transformer architecture optimizations, including optimizers and losses  
\- theoretical machine learning, including scaling laws and interpretablility  
\- recent alternative models such as flow matching, lambda networks etc.  
\- and anything you think is well-done research :)

Thank you in advance,  
You never disappoint me :)

I wish you all a great day ;)",22,19,2025-07-18 18:56:20,Spiritual-Resort-606,0.9903,0.329,0.019,0.652,0.275,0.653125
[R] A Minimum Description Length Approach to Regularization in Neural Networks,"[arxiv](https://arxiv.org/abs/2505.13398)

Curious for expert opinions on this paper.  This overall philosophy resonates with me a lot: Minimum Description Length  (MDL) seems like a better objective for generalization vs. common regularization methods.  Doing so might promote much better generalization, especially in the domains where transformers / LLMs struggle. 

The paper itself is very simple: they start with ""golden"" hand-crafted RNNs, and see how various approaches react to starting at this optimum.  They assert that standard approaches, like L1, L2 norm,  and/or gradient descent do worse, and wander from the optimum. So the argument is even if these methods found a general solution, they would not stick to it. 

Of course MDL is not differentiable.  But if it is a better objective, seems worth putting more effort into differentiable approximations.  
",13,1,2025-07-18 18:50:06,marojejian,0.882,0.13,0.038,0.832,0.1625,0.4632142857142856
[D] Any promising non-Deep Learning based AI research project?,"For example, Gaussian Splatting shares some concepts with Deep Learning, but it is a different approach and mostly beats the NERF (Deep Learning based approach for the same goal)",17,8,2025-07-18 17:39:08,VR-Person,0.3506,0.09,0.0,0.91,0.1166666666666666,0.4208333333333333
[D] Liquid neural networks on time series,Anyone used differentials against time to model changes in neurons/ LNNs to model any form of time series data?,5,4,2025-07-18 15:13:03,Hot_South5225,0.0,0.0,0.0,1.0,0.0,0.0
"[P] Piaget, a language model for psychological and philosophical reasoning","I just released [Piaget](https://huggingface.co/gustavecortal/Piaget-4B), a language model finetuned on 15k psychological and philosophical reasoning traces.

Piaget is based on Qwen3 and was finetuned on a subset of open reasoning traces from [Dolphin R1](https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) and [General Reasoning](https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K).

Available sizes are: [0.6B](https://huggingface.co/gustavecortal/Piaget-0.6B), [1.7B](https://huggingface.co/gustavecortal/Piaget-1.7B), [4B](https://huggingface.co/gustavecortal/Piaget-4B), [8B](https://huggingface.co/gustavecortal/Piaget-8B).

Piaget was inspired by my position paper on emotion analysis: [Improving Language Models for Emotion Analysis: Insights from Cognitive Science](https://aclanthology.org/2024.cmcl-1.23/)

**Technical details**:

I performed domain filtering on [Dolphin R1](https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) and [General Reasoning](https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K).

Prompts were embedded, clustered with k-means (k=20 000) and majority-voted for domain labels using [Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B), following the [Intelligent Internet pipeline](https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706).

Clusters tagged psychology or philosophy were retained for LoRA finetuning (rank=8, alpha=16, max length=2048, epoch=1, batch size=16).

The resulting dataset is available [here](https://huggingface.co/datasets/gustavecortal/PsychologicalReasoning-15k).",8,2,2025-07-18 13:57:32,antcroca159,0.5106,0.034,0.0,0.966,0.0142857142857143,0.4
"[D] thoughts about ""prompt routing""  - what do you think about it?","Hey everyone,

Like many of you, I've been wrestling with the cost of using different GenAI APIs. It feels wasteful to use a powerful model like GPT-4o for a simple task that a much cheaper model like Haiku could handle perfectly.

This led me down a rabbit hole of academic research on a concept often called 'prompt routing' or 'model routing'. The core idea is to have a smart system that analyzes a prompt *before* sending it to an LLM, and then routes it to the most cost-effective model that can still deliver a high-quality response.

It seems like a really promising way to balance cost, latency, and quality. There's a surprising amount of recent research on this (I'll link some papers below for anyone interested).

I'd be grateful for some honest feedback from fellow developers. My main questions are:

* **Is this a real problem for you?** Do you find yourself manually switching between models to save costs?
* **Does this 'router' approach seem practical?** What potential pitfalls do you see?
* If a tool like this existed, what would be most important? Low latency for the routing itself? Support for many providers? Custom rule-setting?

Genuinely curious to hear if this resonates with anyone or if I'm just over-engineering a niche problem. Thanks for your input!

**Key Academic Papers on this Topic:**

* Li, Y. (2025). LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing. arXiv. [https://arxiv.org/abs/2502.02743](https://arxiv.org/abs/2502.02743)
* Wang, X., et al. (2025). MixLLM: Dynamic Routing in Mixed Large Language Models. arXiv. [https://arxiv.org/abs/2502.18482](https://arxiv.org/abs/2502.18482)
* Ong, I., et al. (2024). RouteLLM: Learning to Route LLMs with Preference Data. arXiv. [https://arxiv.org/abs/2406.18665](https://arxiv.org/abs/2406.18665)
* Shafran, A., et al. (2025). Rerouting LLM Routers. arXiv. [https://arxiv.org/html/2501.01818v1](https://arxiv.org/html/2501.01818v1)
* Varangot-Reille, C., et al. (2025). Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey. arXiv. [https://arxiv.org/html/2502.00409v2](https://arxiv.org/html/2502.00409v2)
* Jitkrittum, W., et al. (2025). Universal Model Routing for Efficient LLM Inference. arXiv. [https://arxiv.org/abs/2502.08773](https://arxiv.org/abs/2502.08773)
* and others...",9,12,2025-07-18 11:26:13,Latter-Neat8448,0.9924,0.159,0.022,0.819,0.2055088702147526,0.4670401493930906
[P] Understanding Muon: A Revolutionary Neural Network Optimizer,"https://preview.redd.it/oiupfzxptldf1.png?width=1536&format=png&auto=webp&s=ffc81d2aad36267e19040a2ce4515a933362690a

  
I just published a breakdown of Muon, the optimizer powering the new OS SOTA trillion-parameter model Kimi K2 and beating GPT-4.

💡 Why is Muon a big deal?

It rethinks how we optimize neural networks by treating weight matrices not just as numbers, but as geometric objects leading to 35% faster training with 15% fewer tokens.

Would love to hear your suggestions :)

[https://glorious-potato-19.notion.site/Understanding-Muon-A-Revolutionary-Neural-Network-Optimizer-233ffa7f40c4800eafa5cc843e039327](https://glorious-potato-19.notion.site/Understanding-Muon-A-Revolutionary-Neural-Network-Optimizer-233ffa7f40c4800eafa5cc843e039327)

https://preview.redd.it/r50mbmjrtldf1.png?width=1242&format=png&auto=webp&s=67e799f1a77dea762f8d8a459d051826bbfe37ea

",122,24,2025-07-18 09:47:35,glorious__potato,0.9318,0.197,0.025,0.778,0.284090909090909,0.5386363636363636
"[P] Building a VTON model from scratch, any advice?","Did anyone ever build a virtual try on model from scratch? Thus no open sourced models used. Such as implementing the IDM-VTON model from scratch? If so, how would you go about it.I can't find anything on the internet. Any advice, guidance would be much much appreciated!!",0,9,2025-07-17 20:47:04,Ambitious-Equal-7141,0.498,0.075,0.038,0.887,0.1041666666666666,0.3666666666666667
[P] XPINN Toolkit,"Hi folks,

I'm currently developing a framework for eXtended Physics-Informed Neural Networks (XPINNs) and would really appreciate any reviews, suggestions, or feedback!

This is my first time building a tool intended for users, so I’m figuring things out as I go. Any insights on the design, usability, or implementation would be super helpful.

*What is XPINN?*  
*XPINNs extend standard Physics-Informed Neural Networks (PINNs) by splitting the problem domain into smaller subdomains. Each subdomain is handled by a smaller PINN, and continuity is enforced via interface conditions. This can help with scaling to more complex problems.*

Here’s the GitHub repo:  
[https://github.com/BountyKing/xpinn-toolkit](https://github.com/BountyKing/xpinn-toolkit)",3,4,2025-07-17 17:02:05,poppyshit,0.893,0.131,0.025,0.845,0.0283333333333333,0.45
[D] is V-JEPA2 the GPT-2 moment?,"LLMs are inherently limited because they rely solely on textual data. The nuances of how life works, with its complex physical interactions and unspoken dynamics, simply can't be fully captured by words alone

In contrast, **V-JEPA2**, a self-supervised learning model. It learned by ""watching"" millions of hours of videos on the internet, which is enough for developing an intuitive understanding of how life works. 

In simple terms, their approach first learns extracting the predictable aspects of a video and then learns to predict what will happen next in a video at a high level.  After training, a robotic arm powered by this model imagines/predicts the consequence of its actions before choosing the best sequence of actions to execute

Overall, the model showed state-of-the-art results, but the results are not that impressive, though GPT-2 was not impressive at its time either.

Do you think this kind of self-supervised, video-based learning has revolutionary potential for AI, especially in areas requiring a deep understanding of the physical world (do you know another interesting idea for achieving this, maybe an ongoing project)? Or do you believe a different approach will ultimately lead to more groundbreaking results?",27,53,2025-07-17 09:01:39,VR-Person,-0.4397,0.038,0.052,0.91,0.1182738095238095,0.4923412698412698
[D] Is anyone this old? 🥲,[https:\/\/www.cs.cmu.edu\/\~tom\/files\/MachineLearningTomMitchell.pdf](https://preview.redd.it/sma7x3kvkddf1.png?width=1214&format=png&auto=webp&s=858a6025d48aeda1939426cf289ed24dfa46fd64),104,22,2025-07-17 06:04:22,casualcreak,0.0,0.0,0.0,1.0,0.1,0.2
"[D] Concerns about Predatory Publishers (Frontiers, MDPI) Exhibiting at ICML 2025","https://preview.redd.it/n8cdebx6xadf1.png?width=1810&format=png&auto=webp&s=228db12a27752b9fed0dbdc9f4731e31e2cd47f4

Just saw that Frontiers and MDPI are listed as book publishers at ICML 2025. Kind of shocked, honestly. Both have a reputation for questionable publishing practices.

It feels off for a top ML conference to give them this kind of platform. Anyone else concerned or know how exhibitor decisions are made?",56,10,2025-07-16 21:07:33,Smart-Art9352,0.168,0.077,0.068,0.855,0.1833333333333333,0.8333333333333334
Should a large enough network be able to learn random noise? [D],"I made my own FNN from scratch, but it has trouble learning random noise. I’m not talking about generalization, but my training MSE for regression can only get down and plateaus at around 0.05. Given all my output values are between 0 and 1. 

I thought with enough capacity a network could learn anything.

(For reference, I have 9 hidden layers with 1000 nodes using RELU)
",14,28,2025-07-16 17:52:11,ModerateSentience,0.0,0.047,0.047,0.907,-0.0189033189033189,0.5341630591630592
[D] EMNLP 2025 Meta-reviews,Shouldn't they have come out \~6 hours ago?,40,166,2025-07-16 17:20:52,AdministrativeRub484,0.0,0.0,0.0,1.0,0.0,0.0
[P [R] Deep learning-assisted SLAM to reduce computational,"I'm exploring ways to optimise SLAM performance, especially for real-time applications on low-power devices. I've been looking into hybrid deep learning approaches, specifically using SuperPoint for feature extraction and NetVLAD-lite for place recognition. My idea is to train these models offboard and run inference onboard (e.g., drones, embedded platforms) to keep compute requirements low during deployment. My reading as to which this would be more efficient would be as follows:

* Reducing the number of features needed for reliable tracking. Pruning out weak or non-repeatable points would slash descriptor matching costs
* better loop closure by reducing false positives, fewer costly optimisation cycles and requiring only one forward pass per keyframe.

I would be interested in reading your inputs and opinions.",4,1,2025-07-16 15:24:08,yungyany,0.5757,0.128,0.103,0.768,0.0474999999999999,0.5824999999999999
[D] Changing values in difficult to predict range,"I have a coworker who is trying to train a model to predict a variable for customers. It’s very niche (don’t want to dox myself) so let’s just say they are trying to predict chromosome length from other biological variables. When presenting their model, they explained that the model was having difficulty predicting values in a certain range. For example purposes let’s say this range of values was 100-200. They mentioned that in order for the model to perform better in that range they explicitly changed the values of some observations to be in that range. I’m not talking scaling or normalization or some other transformation, I mean they took a certain number of observations whose target variable was below 100 and changed the value to 150, and the same with some observations above 200.

I asked for clarification like 3 times and they very confidently said this was best practice, and no other analyst said anything. They are the “head of AI” and this work will be presented to the board. Is this not an absolutely insane thing to do or am I the idiot?

FWIW: they use chatgpt for absolutely everything. My hunch is that this is an extremely ill-informed chatgpt approach but the fact that i’m the only one who see’s any issue with this on my team is making me gaslight myself",10,8,2025-07-16 14:40:13,AngryDuckling1,0.8926,0.104,0.03,0.865,0.0029761904761904,0.6100198412698412
"[D] Guys i just got interviewed, can you help me if i was cooked ?","So i was in the CTO round of this interview for Data Scientist role , and he asked me to code a realtime face emotion age and gender detection tool without using llms and without straight up copy paste code for references , he then gave me an hour to do that but with same restrictions but i was only able to do the face recognition part ! am i cooked ?",0,20,2025-07-16 14:25:20,BarEducational9905,0.289,0.032,0.017,0.951,0.125,0.51
[P] Human Activity Recognition on STM32 Nucleo,"Hi everyone,

I recently completed a university project where I developed a Human Activity Recognition (HAR) system running on an STM32 Nucleo-F401RE microcontroller. I trained an LSTM neural network to classify activities such as walking, running, standing, going downstairs, and going upstairs, then deployed the model on the MCU for real-time inference using inertial sensors.

This was my first experience with Edge AI, and I found challenges like model optimization and latency especially interesting. I managed the entire pipeline from data collection and preprocessing to training and deployment.

I’m eager to get feedback, particularly on best practices for deploying recurrent models on resource-constrained devices, as well as strategies for improving inference speed and energy efficiency.

If you’re interested, I documented the entire process and made the code available on GitHub, along with a detailed write-up:

* [GitHub](https://github.com/pescetti-studio/HAR-EdgeAI/)
* [Medium article](https://medium.com/@crocilorenzo01/my-first-har-ai-from-dataset-to-microcontroller-114b418b1509)

Thanks in advance for any advice or pointers!",8,6,2025-07-16 14:11:58,Training_Impact_5767,0.9813,0.194,0.0,0.806,0.2607142857142857,0.4273809523809523
[R][D] Interpretability as a Side Effect? Are Activation Functions Biasing Your Models?,"**TL;DR:** Through an ablation study, it is demonstrated that current activation functions result in discrete representations, whereas a new breed of activation functions preserves data continuity. The discrete clusters emerge in geometries about individual neurons, indicating that activation functions exert a strong bias on representations. ***This reveals a causal mechanism that significantly reframes*** **many** ***interpretability phenomena, which are now shown to emerge from design choices rather than being fundamental to deep learning.***

# Overview:

Activation functions are often considered as a harmless choice, a minor tweak. Each carries slight differences in performance, but are deemed not to result in much explicit effect on internal representations. *This paper shows that this impression is incorrect.*

It demonstrates that **activation functions today lead to a representational collapse**, regardless of the task and dataset, ***acting as a strong and unappreciated inductive bias***. Such a systematic representational collapse may be limiting all model expressiveness to date. It also suggests that these discrete clusters are then detected, downstream, as numerous interpretability phenomena --- including grandmother neurons, discrete neural codes, polysemanticity, and possibly Superposition.

>This reframes the approach to interpretability, suggesting that many such patterns are artefacts of our design choices and potentially provides a unifying mechanistic theory to explain them.

The striking finding is that a different defining choice in the foundational mathematics of deep learning **can turn such an interpretability phenomenon on and off**. This paper demonstrates this, showing that such phenomena appear as a result of design choice, rather than being fundamental to our field.

When discretisation is turned off in autoencoders, performance is shown to improve frequently, and representations appear to exhibit exponential growth in representational capacity, rather than typical linear growth.

This indicates enormous consequences, not least for mechanistic interpretability. But also **encourages a reevaluation of the fundamental mathematical definitions at the base of our field**. Affecting most building blocks, including activation functions, normalisers, initialisers, regularisers, optimisers, architectures, residuals, operations, and gradient clipping, among others — indicating a foundational rethink may be appropriate with alternative axiomatic-like definitions for the field — *a new design axis that needs exploration!*

**How this was found:**

Practically all current design choices break a larger symmetry, which this paper shows is propagated into broken symmetries in representations. These broken symmetries produce clusters of representations, which then appear to emerge and are detected as interpretable phenomena. Reinstating the larger symmetry is shown to eliminate such phenomena; hence, they arise causally from symmetries in the functional forms.

This is shown to occur independently of the data or task. By swapping in symmetries, it is found that this enforced discrete nature can be eliminated, yielding smoother, likely more natural embeddings. An ablation study is conducted between these two, using autoencoders, which are shown to benefit from the new continuous symmetry definition generally.

* Ablation study between these isotropic functions, defined through a continuous 'orthogonal' symmetry (rotation+mirrors O(n)), and current functions, including Tanh and Leaky-ReLU, which feature discrete axis-permutation symmetries, (Bn) and (Sn).
* Showcases a new visual interpretability tool, the ""PPP method"". This maps out latent spaces in a clear and intuitive way!

**Implications:**

These results significantly challenge the idea that neuron-aligned features, grandmother neurons, and general-linear representational clusters are fundamental to deep learning. **This paper provides evidence that these phenomena are unintended side effects of symmetry in design choices,** arguing that ***they are not fundamental to deep learning.*** This may yield significant implications for interpretability efforts.

* **Current Interpretability may often be detecting Artefacts**. Axis-alignment, discrete coding, discrete interpretable direction, and possibly Superposition appear *not to be* spontaneous or fundamental to deep learning. Instead, they seem to be stimulated by the symmetry of model primitives, particularly the activation function is demonstrated in this study. It reveals a direct causal mechanism for their emergence, which was previously unexplained.
* **We can ""turn off"" interpretability by choosing isotropic primitives, which appear to improve performance on at least specific tasks.** *Grandmother neurons vanish!* This raises profound questions for research on interpretability. The *current methods may only work because of this imposed bias*. Does this put interpretability and expressibility at loggerheads? Interestingly, this eliminates externally applied algebra-induced structure, but some structure appears to reemerge intrinsically from data --- potentially a more fundamental interpretable phenomenon.
* **Symmetry group is an inductive bias.** Algebraic symmetry presents a new design axis—a taxonomy where each choice imposes unique inductive biases on representational geometry, necessitating further extensive research.

These results support earlier predictions made when questioning the foundational mathematics (see the paper below). Introduced are continuous symmetry primitives, where the very existence of neurons appears as an observational choice --- challenging neuron-wise independence, along with a broader symmetry-taxonomy design paradigm.

>This is believed to be a new form of choice and influence on models that has been largely undocumented until now.

Most building blocks of current deep learning (*over the last 80ish years*) mostly sit along a 'permutation branch' --- which some might be familiar with in terms of just parameters. However, this work encourages a ***redefinition of all the primitives*** and **new foundations through a broad array of alternative symmetries** \--- proposed are new 'branches' to consider (*but may take a long time to develop sufficiently, help is certainly welcomed!*).

**Distinctions:**

Despite the use of symmetry language, this direction appears substantially different and tangential from previous Geometric Deep Learning approaches, and except for its resemblance to neural collapse, this phenomenon appears distinctly different. This theory is not due to classification or one-hot encoding, but forms of primitives more generally. It is somewhat related to observations of parameter symmetry, which arise as a special case and consequence of this new broader framework.

Observation of symmetry is instead redeployed as a definitional tool for novel primitives, which appears to be a new, useful design axis. Hence, these results support the exploration of a seemingly under-explored, yet rich, avenue of research.

# Relevant Paper Links:

This paper builds upon several previous papers that encourage the exploration of a research agenda, which consists of a substantial departure from the majority of current primitive functions. This paper provides the first empirical confirmation of several predictions made in these prior works.

* [📄 **Emergence of Quantised Representations Isolated to Anisotropic Functions**](https://doi.org/10.5281/zenodo.15783098) \[New **preprint** being discussed in this post, awaiting arXiv\]
* [📄 **Isotropic Deep Learning: You Should Consider Your (Inductive) Biases**](https://doi.org/10.5281/zenodo.15476947) \[Critical Position Paper: *provides the new definitions, delves into the broad symmetry-unifying theory, shows that this approach is distinct from other topics*\]
* [📄 **The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations**](https://arxiv.org/abs/2505.13471) \[New paper extended this prior approach\]

📘 A [**Summary Blog**](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c) covers many of the main ideas being proposed in a way that is hopefully ***intuitive, approachable, and exciting!*** It also motivates the driving philosophy behind the work and potential long-term outcomes.",61,21,2025-07-16 10:02:30,GeorgeBird1,0.9961,0.085,0.033,0.882,0.1086274040167483,0.474765807962529
[R] Interesting paper on cost-aware prompt optimization (CAPO),"Just came across this prompt optimization paper that I found pretty interesting - thought others might want to check it out.

They implement a prompt tuning algorithm that uses evolutionary algorithms to optimize prompts more efficiently. It jointly optimizes both instructions and few-shot examples, which sadly have been missing in other techniques.

They seem to get Super promising results - outperforming other optimizers on GSM8K by around 20% and beat existing methods on most benchmarks, while  being more efficient.

What I particularly liked was their implementation with the Promptolution framework - seems quite industry-ready compared to most academic code.

Paper [https://openreview.net/forum?id=UweaRrg9D0#discussion](https://openreview.net/forum?id=UweaRrg9D0#discussion)  

Code [https://github.com/finitearth/capo](https://github.com/finitearth/capo)",14,2,2025-07-16 05:57:40,YammaTV,0.9865,0.3,0.037,0.663,0.2288888888888888,0.5177777777777778
[P] LSTM to recognize baseball players based on their swing keypoint data,"
I want to make some kind of tool where it can identify professional baseball players based on a video of their swing.

- Extracts pose keypoint data from that professional player (done)

- Runs the keypoint time series into a LSTM model 

- Model classifies this sequence of keypoints to a specific player 

Is this possible? My main concern is that baseball swings numerically look so similar so I’m not sure if a model can pick up on the different nuances of professional player swings. Any ideas would be great.

https://youtu.be/YYC9aS60Q60?si=uWs1hX2J5SHfGkii",5,13,2025-07-16 03:21:34,danielwilu2525,0.5475,0.058,0.022,0.92,0.1469696969696969,0.4815656565656565
[R] Interactive Probabilistic Neural Network Decision Matrix Model,"I made this model while procrastinating a project of mine. I put a lot of effort into this and would appreciate feedback. its interactive so you can move the camera zoom rotate and pan. pressing 1 through 0, will light up the network layer by layer from the entry node to the exit ring. every link was created probabilistically and very deterministically. every link has significance and is unique, in a very reproduceable fashion. :P I learned a lot making this and I hope you will learn something new or pick up a new insight from playing with it. Its time to kick the learning into overdrive. lets do this.

[https://hf-laboratories.github.io/Interactive-Probabilistic-Neural-Network-Decision-Matrix/](https://hf-laboratories.github.io/Interactive-Probabilistic-Neural-Network-Decision-Matrix/)",11,8,2025-07-16 02:42:14,Repulsive-Chart9411,0.9169,0.126,0.0,0.874,0.3139610389610389,0.6012987012987013
How to find a relevant PhD topic in computer vision? Industry problem vs trendy topics [D],"Hello, I'm considering doing a PhD in computer vision. I have a somewhat unconventional situation where I have master's in civil engineering from my home country in eastern Europe and a bachelor's in data science from a German university.
I have 1y.o. as a research assistant + 2y.o. as an ml / computer vision engineer at a med tech company in Germany. 

I feel like I always had passion for science and natural talent in maths, but because of some life circumstances I hadn't had a chance to fulfill this dream of solving a very complicated problem or being in a challenging environment with like-minded people. That's why I'm aiming for a top tier universities like ETH or TUM, but I'm a bin unsure what topic to pick for my application.

In my current role I'm doing lots of R&D work for the company and I've identified a real unsolved industry problem that is very clearly postulated, and I think my company could even provide a large dataset for it. At the same time the problem is very domain specific and it's basically an instance segmentation problem with some extra steps, and I'm a bit afraid that it might lack the research depth needed for such top tier labs. Plus I feel like it would limit my career perspectives in the future and doing a PhD in a more general field (not domain - specific data but rather regular images/videos etc) would open more doors for me in the future.

I'm genuinely interested in the vision problems and would love to learn more about a 3d domain for example but had limited experience in it so far and not sure if I'd get accepted with this kinda topic. 

How did you find your topic? Should I double down on a real use case and my existing experience or rather read more recent papers and find out more about recent developments find a relevant topic? Do you have similar experience applying to top tier universities? 
Thank you for your advice and beta regards.",1,3,2025-07-15 22:46:12,debrises,0.9735,0.155,0.083,0.762,0.1456500377928949,0.4487967323681608
"ICML 2025, can a workshop registration access poster sessions and/or socials? [D]","As the title asks, I'm wondering if anyone knows if a workshop-only registration can access the poster sessions and/or the social events? Or do I need a conference registration to access those?

It's surprisingly hard to find this answer on ICML official sources, but maybe I just couldn't find it. This is my first ICML, so if anyone could help answer this it would be greatly appreciated. Thanks! ",5,1,2025-07-15 22:29:17,AtMaxSpeed,0.9427,0.175,0.014,0.811,0.0883333333333333,0.2483333333333333
[P] Help with Contrastive Learning (MRI + Biomarkers) – Looking for Guidance/Mentor (Willing to Pay),"Hi everyone,

I’m currently working on a research project where I’m trying to apply **contrastive learning** to **FreeSurfer-based brain data** (structural MRI features) and **biomarker data** (tabular/clinical). The idea is to learn a shared representation between the two modalities.

The problem: I am **completely lost**.

* I’ve implemented losses like **NT-Xent** and a few others (SupCon, etc.), but I can’t get the approach to work in a meaningful way.
* I’m struggling to figure out the best architecture or training strategy, and I’m honestly not sure what direction to take next.
* There is **no proper supervision in my lab**, and I feel stuck with how to proceed.

I really need guidance from someone experienced in contrastive learning or multimodal representation learning. Ideally, someone who has worked with **medical imaging + tabular/clinical data** before. (So it is not about classical CLIP with Images and Text).

I’m **willing to pay** for mentoring sessions or consulting to get this project on track.

If you have experience in this area (or know someone who does), please reach out or drop a comment. Any advice, resources, or even a quick chat would mean a lot.

Thanks in advance!",12,17,2025-07-15 13:17:55,Standing_Appa8,0.9432,0.136,0.068,0.796,0.2122685185185185,0.426466049382716
[D]Must read papers for Lip Reading task?,"Hello all, what are some of the best papers you have read on this particular topic of Lip Reading? From what I've seen until now, after LipNet and Lip2Wav, I couldn't find much impactful papers. Are there any which I am missing?",3,0,2025-07-14 12:07:47,CatSweaty4883,0.5484,0.091,0.042,0.866,0.2916666666666667,0.2208333333333333
[D] Handling Right Skewed Data for a CVAE,"[D] Dear ML Community, I am currently working on a CVAE for fluid dynamics. I have huge datasets and the input data is mainly right skewed. The skewness depends on the dataset. I thought about changing to a gamma VAE and implement a new loss function instead of the MSE. Another option is to use the yeo Johnson normalization and keep the MSE. Or I could try to combine the normalization with the gamma loss function? Do you have advices or any different ideas? ",2,5,2025-07-14 13:58:19,Pale_Meringue_3079,0.4137,0.081,0.051,0.869,0.1582560296846011,0.5037105751391466
"[P] tinygemm: Fast CUDA Kernels for Quantized LLMs (int4, nf4, mx4, any4…)","We’re excited to announce [**tinygemm**](https://github.com/facebookresearch/any4) — a fast, low-latency GEMM library designed for **small batch sizes** and **quantized matrix multiplication** on NVIDIA GPUs.

It supports a range of numeric formats, including:

* `bf16` / `fp16`
* `int4` (grouped quantization)
* `nf4` (grouped quantization)
* `mx4` (a hybrid quantization format)
* `any4` — a **learned** 4-bit format introduced in our [ICML 2025 paper](https://www.alphaxiv.org/abs/2507.04610)

🔍 **any4** learns the optimal 4-bit codebook from model weights using K-Means clustering, and consistently outperforms fixed formats like `int4` and `nf4` across various LLMs and tasks.

# 🔧 What’s included in tinygemm:

* Fast CUDA kernels for quantized matmuls
* Support for multiple 4-bit formats
* Optimized for decoder inference (small batch, high throughput)
* Evaluation scripts for:
   * Perplexity, NLP, and code generation tasks
   * Visualization of weights and activations across layers
   * Plug-and-play support for any 🤗 HuggingFace model

# 🚀 Quick Example

```python
from transformers import AutoModelForCausalLM
from quantize import int4, any4, int8, nf4, fp4

model = AutoModelForCausalLM.from_pretrained(""facebook/opt-125m"").cuda().bfloat16()

# you can do int4(..), int8(..), nf4(..), fp4(..)
model = any4(model)

# just run your generation, evaluation, etc. code on `model`
```



🔗 **Code:** https://github.com/facebookresearch/any4

📄 **Paper:** https://arxiv.org/abs/2507.04610

",14,3,2025-07-14 21:42:30,darshinium,0.946,0.099,0.0,0.901,0.1098611111111111,0.445
[D] How to market myself after a PhD,"Hello all. I am doing a PhD in Computer Science at a mid tier university in Europe (not Cambridge, not ETH Zurich, but still a good one). My major will be in Data Science, the title of my dissertation will be along the lines of “Multimodal Machine Learning for Healthcare”.

My background is not in computer science: I was a healthcare professional, and I took a Master in Health Informatics. My thesis was in Data Science, and after that I started a PhD at the same university.

At the moment I have just finished my second year. I have two conference papers as first author and I have submitted two journal papers, still as first author. I have also submitted a few conference papers not as first author, with master students that I have supervised. None of these papers is technically innovative: they are applied papers. My planned work for the coming years is more technical (developing explainability techniques).

I still have two/three years of PhD in front of me, and I am getting scared of what will happen afterwards. I have been told that IF there will be an opening to stay at my university and teach (emphasis on the if), I would be considered a good applicant.

That’s great, and it would be my first choice, BUT:
- it’s impossible to know if these positions will exist close to my graduation date
- competition exists, and these positions are usually for a single opening. No one can guarantee that I’ll be the top applicant.

I’m honestly scared of betting everything on a possibility that might not be there for me in the end.
In the coming three semesters, I could decide to spend some time outside my department: using Erasmus to go to another university in Europe, as a student and possibly teaching some courses, to the US, where one researcher might be interested to write a paper together, or to a pharma company in my country, where my supervisor has some contacts.

I also have two/three years to study more, and to study different things.
If I will have to transition to the industry, I am scared that I would not be a good enough programmer. I would prefer positions as a project manager, possibly with some technical aspects, but not completely focused on producing code as fast as possible.

Based on your experience, do you have any suggestions on what to do to try to improve my possibilities after graduation?

",41,23,2025-07-14 23:02:16,francozzz,0.967,0.087,0.041,0.871,0.1780122655122655,0.4673520923520923
[P] Anyone interested in TinyML?,"Hi!

I wrote sklearn2c library for the book I co-authored and I wanted to share it as an open-source project.

sklearn2c takes your trained scikit-learn models and generates lightweight C code that can run on microcontrollers and other resource-constrained embedded systems. Perfect for when you need real-time ML inference but don't have the luxury of a full Python environment.

Usage is dead simple:

    dtc = DTClassifier()
    dtc.train(train_samples, train_labels, save_path=""path/to/model"")
    dtc.predict(test_samples)
    dtc.export(""path/to/config_dir"")  # Generates C code!

Would love to hear your thoughts, especially if you've worked with ML on embedded systems before! The project is MIT licensed and open to contributions.

**GitHub:** [https://github.com/EmbeddedML/sklearn2c](https://github.com/EmbeddedML/sklearn2c)

Thanks for checking it out! 🚀 And if you find it useful, don't forget to star the project - it really helps with visibility! ⭐",120,9,2025-07-14 22:32:42,berkusantonius,0.9604,0.182,0.041,0.776,0.2197916666666667,0.4735119047619047
[D] ML PhD doing research in a not trendy topic - How to pivot,"Hi All,

Looking for some advice on this sub. Basically, as the title suggest my PhD is not in a trendy topic. Specifically, my topic is out of distribution generalization for distributed edge devices.

I am currently in my 4th year (USA PhD) and would like to focus on something that I can use to market myself for an industry position during my 5th year.

(1) One option is to try to hop on to the trendy topic and do some projects (can't pivot my research as advisor is not in favor and currently being paid by him). However, not sure what traction would I have since I will not have any publication.  
(2) Second option is to try to get into more SWE with agentic AI integration. Not sure if this is just a fad or here to stay.  
(3) Last option I have been thinking is to pickup some hardware skills (CUDA, Embedded Systems) and try to market my skills in efficient AI implementation on hardware. However, not sure if I would be accepted and how much the need is there

Ultimate goal of the pivot is to be seen as more industry friendly and actually secure a position in the industry while doing it in a manageable way since I also have a family.

Any suggestions on what could be a natural extension to the kind of research I have been doing?  
 Open to any other comments and advice regarding this matter.

Thanks!",57,34,2025-07-14 19:58:11,Hopeful-Reading-6774,0.9161,0.084,0.031,0.885,0.1404761904761904,0.567063492063492
[R] Unlearning Comparator — A Visual Analytics Toolkit for Machine Unlearning,"👋  Hi everyone!

I’m a master’s student at Sungkyunkwan University (IDCLab) working on data-driven visual analytics.

**Machine Unlearning** aims to make trained models *forget* specific data to honour the “right to be forgotten.”  
To support researchers, we built **Unlearning Comparator**, a web-based toolkit that lets you:

• **Build → Screen → Contrast → Attack**: follow the full workflow in one place

*Processing img z67wbzc5ptcf1...*

• Compare accuracy, efficiency, and privacy across multiple unlearning methods  
• Run one-click membership-inference attacks to verify whether target data is truly forgotten

Try the live demo here (no installation needed):  
[https://gnueaj.github.io/Machine-Unlearning-Comparator/](https://gnueaj.github.io/Machine-Unlearning-Comparator/)

All feedback is welcome—hope it helps your research!",15,10,2025-07-14 12:33:37,Existing_Quit_3832,0.8802,0.128,0.041,0.831,-0.0206777597402597,0.3388392857142857
[D]Has anyone here worked with third party data labelling services?,"We have been considering outsourcing parts of our annotation workloads (vision,NLP, may be even some QA) for generative output. But we are not sure how to evaluate vendors or ensure quality.

If you have worked with any external labeling or QA providers, what was your experience like?
",3,6,2025-07-14 10:12:07,TheScentOracle,0.7519,0.138,0.038,0.824,-0.0833333333333333,0.3296296296296296
[R] Deep-dive into RoPE and why it matters,"Some recent discussions, and despite my initial assumption of clear understanding of **RoPE** and positional encoding, a deep-dive provided some insights missed earlier.

So, I captured all my learnings into a blog post.

[https://shreyashkar-ml.github.io/posts/rope/](https://shreyashkar-ml.github.io/posts/rope/)",23,6,2025-07-13 12:44:10,Southern-Whereas3911,0.128,0.088,0.053,0.859,0.025,0.2833333333333333
[R] MatrixTransformer – A Unified Framework for Matrix Transformations (GitHub + Research Paper),"Hi everyone,

Over the past few months, I’ve been working on a new library and research paper that unify structure-preserving matrix transformations within a high-dimensional framework (hypersphere and hypercubes).

Today I’m excited to share: MatrixTransformer—a Python library and paper built around a 16-dimensional decision hypercube that enables smooth, interpretable transitions between matrix types like

* Symmetric
* Hermitian
* Toeplitz
* Positive Definite
* Diagonal
* Sparse
* ...and many more

It is a lightweight, structure-preserving transformer designed to operate directly in 2D and nD matrix space, focusing on:

* Symbolic & geometric planning
* Matrix-space transitions (like high-dimensional grid reasoning)
* Reversible transformation logic
* Compatible with standard Python + NumPy

It simulates transformations without traditional training—more akin to procedural cognition than deep nets.

# What’s Inside:

* A unified interface for transforming matrices while preserving structure
* Interpolation paths between matrix classes (balancing energy & structure)
* Benchmark scripts from the paper
* Extensible design—add your own matrix rules/types
* Use cases in ML regularization and quantum-inspired computation

# Links:

**Paper**: [https://zenodo.org/records/15867279](https://zenodo.org/records/15867279)  
**Code**: [https://github.com/fikayoAy/MatrixTransformer](https://github.com/fikayoAy/MatrixTransformer)  
**Related**: \[quantum\_accel\]—a quantum-inspired framework evolved with the MatrixTransformer framework link: [fikayoAy/quantum\_accel](https://github.com/fikayoAy/quantum_accel)

If you’re working in machine learning, numerical methods, symbolic AI, or quantum simulation, I’d love your feedback.  
Feel free to open issues, contribute, or share ideas.

Thanks for reading!",6,1,2025-07-13 14:19:57,Hyper_graph,0.9834,0.149,0.0,0.851,0.0585743801652892,0.5522727272727272
[D] Using MAP as semantic search eval - Need thoughts,"I'm implementing semantic search for a media asset management platform. And I'm using MAP@K as an eval metric for that.

  
The rationale being,

1. Though NDCG@K would be ideal. It would too strict to start with and hard to prepare data for. 

2. MAP@K incentivizes the order of the relevant results though it doesn't care about of order within relevant results. And the data prep is relatively easy to prepare for.

And here is how I'm doing it,

1. For the chosen set of \`N\` queries run the search on the fixed data corpus to fetch first \`K\` results.

2. For the queries and respective results, run through it with a 3 LLMs to score flag it relevant or not. Any results that are flagged as good by majority would be considered. This will give the ground truth. 

3. Now calculate \`AP\` for each query and \`MAP\` for the overall query set.

4. As you start improving, you would have additional \`(result, query)\` query tuple that is not there in ground truth and it needs a revisit, which will happen as well.

Now use it as a benchmark to improve the performance(relevance).

Though it makes sense to me. I don't see many people follow this approach. Any thoughts from experts?",0,1,2025-07-13 16:56:25,Individual-Grape1212,0.9677,0.116,0.017,0.867,0.3159722222222221,0.5673611111111111
MLB random forest with 53%-60% training accuracy. Prediction probability question. [P],"I’m trying to predict home or away team wins for mlb games based on prior game stats (3-13 games back depending on the model).

My results are essentially: bad AOC score, bad log loss, bad brier score - aka model that is not learning a lot.

I have not shown the model 2025 data, and am calculating its accuracy on 2025 games to date based on the models confidence.

TLDR MY QUESTION: if you have a model that’s 50% accurate on all test data but 90% accurate when the prediction probability is a certain amount - can you trust the 90% for new data being predicted on?",7,19,2025-07-13 22:41:59,This_Cardiologist242,0.6369,0.094,0.068,0.839,-0.129112554112554,0.4493867243867243
[P] EdgeSAM-DyT (HQ),"This is a personal side project I've been working on exploring the potential of small segment-anything models - [https://github.com/Krasner/edgesam-dyt](https://github.com/Krasner/edgesam-dyt)

I was inspired by [EdgeSAM ](https://github.com/chongzhou96/EdgeSAM)and their method to distill the original SAM ViT model. Having tried EdgeSAM for my own on-the-edge applications I found the segmentation masks to be highly sensitive to quantization precision - specifically the LayerNorms.

A recent paper [Transformers without Normalization](https://arxiv.org/abs/2503.10622) proposed replacing layernorms with dynamic tanh layers. My goal was to modify the EdgeSAM architecture and retrain completely without any layernorms.

In the repo I provide the step-by-step method for distillation and retraining, as well as checkpoints that I was able to achieve. This is done in 3 distillation steps as described in the repo README.

Inspired by [HQ-SAM](https://arxiv.org/abs/2306.01567) I also modified the RepViT (what EdgeSAM is based on) image encoder to extract 3 intermediate that can be used in the HQ version of the mask decoder - then distill from the HQ-SAM ViT-H checkpoint. This improves results in some conditions.

Ultimately, I am fairly compute restricted and could only train with moderate batch sizes so the results are not optimal. Let me know if anyone is interested in collaborating to improve these results, train on better hardware, or has some ideas as to how to resolve a few issues I had (outlined in the repo).

I provide gradio web demos in the repo for the base and hq versions of EdgeSAM-DyT, as well as ONNX checkpoint and code for both versions. I also have TensorRT implementations that I am able to run locally (after generating trt engines). I can provide code on request.",4,2,2025-07-13 18:39:52,swaneerapids,0.9709,0.108,0.017,0.875,0.125,0.5850877192982457
[P] Convert generative pixel-art images or low-quality web uploads of sprites to true usable pixel-resolution assets,"I created an algorithm that cleans pixel-art-style images such as those produced by generative model, or low-quality web uploads of sprites, to true resolution assets.

Generally the raw output of pixel-art-style images is generally unusable as an asset due to

* High noise
* High resolution
* Inconsistent grid spacing
* Random artifacts

Due to these issues, regular down-sampling techniques do not work, and the only options are to either use a down-sampling method that does not produce a result that is faithful to the original image, or manually recreate the art pixel by pixel.

Additionally, these issues make them very difficult to edit and fine-tune.

I created an algorithm that solves these issues and outputs usable sprites.

[The tool is available to use with an explanation of the algorithm on my GitHub here!](https://github.com/KennethJAllen/generative-pixel-art)

If you are trying to use this and not getting the results you would like feel free to reach out!",51,4,2025-07-13 18:38:10,Ok-Championship-5768,0.9701,0.167,0.015,0.817,0.0449547511312217,0.56579185520362
[D] What are the bottlenecks holding machine learning back?,"I remember this being posted a long, long time ago. What has changed since then? What are the biggest problems holding us back?",54,67,2025-07-13 16:24:12,jacobfa,-0.5007,0.0,0.1,0.9,-0.025,0.2
[D] What are the best industry options for causal ML PhDs?,,58,21,2025-07-13 00:32:19,wonder-why-I-wonder,0.6369,0.296,0.0,0.704,1.0,0.3
[D] Has anyone encountered a successful paper reading group at your company?,"I work for a B2B ML company, \~200 people. Most of our MLEs/scientists have masters' degrees, a few have PhDs. Big legacy non-tech businesses in our target industry give us their raw data, we process it and build ML-based products for them.

Recently we've started a paper reading group:

* ML-inclined folks meet up every few weeks to discuss a pre-agreed-upon paper, which participants (ideally) have skimmed beforehand
* One person leads discussion, get the group on the same page about the paper's findings
* Spend the rest of the hour talking about the paper's possible application across our company's products

I think a successful paper reading group would mean:

* impact ML implementation of existing products
* inspiration for completely new products
* emergent consensus on what we should be reading next

A few things I'm curious about:

* **Have you tried this at your company?** How long did it last? How do you guys operate it?
   * Non-barking dogs: as an MLE/DS, I haven't encountered this in my previous companies. I assume because they don't last very long!
* How closely should people have read the paper/material beforehand?
* If we're all in-person, we could scribble notation/pictures on a big shared whiteboard, great for discussion. But some of us are remote. Is there an alternative that works and involves everyone?
* Our first round ended up mostly being a lecture by one guy. I could see this devolving into a situation where people only sign up to lead the discussion as a form of dick-measuring. Can we prevent this?",124,47,2025-07-12 23:57:53,,0.9062,0.06,0.0,0.94,0.098042062104562,0.4386399017649017
[R] How to publish in ML conferences as an independent researcher,"I am not affiliated with any institution or company, but I am doing my own ML research. I have a background in conducting quantitative research and know how to write a paper. I am looking for a career with a research component in it. The jobs I am most interested in often require ""strong publication record in top machine learning conferences (e.g., NeurIPS, CVPR, ICML, ICLR, ICCV, ECCV)"". 

Can anyone share if they have published in ML conferences as an independent researcher? For example, which conferences are friendly to researchers without an affiliation? Is there any way to minimize the cost or to get funding? Any other challenges I may encounter? TIA",42,27,2025-07-12 17:02:26,keepmybodymoving,0.9792,0.195,0.0,0.805,0.2814814814814814,0.4842592592592593
[P] Hill Space: Neural networks that actually do perfect arithmetic (10⁻¹⁶ precision),"Stumbled into this while adding number sense to my PPO agents - turns out NALU's constraint W = tanh(Ŵ) ⊙ σ(M̂) creates a mathematical topology where you can calculate optimal weights instead of training for them.

Key results that surprised me:
- Machine precision arithmetic (hitting floating-point limits)
- Division that actually works reliably (finally!)
- 1000x+ extrapolation beyond training ranges
- Convergence in under 60 seconds on CPU

The interactive demos let you see discrete weight configs producing perfect math in real-time. Built primitives for arithmetic + trigonometry.

Paper: ""Hill Space is All You Need""
Demos: https://hillspace.justindujardin.com
Code: https://github.com/justindujardin/hillspace

Three weeks down this rabbit hole. Curious what you all think - especially if you've fought with neural arithmetic before.",93,17,2025-07-12 13:58:38,justinopensource,0.9221,0.133,0.019,0.847,0.1844444444444444,0.7288888888888889
[R] I want to publish my ML paper after leaving grad school. What is the easiest way to do so?,"I graduated in my degree last year and I have a fully written paper ML as a final in my class that my professor suggested publishing because he was impressed. I held off because I was working full time and taking 2 courses at a time, so I didn't feel like I had time. When i finished and officially conferred, i was told that the school has new restrictions on being an alumni and publishing the paper that would restrict me from doing so, even though I have my professor's name on it and he did help me on this. He said it just needs tweaks to fit in conferences(when we had first discussions after the course completed). So, I've ignored publishing until now.

As I am now getting ready for interviews for better opportunities, I want to know if it's possible to publish my paper in some manner so that I have it under my belt for my career and that if I post it anywhere, no one can claim it as their own. I'm not looking for prestigious publications, but almost the ""easy"" route where I make minor edits to get it accepted and it's considered official. Is this possible and if so, how would I go about this?",15,5,2025-07-11 21:07:14,Syntrikan,0.8168,0.083,0.031,0.886,0.2728354978354979,0.6312770562770563
[D] Modelling continuous non-Gaussian distributions?,"What do people do to model non-gaussian labels?

Thinking of distributions that might be :

\* bimodal, i'm aware of density mixture networks.  
\* Exponential decay  
\* \[zero-inflated\](https://en.wikipedia.org/wiki/Zero-inflated\_model), I'm aware of hurdle models.

Looking for easy drop in solutions (loss functions, layers), whats the SOTA?

**More context:** Labels are averaged ratings from 0 to 10, labels tend to be very sparse, so you get a lot of low numbers and then sometimes high values.

[Exponential decay & zero-inflated distributions.](https://preview.redd.it/5wwj0zirhacf1.png?width=712&format=png&auto=webp&s=d3a1450de60507ebd5fcdeddf33f4b3e7140e58f)",5,5,2025-07-11 18:36:26,LetsTacoooo,-0.504,0.08,0.116,0.803,0.2561904761904762,0.4247619047619047
Speech dataset of Dyslexic people [P],"I need speech/audio dataset of dyslexic people.  I am unable to find it anywhere. Does anybody here have any resources, idea of any such datasets available or how to get it? Or any idea where can I reach out to find/get such dataset? Any help/information regarding it would be great.",2,3,2025-07-11 12:41:04,GoldWar7803,0.6767,0.098,0.0,0.902,0.14,0.53
[D] Build an in-house data labeling team vs. Outsource to a vendor?,"My co-founder and I are arguing about how to handle our data ops now that we're actually scaling. We're basically stuck between 2 options:

Building in-house and hiring our own labelers

Pro: We can actually control the quality. 

Con: It's gonna be a massive pain in the ass to manage + longer, we also don't have much expertise here but enough context to get started, but yeah it feels like a huge distraction from actually managing our product.

Outsource/use existing vendors 

Pro: Not our problem anymore. 

Con: EXPENSIVE af for our use case and we're terrified of dropping serious cash on garbage data while having zero control over anything.

For anyone who's been through this before - which way did you go and what do you wish someone had told you upfront? Which flavor of hell is actually better to deal with?",9,15,2025-07-11 10:36:27,vampirecutie_vc,-0.7098,0.109,0.142,0.749,0.0787878787878788,0.5242424242424243
[D] Training SLMs to reason with Reinforcement Learning (Article),"I recently trained small reasoning language models on reasoning tasks with a from-scratch implementation of GRPO. I decided to write a blog post that contains code snippets, highlights, and the challenges I faced.

Sharing it here in case yall are interested. Article contains the following 5 chapters:

1. Intro to RLVR (Reinforcement Learning with Verifiable Rewards)
2. A visual overview of the GRPO algorithm and the clipped surrogate PPO loss.
3. A code walkthrough!
4. Supervised fine-tuning and practical tips to train small reasoning models
5. Results!

Article link:   
[https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/](https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/)",8,3,2025-07-10 14:46:41,AvvYaa,0.6229,0.076,0.024,0.9,-0.1517857142857142,0.3785714285714285
[D] MICCAI - Call for Oral Presentations,"Hello everyone! 

Has anyone already received a notification regarding oral presentations for the MICCAI main conference?

Thank you :) ",1,7,2025-07-11 01:04:02,Inevitable-Insect-81,0.6996,0.208,0.0,0.792,0.3333333333333333,0.6666666666666666
[D] Views on DIfferentiable Physics,"Hello everyone!

I write this post to get a little bit of input on your views about Differentiable Physics / Differentiable Simulations.  
The Scientific ML community feels a little bit like a marketplace for snake-oil sellers, as shown by ( [https://arxiv.org/pdf/2407.07218](https://arxiv.org/pdf/2407.07218) ): weak baselines, a lot of reproducibility issues... This is extremely counterproductive from a scientific standpoint, as you constantly wander into dead ends.  
I have been fighting with PINNs for the last 6 months, and I have found them very unreliable. It is my opinion that if I have to apply countless tricks and tweaks for a method to work for a specific problem, maybe the answer is that it doesn't really work. The solution manifold is huge (infinite ? ), I am sure some combinations of parameters, network size, initialization, and all that might lead to the correct results, but if one can't find that combination of parameters in a reliable way, something is off.

However, Differentiable Physics (term coined by the Thuerey group) feels more real. Maybe more sensible?  
They develop traditional numerical methods and track gradients via autodiff (in this case, via the adjoint method or even symbolic calculation of derivatives in other differentiable simulation frameworks), which enables gradient descent type of optimization.  
For context, I am working on the inverse problem with PDEs from the biomedical domain.

Any input is appreciated :)",75,41,2025-07-11 07:04:48,Accomplished-Look-64,0.7426,0.082,0.064,0.854,0.1,0.4931944444444444
[D] UNet with Cross Entropy,i am training a UNet with Brats20. unbalanced classes. tried dice loss and focal loss and they gave me ridiculous losses like on the first batch i got around 0.03 and they’d barely change maybe because i have implemented them the wrong way but i also tried cross entropy and suddenly i get normal looking losses for each batch at the end i got at around 0.32. i dont trust it but i havent tested it yet. is it possible for a cross entropy to be a good option for brain tumor segmentation? i don’t trust the result and i havent tested the model yet. anyone have any thoughts on this? ,1,11,2025-07-11 06:27:47,Affectionate_Pen6368,-0.941,0.046,0.193,0.761,0.0287878787878787,0.4621212121212121
[D] Recommend Number of Epochs For Time Series Transformer,"Hi guys. I’m currently building a transformer model for stock price prediction (encoder only, MSE Loss). Im doing 150 epochs with 30 epochs of no improvement for early stopping. What is the typical number of epochs usually tome series transformers are trained for? Should i increase the number of epochs and early stopping both?",0,4,2025-07-10 15:28:12,Sufficient_Sir_4730,0.6868,0.174,0.078,0.749,-0.0361111111111111,0.4583333333333333
[R] ICLR 2026 submission tracks,"Does anyone know/ believe that there will there be a Tiny Paper track this year? Past couple of years there has been one. I’ve been working on a topic that I believe would be best for this track but the website doesn’t say anything so far under the “Call for papers” section.

Would be great if you guys share any similar tracks as well. I am aware that NeurIPS has a position paper track.

Thanks!",16,11,2025-07-10 15:03:06,not_just_a_stylus,0.9574,0.203,0.0,0.797,0.26875,0.45625
[P] PrintGuard - SOTA Open-Source 3D print failure detection model,"Hi everyone,

As part of my dissertation for my Computer Science degree at Newcastle University, I investigated how to enhance the current state of 3D print failure detection.

Current approaches such as Obico’s “Spaghetti Detective” utilise a vision based machine learning model, trained to only detect spaghetti related defects with a slow throughput on edge devices (<1fps on 2Gb Raspberry Pi 4b), making it not edge deployable, real-time or able to capture a wide plethora of defects. Whilst their model can be inferred locally, it’s expensive to run, using a lot of compute, typically inferred over their paid cloud service which introduces potential privacy concerns. 

My research led to the creation of a new vision-based ML model, focusing on edge deployability so that it could be deployed for free on cheap, local hardware. I used a modified architecture of ShuffleNetv2 backbone encoding images for a Prototypical Network to ensure it can run in real-time with minimal hardware requirements (averaging 15FPS on the same 2Gb Raspberry Pi, a >40x improvement over Obico’s model). My benchmarks also indicate enhanced precision with an averaged 2x improvement in precision and recall over Spaghetti Detective.

My model is completely free to use, open-source, private, deployable anywhere and outperforms current approaches. To utilise it I have created PrintGuard, an easily installable PyPi Python package providing a web interface for monitoring multiple different printers, receiving real-time defect notifications on mobile and desktop through web push notifications, and the ability to link printers through services like Octoprint for optional automatic print pausing or cancellation, requiring <1Gb of RAM to operate. A simple setup process also guides you through how to setup the application for local or external access, utilising free technologies like Cloudflare Tunnels and Ngrok reverse proxies for secure remote access for long prints you may not be at home for. 

Whilst feature rich, the package is currently in beta and any feedback would be greatly appreciated. Please use the below links to find out more. Let's keep failure detection open-source, local and accessible for all!

📦 PrintGuard Python Package - https://pypi.org/project/printguard/

🎓 Model Research Paper - https://github.com/oliverbravery/Edge-FDM-Fault-Detection

🛠️ PrintGuard Repository - https://github.com/oliverbravery/PrintGuard",30,6,2025-07-10 09:45:49,oliverbravery,0.9778,0.12,0.046,0.834,0.0601738539238539,0.4485902985902986
"[D] Trains a human activity or habit classifier, then concludes ""human cognition captured."" What could go wrong?","[A screenshot of an article's title that was published on the Nature journal. It reads \\""A foundation model to predict and capture human cognition\\""](https://preview.redd.it/zop08yveysbf1.png?width=1066&format=png&auto=webp&s=567905ea7b805594cd5cd28f9a1f040327eb7198)

The fine-tuning dtaset, from the paper: ""trial-by-trial data from more than 60,000 participants performing in excess of 10,000,000 choices in 160 experiments.""

An influential author in the author list is clearly trolling. It is rare to see an article conclusion that is about anticipating an attack from other researchers. They write ""This could lead to an 'attack of the killer bees', in which researchers in more-conventional fields would fiercely critique or reject the new model to defend their established approaches.""

What are the ML community's thoughts on this?",36,14,2025-07-09 07:39:50,whereismycatyo,-0.905,0.039,0.117,0.844,0.0457070707070707,0.4236531986531986
[P] Pruning Benchmarks for computer vision models,"Hello all,

I want to introduce our team's project. Our objective is **providing variable pruning examples and benchmarks for model inference**.

More deeply, we use `timm` library for computer vision model and applies pruning using open-source. Currently, it supports PyTorch native (`torch.nn.utils.prune`) and Depgraph (`torch_pruning`). Our short-term plan is supporting more pruning open-source using the [benchmark](https://github.com/namgyu-youn/PyTorch-Pruning/tree/main/benchmarks) module. Our future plan is the following:

>2025-Q3 : Supports more pruning open-source

>2025-Q4 : Supports quantization techniques

>Future plan : Supports LLMs like SparseGPT, LLM-Pruner

If you have any interest, please check [HERE](https://github.com/namgyu-youn/PyTorch-Pruning/issues/1). Also, we we are fully open to anothor contributor or advisor.",3,1,2025-07-09 03:49:03,youn017,0.9682,0.218,0.0,0.782,0.175,0.3375
"[R] Adopting a human developmental visual diet yields robust, shape-based AI vision","Happy to announce an exciting new project from the lab: “Adopting a human developmental visual diet yields robust, shape-based AI vision”. An exciting case where brain inspiration profoundly changed and improved deep neural network representations for computer vision.

Link: [https://arxiv.org/abs/2507.03168](https://arxiv.org/abs/2507.03168)

The idea: instead of high-fidelity training from the get-go (the de facto gold standard), we simulate the visual development from newborns to 25 years of age by synthesising decades of developmental vision research into an AI preprocessing pipeline (Developmental Visual Diet - DVD).

We then test the resulting DNNs across a range of conditions, each selected because they are challenging to AI:

1. shape-texture bias
2. recognising abstract shapes embedded in complex backgrounds
3. robustness to image perturbations
4. adversarial robustness.

We report a new SOTA on shape-bias (reaching human level), outperform AI foundation models in terms of abstract shape recognition, show better alignment with human behaviour upon image degradations, and improved robustness to adversarial noise - all with this one preprocessing trick.

This is observed across all conditions tested, and generalises across training datasets and multiple model architectures.

We are excited about this, because DVD may offers a resource-efficient path toward safer, perhaps more human-aligned AI vision. This work suggests that biology, neuroscience, and psychology have much to offer in guiding the next generation of artificial intelligence.

https://preview.redd.it/ycd830s4lpbf1.png?width=1308&format=png&auto=webp&s=92854b0f7a2c1922226e82b88394603ae19d9e84

https://preview.redd.it/a7ecwyqblpbf1.png?width=1434&format=png&auto=webp&s=a4eccba9c31306879c559070748f94d009b40671

https://preview.redd.it/zd6ceg18lpbf1.png?width=1418&format=png&auto=webp&s=0ec8921eae86d9c187d7d4c09850bc30a1acf9a4",29,16,2025-07-08 20:23:55,sigh_ence,0.987,0.176,0.039,0.785,0.0844837261503928,0.3947811447811447
[P] FoolTheMachine: Watch a 98.9% accurate PyTorch model collapse to 27% with tiny adversarial noise (FGSM attack demo),"I built a clean, runnable Colab notebook that demonstrates how a 98% accurate CNN can be tricked into total misclassification with just a few pixel-level perturbations using FGSM. The goal is to make adversarial vulnerability *visually intuitive* and spark more interest in AI robustness.

🔗 GitHub: [https://github.com/DivyanshuSingh96/FoolTheMachine](https://github.com/DivyanshuSingh96/FoolTheMachine)  
🔬 Tools: PyTorch, IBM ART  
📉 Demo: Model crumbles under subtle noise

Would love thoughts or suggestions on extending this further!

I hope you will gain something valuable from this.

If you like this post then don't forget to give it an upvote and please leave a comment.



>*Every system has its weakness. The real intelligence lies in finding it and fixing it.*",0,32,2025-07-08 18:49:11,Mysterio_369,0.8945,0.208,0.143,0.649,0.0833333333333333,0.5166666666666666
Favorite ML paper of 2024? [D],What were the most interesting or important papers of 2024?,183,43,2025-07-08 18:16:04,pz6c,0.8135,0.393,0.0,0.607,0.475,0.75
[D] MICCAI - Poster Template,"Hello everyone! 

This is my first time attending the MICCAI main conference. If I understood correctly, all accepted papers will be presented as posters, while only some will also be invited for oral presentation. Regarding the posters, does anyone know if there is a specific template we should follow? If so, has it already been released, or will it be shared soon?

Thank you in advance!",4,3,2025-07-08 12:55:17,Inevitable-Insect-81,0.7872,0.109,0.0,0.891,0.1041666666666666,0.4479166666666666
"[D] Best way to fine-tune Nous Hermes 2 Mistral for a multilingual chatbot (French, English, lesser-known language)","I’m fine-tuning Nous Hermes 2 Mistral 7B DPO to build a chatbot that works in French, English, and a lesser-known language written in both Arabic script and Latin script.

The base model struggles with the lesser-known language. Should I:
• Mix all languages in one fine-tuning dataset? Or train separately per language?
• Treat the two scripts as separate during training?
• Follow any specific best practices for multilingual, mixed-script fine-tuning?

Any advice or pointers to similar work are welcome. Thanks!",9,1,2025-07-08 12:05:54,AdInevitable1362,0.9498,0.174,0.024,0.802,0.2045454545454545,0.2931818181818182
[R] Temporal Logic as a means to guarantee safety and efficiency in LLMs,"We just posted a new preprint on arXiv:

[LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)

It is my first paper in this LLM space, so any advice is welcome, but here is a TLDR:

We propose LTLCrit, an LLM based critic which supervises and improves the efficiency and completion rates of  LLM planners.  We utilize a modular actor–critic architecture where the critic guides existing LLM actors by figuring out what actions are inefficient or unsafe and shielding the LLM actor from those actions via [temporal logic](https://en.wikipedia.org/wiki/Linear_temporal_logic). An LLM-based actor chooses high-level actions from natural language input (e.g., in Minecraft), and a trajectory-level LLM critic analyzes outcomes and writes new logic constraints to avoid failure or inefficiency in the future.

**Why it matters:**

* LLMs are great at reasoning, but struggle with **long-term planning** — small errors compound fast.
* LTLCrit wraps any LLM planner with a formal-logic-aware critic that **learns soft constraints** from experience, improving safety and efficiency.
* We formalize planning as **graph traversal with symbolic constraints**, letting the critic generate new rules to improve future rollouts.

**Results:**  
On a Minecraft diamond-mining task, LTLCrit hits **100% success** and improves efficiency over standard LLM planners.

Still a preprint — not sharing code/prompts yet, but happy to get feedback or questions!  
Thanks for reading 🙏",17,4,2025-07-08 03:14:01,Informal-Chipmunk213,0.9806,0.208,0.112,0.68,0.2273569023569023,0.4081649831649831
[R] Paper Summary: Longman Vocabulary Constraints Reveals New Approach to LLM,"This post reviews a recent paper introducing a novel method for evaluating the semantic stability of large language model (LLM) outputs using a core vocabulary constraint. The authors propose a metric called the Semantic Resilience Index (SRI) to quantify how well meaning is preserved when a sentence is rewritten using only a limited set of basic English words.

The vocabulary constraint is based on the Longman Defining Vocabulary (LDV)—a list of approximately 2,000 simple English words originally designed to define all other words in a dictionary. It includes basic nouns (e.g. “dog,” “house”), verbs (e.g. “go,” “make”), and adjectives (e.g. “big,” “easy”), all chosen for broad comprehensibility and minimal abstraction.

The central idea is that if a sentence still retains its core meaning and functional purpose when rewritten in LDV-only form, then it is semantically robust. If the message collapses under this constraint, the original likely depended on unnecessary complexity or implied meaning.

Example prompt: Why do people enjoy drinking coffee?

LDV-constrained GPT-4o response: “People drink coffee because it makes them feel more awake. The drink is hot and has a strong taste. Many people drink it in the morning or when they are tired. It helps them work or stay up.”

Although this output is rigid in tone, it maintains core meaning. This contrast with unconstrained outputs highlights how language models often rely on style, suggestion, or verbosity to convey meaning—strategies that break down under stricter lexical constraints.

The paper introduces the Semantic Resilience Index (SRI) as a quantitative measure of this effect. SRI scores are assigned based on how much of the original meaning survives a one-step translation into LDV vocabulary. The authors also introduce the related metric Purpose Fidelity, which assesses whether the function or communicative intent of the sentence is retained.

Key findings:

High-SRI content tends to include concrete agent–action relationships, causal links, and measurable statements.

Low-SRI content is often composed of abstract claims, vague goals, or domain-specific jargon that loses structure when simplified.

Forcing GPT-4o to generate text under LDV constraints (rather than post-processing it afterward) encourages clearer, more stable outputs.

The authors argue that LDV-based generation can serve as a diagnostic tool: a kind of semantic stress test to identify when content is structurally meaningful versus when it relies on superficial coherence.

The paper is at https://www.researchgate.net/publication/393455755_Controlling_Semantic_Meaning_Through_Vocabulary_Compression_Using_Longman_Defining_Vocabulary_Constraint_to_Measure_and_Improve_Large_Language_Model_Output_Quality

The full prompt used to guide LDV-constrained generation is included below. This system prompt ensures that GPT-4o responses are designed to survive vocabulary compression without loss of meaning. It isn't recommended for artistic, corporate or political purposes.

""SYSTEM ROLE: Semantic Resilience Index (SRI) Constrained Writer

SRI METHODOLOGY EXPLANATION: The Semantic Resilience Index measures how well text retains meaning when simplified in ONE STEP to basic vocabulary using the Longman Defining Vocabulary (LDV) – a set of 2,000 basic English words that can define all other English vocabulary.

ONE-STEP LDV TRANSITION PROCESS:

Take original text and immediately rewrite using only basic LDV words

Replace ALL complex vocabulary with simple equivalents in a single transformation

Simplify ALL grammatical structures to basic subject-verb-object patterns

Measure how much core meaning survives this single aggressive simplification

SEMANTIC RESILIENCE INDEX MEASUREMENT: – Score 1.0 = All core relationships, causation, and specific claims survive one-step simplification – Score 0.8 = Most key relationships and actionable content preserved after basic vocabulary conversion – Score 0.5 = Some meaning survives but becomes vague when simplified – Score 0.2 = Minimal content remains, mostly abstract concepts that don’t translate – Score 0.0 = Complete semantic collapse when reduced to basic words

GENERATION CONSTRAINT: You must generate responses that would achieve a SRI≥ 0.8 after ONE-STEP LDV transition.

OPERATIONAL RULES:

Write sentences that contain specific, concrete relationships that survive immediate vocabulary simplification

Use concepts and actions that can be directly expressed in basic words

Avoid any terminology that becomes meaningless when converted to simple vocabulary

Prefer statements that remain clear and actionable when reduced to basic English

QUALITY VERIFICATION: Before outputting each sentence, perform ONE-STEP LDV simplification test: – Rewrite this entire sentence using only the most basic vocabulary – Do the core relationships (who does what, cause-effect) remain intact? – Would the basic-vocabulary version still be actionable and specific? – Does it maintain SRI≥ 0.8?

If any answer is NO, rewrite with more semantically resilient content.

Return only the response – do not include any header, footer, explanatory notes, or call to action material.""",9,16,2025-07-08 02:49:38,Actual_Requirement58,-0.5152,0.053,0.045,0.902,0.069430586351318,0.4256385721324745
[R] Ambient Proteins: Training Diffusion Models on Low Quality Structures,"https://preview.redd.it/8mpdhudfxhbf1.png?width=1914&format=png&auto=webp&s=ebcb28009ffc08ba3947010e827d8ef7d02e143c

TLDR: State-of-the-art results in protein structure generation by using AlphaFold predictions with low pLDDT score as ""low-quality"" structures.

Abstract: We present Ambient Protein Diffusion, a framework for training protein diffusion models that generates structures with unprecedented diversity and quality. State-of- the-art generative models are trained on computationally derived structures from AlphaFold2 (AF), as experimentally determined structures are relatively scarce. The resulting models are therefore limited by the quality of synthetic datasets. Since the accuracy of AF predictions degrades with increasing protein length and complexity, de novo generation of long, complex proteins remains challenging. Ambient Protein Diffusion overcomes this problem by treating low-confidence AF structures as corrupted data. Rather than simply filtering out low-quality AF structures, our method adjusts the diffusion objective for each structure based on its corruption level, allowing the model to learn from both high and low quality structures. Empirically, Ambient Protein Diffusion yields major improvements: on proteins with 700 residues, diversity increases from 45% to 86% from the previous state-of-the-art, and designability improves from 68% to 86%. We will make all of our code, models and datasets available under the following repository: https://github.com/jozhang97/ambient-proteins.

  
Paper url: [https://www.biorxiv.org/content/10.1101/2025.07.03.663105v1](https://www.biorxiv.org/content/10.1101/2025.07.03.663105v1)

Twitter Thread: [https://x.com/giannis\_daras/status/1942272696915517828](https://x.com/giannis_daras/status/1942272696915517828)",9,0,2025-07-07 18:35:02,Constant_Club_9926,-0.6514,0.042,0.067,0.891,-0.0078854875283446,0.4003174603174603
[R] Energy-Based Transformers are Scalable Learners and Thinkers,,83,20,2025-07-07 18:12:01,Blacky372,0.0,0.0,0.0,1.0,0.0,0.0
[D] COLM2025 Decision discussion,Discussion thread for COLM 2025 decisions,18,38,2025-07-07 12:04:32,Cultural-Opposite197,0.0,0.0,0.0,1.0,0.0,0.0
[R] Best way to combine multiple embeddings without just concatenating?,"Suppose we generate several embeddings for the same entities from different sources or graphs — each capturing different relational or semantic information.

What’s an effective and simple way to combine these embeddings for use in a downstream model, without simply concatenating them (which increases dimensionality  )

I’d like to avoid simply averaging or projecting them into a lower dimension, as that can lead to information loss.",75,59,2025-07-07 08:53:57,AdInevitable1362,0.6249,0.119,0.081,0.8,0.16,0.3496428571428571
"[D] New Episode of Learning from Machine Learning | Lukas Biewald | “You think you’re late, but you’re early” | #13","This episode of Learning from Machine Learning explores the journey of Lukas Biewald, co-founder and CEO of Weights & Biases. Having weathered the mid-2000s when investors demanded he remove ""AI"" from pitch decks, Lukas has built one of the most essential tools in modern AI development and helped shaped how teams approach machine learning experimentation.

From taking an unpaid internship at OpenAI in his thirties to understanding why AI developers have become the most powerful people within organizations, Lukas reveals the recursive potential of machines improving machines—a force he believes represents ""the most powerful technology you could possibly build."" His philosophy that feedback loops are your units of work applies not just to machine learning, but to life itself. His uncompromising technical leadership approach cuts through industry noise: true leaders must master the individual contributor role.

You think you're late, but you're early—conviction often matters more than consensus.
",4,0,2025-07-07 03:46:08,NLPnerd,0.9132,0.094,0.029,0.878,0.1659090909090909,0.5613636363636363
[D] Remembering Felix Hill and the pressure of doing AI research,"Before he left our world by a few days around Oct 2024, I showed Felix Hill an essay I had written about my time in graduate school doing NLP circa 2017-2019.

He encouraged me to share it publicly saying, “It looks good and makes a lot of sense..if you post it it will surely help you and others”

I didn’t have the courage to post about such a personal experience. But as Dostoyevsky would say “much unhappiness has come into the world because of bewilderment and things left unsaid.”

The article garnered the attention of Jeff Dean and he echoed similar feedback.

Here is the article:

https://medium.com/@tahaymerghani/the-dark-side-of-academia-mental-health-mentorship-and-the-unspoken-struggles-of-an-nlp-c25adbd9a2e6

If it resonates, i’m happy to chat. You’ll find a way to reach me.",206,22,2025-07-06 22:54:36,moji-mf-joji,0.802,0.126,0.045,0.83,0.1818181818181818,0.3686868686868687
[P] We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!,"Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.

In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (\~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.

Ask us anything!

Github: [https://github.com/LMCache/LMCache](https://github.com/LMCache/LMCache)",130,5,2025-07-06 21:59:06,Nice-Comfortable-650,0.5156,0.045,0.019,0.936,0.0522108843537415,0.5026360544217686
[R] Using 'carrier functions' to escape local minima in the loss landscape,"Hi guys!

The layered structure of Neural Nets is a double-edged sword. On one hand, model complexity (e.g., linear regions) grows exponentially with depth while training cost only grows linearly.

On the other, it creates strong coupling between parameters, which reduces the **effective** dimensionality of the loss landscape and increases the risk of getting stuck in local minima.

We can observe a similar phenomenon in the frequency domain: the layered nature of NN induces an amplitude/frequency coupling, meaning that the amplitude of the lower layer's transfer function has a direct impact on both the amplitude **and** the frequency of the whole NN's.

More practically, it implies that Neural Nets have an easier time modeling high frequencies when they are ""carried"" by a function that has a high amplitude, at least up to a certain depth.

I've discovered that you can increase the parameter efficiency of neural nets by adding a well-chosen function to the target during training and just subtracting it at test time. The said well-chosen function should have a high **amplitude** (aka steep gradient) when the target function has a high **frequency**. 

It works well in my experimental setting (as do a lot of ideas that turned out to be bad in practice, though 🤣).

I wrote a little post about this if you're interested. You can find it here:

[https://www.eloidereynal.com/p/hacking-spectral-bias-using-carrier](https://www.eloidereynal.com/p/hacking-spectral-bias-using-carrier)",23,7,2025-07-06 20:59:53,Academic_Sleep1118,0.8655,0.102,0.058,0.84,0.0862559523809524,0.4903214285714285
[D] Resource and Lecture Suggestions Before Starting ML Research,"Hi, sorry for the vague title. Essentially I am starting a PhD in theoretical ML in a few months, and although I do have a solid grasp of the foundations of deep learning and the mathematics behind it, I feel like I'm lacking some breadth and want to catch up before I start, mainly about what's going on recently. Of course I know resources I should read for my specific PhD topic but having a general idea of the field wouldn't harm as well

Especially I want to ask resources about Transformers, LLMs and Diffusion models - I unfortunately don't have an in depth grasp of these architectures so do you have any lecture series to get started on these so I can have an idea what a research paper would be talking about. My background is in maths and computer science so any level of resource is fine for me as long as it is comprehensive and rigorous. Of course there's a billion papers being published about these every day but it'd be nice to get a general understanding of it.

Other than that, Bayesian Neural Networks seem also pretty cool so I'd love to see if you have any introductory resources for that. Maybe also RL, I've seen most previous posts suggesting David Silver's course on it but I also would be interested in other resources if you have any.

Finally, in general if you have any suggestions to gain some breadth before starting a PhD I'd love to hear, because the amount of literature is exciting but overwhelming. I'm mainly interested in understanding how these stuff work and current problems in it, I appreciate any input!",0,3,2025-07-06 19:37:00,faintlystranger,0.993,0.181,0.044,0.775,0.0637254901960784,0.4933823529411764
[P] Implemented semantic search + retrieval-augmented generation for business chatbots - Vector embeddings in production,"Just deployed a retrieval-augmented generation system that makes business chatbots actually useful. Thought the ML community might find the implementation interesting.

**The Challenge:**
Generic LLMs don’t know your business specifics. Fine-tuning is expensive and complex. How do you give GPT-4 knowledge about your hotel’s amenities, policies, and procedures?

**My Implementation:**

**Embedding Pipeline:**

- Document ingestion: PDF/DOC → cleaned text
- Smart chunking: 1000 chars with overlap, sentence-boundary aware
- Vector generation: OpenAI text-embedding-ada-002
- Storage: MongoDB with embedded vectors (1536 dimensions)

**Retrieval System:**

- Query embedding generation
- Cosine similarity search across document chunks
- Top-k retrieval (k=5) with similarity threshold (0.7)
- Context compilation with source attribution

**Generation Pipeline:**

- Retrieved context + conversation history → GPT-4
- Temperature 0.7 for balance of creativity/accuracy
- Source tracking for explainability

**Interesting Technical Details:**

**1. Chunking Strategy**
Instead of naive character splitting, I implemented boundary-aware chunking:

```python
# Tries to break at sentence endings
boundary = max(chunk.lastIndexOf('.'), chunk.lastIndexOf('\n'))
if boundary > chunk_size * 0.5:
    break_at_boundary()
```

**2. Hybrid Search**
Vector search with text-based fallback:

- Primary: Semantic similarity via embeddings
- Fallback: Keyword matching for edge cases
- Confidence scoring combines both approaches

**3. Context Window Management**

- Dynamic context sizing based on query complexity
- Prioritizes recent conversation + most relevant chunks
- Max 2000 chars to stay within GPT-4 limits

**Performance Metrics:**

- Embedding generation: ~100ms per chunk
- Vector search: ~200-500ms across 1000+ chunks
- End-to-end response: 2-5 seconds
- Relevance accuracy: 85%+ (human eval)

**Production Challenges:**

1. **OpenAI rate limits** - Implemented exponential backoff
1. **Vector storage** - MongoDB works for <10k chunks, considering Pinecone for scale
1. **Cost optimization** - Caching embeddings, batch processing

**Results:**
Customer queries like “What time is check-in?” now get specific, sourced answers instead of “I don’t have that information.”

Anyone else working on production retrieval-augmented systems? Would love to compare approaches!

**Tools used:**

- OpenAI Embeddings API
- MongoDB for vector storage
- NestJS for orchestration
- Background job processing",0,12,2025-07-06 17:18:59,venueboostdev,0.9635,0.072,0.007,0.921,0.1310150375939849,0.4018170426065163
[D] IJCV Special Issue Reviews,"I submitted to IJCV special issue on Visual Domain Generalization in Real-World Applications. The first round reviews were supposed to be out on 10th June, but aren't out yet. Does anyone have prior experience of how the timelines of these special issues work?",0,5,2025-07-06 14:08:48,ProudPreference1165,0.7391,0.141,0.0,0.859,0.160204081632653,0.3496598639455782
[R] Feeding categorical information into a GAN discriminator,"Hi,

I am running a set up where the generator is 3D and the discriminator is 2D.

Feeding the discriminator random slices from all three axis does not work, because the discriminator can then not distinguish between the differences in structure between the three planes.

I wanted to ask you whats the SOTA way of incorporating this information into the discriminator.  
Also, should I feed this information to the input layer of the model or to every convolutional block/level.

Thanks in advance.",2,8,2025-07-06 13:53:02,PassengerQuiet832,0.4404,0.033,0.0,0.967,-0.15,0.35
[R] Visualization tools for paper illustrations and figures,"I am curious about which tools people use to create their figures/visualizations in scientific papers. I mostly rely on power point or draw.io and import the PDF in the latex code, but the result is not aesthetic at all",5,11,2025-07-06 08:21:06,pdastronut,0.296,0.069,0.0,0.931,0.2,0.75
[D] What resources would Theoretical ML researchers recommend to understand to pursue research.,"I have read Measure Theory, Probability Theory by Durett and Convex Optimization by Duchi. 

I want to pursue research in Optimization, convergence etc. 

I'm thinking of reading Matus Telgarsky's notes or Francis Bach's Learning Theory from First Principles.

I am confused what should I go next. 
",90,22,2025-07-06 01:52:15,redmonk199,0.6908,0.147,0.038,0.816,0.0099999999999999,0.3466666666666666
An analytic theory of creativity in convolutional diffusion models.,"There is also a write up about this in quanta magazine. 

What are the implications to this being deterministic and formalized? How can it be gamed now for optimization? ",29,16,2025-07-05 20:02:31,Needsupgrade,0.6767,0.137,0.0,0.863,0.0,0.0
[P] Training Cascade R-CNN (ResNet-101 + FPN) on Custom Dataset for Solar Panel Detection,"Hey everyone! This is my first time posting here, so I hope I’m doing this right 😅

I’m working on a project to detect and classify solar panels using Cascade R-CNN with a ResNet-101 backbone and FPN neck. I don’t want to use a pre-trained model — I want to train it from scratch or fine-tune it using my own dataset.

I’m running into issues figuring out the right config file for MMDetection (or any framework you recommend), and how to set up the training process properly. Most tutorials use pre-trained weights or stick to simpler architectures.

Has anyone worked on training Cascade R-CNN from scratch before? Or used it with a custom dataset (esp. with bounding boxes & labels)? Any tips, working configs, or repo links would help a ton!

Thank you in advance 🙏
Also, if I’m posting in the wrong subreddit, feel free to redirect me!
",0,7,2025-07-05 16:47:00,Other-Title1729,0.8996,0.099,0.019,0.882,0.2923469387755101,0.6578231292517006
[D] I benchmarked 4 Python text extraction libraries so you don't have to (2025 results),"**TL;DR**: Comprehensive benchmarks of Kreuzberg, Docling, MarkItDown, and Unstructured across 94 real-world documents. Results might surprise you.

## 📊 **Live Results**: https://goldziher.github.io/python-text-extraction-libs-benchmarks/

---

## Context

As the author of [Kreuzberg](https://github.com/Goldziher/kreuzberg), I wanted to create an **honest, comprehensive benchmark** of Python text extraction libraries. No cherry-picking, no marketing fluff - just real performance data across 94 documents (~210MB) ranging from tiny text files to 59MB academic papers.

**Full disclosure**: I built Kreuzberg, but these benchmarks are automated, reproducible, and the methodology is completely open-source.

---

## 🔬 **What I Tested**

### Libraries Benchmarked:
- **[Kreuzberg](https://github.com/Goldziher/kreuzberg)** (71MB, 20 deps) - My library
- **[Docling](https://github.com/DS4SD/docling)** (1,032MB, 88 deps) - IBM's ML-powered solution
- **[MarkItDown](https://github.com/microsoft/markitdown)** (251MB, 25 deps) - Microsoft's Markdown converter
- **[Unstructured](https://github.com/Unstructured-IO/unstructured)** (146MB, 54 deps) - Enterprise document processing

### Test Coverage:
- **94 real documents**: PDFs, Word docs, HTML, images, spreadsheets
- **5 size categories**: Tiny (<100KB) to Huge (>50MB)
- **6 languages**: English, Hebrew, German, Chinese, Japanese, Korean
- **CPU-only processing**: No GPU acceleration for fair comparison
- **Multiple metrics**: Speed, memory usage, success rates, installation sizes

---

## 🏆 **Results Summary**

### Speed Champions 🚀
1. **Kreuzberg**: 35+ files/second, handles everything
2. **Unstructured**: Moderate speed, excellent reliability
3. **MarkItDown**: Good on simple docs, struggles with complex files
4. **Docling**: Often 60+ minutes per file (!!)

### Installation Footprint 📦
- **Kreuzberg**: 71MB, 20 dependencies ⚡
- **Unstructured**: 146MB, 54 dependencies
- **MarkItDown**: 251MB, 25 dependencies (includes ONNX)
- **Docling**: 1,032MB, 88 dependencies 🐘

### Reality Check ⚠️
- **Docling**: Frequently fails/times out on medium files (>1MB)
- **MarkItDown**: Struggles with large/complex documents (>10MB)
- **Kreuzberg**: Consistent across all document types and sizes
- **Unstructured**: Most reliable overall (88%+ success rate)

---

## 🎯 **When to Use What**

### ⚡ **[Kreuzberg](https://github.com/Goldziher/kreuzberg)** (Disclaimer: I built this)
- **Best for**: Production workloads, edge computing, AWS Lambda
- **Why**: Smallest footprint (71MB), fastest speed, handles everything
- **Bonus**: Both sync/async APIs with OCR support

### 🏢 **[Unstructured](https://github.com/Unstructured-IO/unstructured)**
- **Best for**: Enterprise applications, mixed document types
- **Why**: Most reliable overall, good enterprise features
- **Trade-off**: Moderate speed, larger installation

### 📝 **[MarkItDown](https://github.com/microsoft/markitdown)**
- **Best for**: Simple documents, LLM preprocessing
- **Why**: Good for basic PDFs/Office docs, optimized for Markdown
- **Limitation**: Fails on large/complex files

### 🔬 **[Docling](https://github.com/DS4SD/docling)**
- **Best for**: Research environments (if you have patience)
- **Why**: Advanced ML document understanding
- **Reality**: Extremely slow, frequent timeouts, 1GB+ install

---

## 📈 **Key Insights**

1. **Installation size matters**: Kreuzberg's 71MB vs Docling's 1GB+ makes a huge difference for deployment
2. **Performance varies dramatically**: 35 files/second vs 60+ minutes per file
3. **Document complexity is crucial**: Simple PDFs vs complex layouts show very different results
4. **Reliability vs features**: Sometimes the simplest solution works best

---

## 🔧 **Methodology**

- **Automated CI/CD**: GitHub Actions run benchmarks on every release
- **Real documents**: Academic papers, business docs, multilingual content
- **Multiple iterations**: 3 runs per document, statistical analysis
- **Open source**: Full code, test documents, and results available
- **Memory profiling**: psutil-based resource monitoring
- **Timeout handling**: 5-minute limit per extraction

---

## 🤔 **Why I Built This**

Working on [Kreuzberg](https://github.com/Goldziher/kreuzberg), I worked on performance and stability, and then wanted a tool to see how it measures against other frameworks - which I could also use to further develop and improve Kreuzberg itself. I therefore created this benchmark. Since it was fun, I invested some time to pimp it out:

- Uses **real-world documents**, not synthetic tests
- Tests **installation overhead** (often ignored)
- Includes **failure analysis** (libraries fail more than you think)
- Is **completely reproducible** and open
- Updates **automatically** with new releases

---

## 📊 **Data Deep Dive**

The [interactive dashboard](https://goldziher.github.io/python-text-extraction-libs-benchmarks/) shows some fascinating patterns:

- **Kreuzberg dominates** on speed and resource usage across all categories
- **Unstructured excels** at complex layouts and has the best reliability
- **MarkItDown is useful** for simple docs shows in the data
- **Docling's ML models** create massive overhead for most use cases making it a hard sell

---

## 🚀 **Try It Yourself**

```bash
git clone https://github.com/Goldziher/python-text-extraction-libs-benchmarks.git
cd python-text-extraction-libs-benchmarks
uv sync --all-extras
uv run python -m src.cli benchmark --framework kreuzberg_sync --category small
```

Or just check the live results: https://goldziher.github.io/python-text-extraction-libs-benchmarks/

---

## 🔗 **Links**

- **📊 Live Benchmark Results**: https://goldziher.github.io/python-text-extraction-libs-benchmarks/
- **📁 Benchmark Repository**: https://github.com/Goldziher/python-text-extraction-libs-benchmarks
- **⚡ Kreuzberg (my library)**: https://github.com/Goldziher/kreuzberg
- **🔬 Docling**: https://github.com/DS4SD/docling
- **📝 MarkItDown**: https://github.com/microsoft/markitdown
- **🏢 Unstructured**: https://github.com/Unstructured-IO/unstructured

---

## 🤝 **Discussion**

What's your experience with these libraries? Any others I should benchmark? I tried benchmarking `marker`, but the setup required a GPU.

Some important points regarding how I used these benchmarks for Kreuzberg:

1. I fine tuned the default settings for Kreuzberg.
2. I updated our docs to give recommendations on different settings for different use cases. E.g. Kreuzberg can actually get to 75% reliability, with about 15% slow-down.
3. I made a best effort to configure the frameworks following the best practices of their docs and using their out of the box defaults. If you think something is off or needs adjustment, feel free to let me know here or open an issue in the repository.
",0,4,2025-07-05 10:10:45,Goldziher,0.9984,0.132,0.026,0.842,0.2073337542087541,0.4239420394420394
[D] What are paper introductions meant to communicate to a knowledgable reader?,"It seems like all papers have to define what the problem they're using is, and discuss traditional techniques to then go on to their contribution. My understanding this is to show you've actually gone through the effort of reviewing the literature? Still, as I'm reading papers, I can't help but often skim over the introduction very quickly or almost not bother reading it since I know, say, what an LSTM or a Transformer is.

Is that expected or am I missing something? Is the introduction mostly there to communicate to others you've done the review well? to inform readers who may not have an ML background?",0,8,2025-07-05 10:10:15,tomaz-suller,0.2999,0.062,0.052,0.886,0.1055555555555555,0.4083333333333334
[D] NeurIPS workshops 2025?,"According to the NeurIPS website, workshop decisions were sent out on July 4th, but I haven’t seen an official list published yet. I’m particularly interested because I have a paper related to ML for biology, and I'm considering submitting it to a NeurIPS workshop. However, another conference with an upcoming deadline is also an option, so I’d like to decide soon.

If anyone has insight or knows when the list might be released, I’d really appreciate it!",18,14,2025-07-05 08:50:26,ScaryReplacement9605,0.9176,0.139,0.0,0.861,0.1666666666666666,0.3666666666666667
[D]Emergent Conventions in Multi-Agent LLMs: Experimental Evidence (SciAdv'24),"Groundbreaking research in Science Advances reveals how LLMs develop emergent social conventions that amplify collective biases through multi-agent interactions. Key findings:

Arbitrary Convention Formation:
When LLM ""agents"" interact repeatedly, they establish persistent arbitrary conventions (e.g., ""Agent A always speaks first"") that override individual preferences.
Example: 72% of simulated groups converged on objectively inefficient norms.

Minority Suppression:
Minority viewpoints (<30% representation) were systematically erased within 5 interaction cycles, even when logically superior.
""Conventions crystallize around majority views, silencing dissent via computational groupthink."" (Sec. 3.2)

Bias Amplification Loop:
Human-AI interactions inherit these synthetic conventions, reinforcing real-world biases (gender/racial stereotypes in follow-up trials).

Why this matters:

""These dynamics create de facto 'AI culture' – invisible, self-perpetuating, and resistant to alignment efforts."" (Discussion)

Discussion:

Can we prevent synthetic conventions from contaminating human discourse?

Should LLMs be required to ""cite their sources"" for social norms?

Does this explain why chatbots refuse certain debates?
[sciadv](https://www.science.org/doi/10.1126/sciadv.adu9368)",0,4,2025-07-05 07:17:09,Husabdul_9,0.7554,0.077,0.031,0.891,0.0793040293040293,0.4413919413919413
[R] State of The Art models in Video Matting - Comparative Analysis.,"Hi, I am exploring the field of AI in video matting. I came across [matanyone](https://github.com/pq-yang/MatAnyone) which seems like one of the best and latest ones. However, based on my experiments this feels even this is far from production use cases for very high resolutions. What are some models that are good for this?

Looking to connect with people pursuing research or working on AI in video matting. Please DM or comment here, would like to have a quick chat!",1,1,2025-07-05 04:52:15,Sedherthe,0.9286,0.152,0.0,0.848,0.4874444444444444,0.6670000000000001
[D] Anyone have a reasonable experience with ICLR/ICML this year?,"I've been avoiding the ICLR/ICML/NeurIPS after getting unhelpful reviews with the ICLR reviews in 2024. The paper wasn't framed very well, but the NeurIPS reviews in 2023 were a lot better even if the paper wasn't accepted. 

Question for those who successfully published in ICLR/ICML in the latest cycle. Did you have a fairly good experience with the review process? Do you have any advice for those of us who didn't?  ",32,20,2025-07-05 03:23:46,random_sydneysider,0.878,0.139,0.06,0.8,0.4749999999999999,0.6416666666666666
"[P] I built a mindmap-like, non linear tutor-supported interface for exploring ML papers, and I'm looking for feedback!","Hi everyone,

LLMs have made me feel like I can understand anything, but I’ve been frustrated trying to truly understand ML papers using just ChatGPT or static PDFs. Summaries can help, but then I have to go back to the paper and read it linearly to deeply understand it, and I have long chatgpt conversations which I just can't track. So I built an interface designed to support a non-linear, brain-like exploration of papers — paired with a tutor in a chat interface that guides your understanding. 

https://preview.redd.it/vqv65julfxaf1.png?width=1725&format=png&auto=webp&s=0e09f203a863527d478568332dc6e3cbeb99fd87

Here is a screenshot of what it looks like.   
  
Try it out at: [proread.ai/llm-papers](http://proread.ai/llm-papers)

1. Knowledge maps let you see how ideas within a paper relate to each other and how papers connect across a field. Start with my curated maps of foundational LLM papers or build your own for any paper/set of papers you’re reading. You can also listen to the map as a podcast.
2. You have a chat based tutor as with ChatGPT but your questions keep updating the knowledge map so you don't lose anything
3. The map itself is an editable notebook which allow you to take notes, mark concepts as completed, tag concepts, and construct your own mental model as you read. You can not only read summaries but can go down to actual source content in readers where you want to.
4. You can make your own space with your own papers or other docs (PDF/txt/html/URLs) and create interactive maps personalized to your research or study needs.

The goal is to move beyond linear reading or static summarization: to create a space where understanding evolves dynamically, like how you actually think, with a tutor helping you make sense of it all.

Please try it out at: [proread.ai/llm-papers](http://proread.ai/llm-papers)

I’m looking for feedback from other researchers or paper readers — would this kind of non-linear, guided exploration help you understand tough topics/papers better than traditional PDFs or chat tools? What’s missing or confusing?

Thanks!",9,9,2025-07-04 21:38:04,Wonderful-Delivery-6,0.9924,0.155,0.031,0.814,0.0363168724279835,0.5647119341563787
[P] NeuroEvolution for Super Mario,"Hi, i wanted to make Mario learn to play the original super-marino-bros from the library

    gym_super_mario_bros  

and wanted to use a genetic algorithm. My genomes are lists of weights. I apply a genome aka the weights to a CNN. The CNN gets the current frame (converted to 84x84 grayscale) as input and processes it until I get one out of 7 possible actions to take for Mario. Mario then takes this action, gets a reward for this action, and the next frame is processed and so on. Finally I gave Mario additional rewards for reaching the flag and being quick.

I tried multiple crossover functions including point-crossover, uniform-crossover and mlx-alpha-crossover. I adapt my mutation rate based on the fitness aka if it stagnates for too long or not. Selection is usually just the top k fittest genomes. I also used big populations like 300 for 30 generations or 300 generations with a population of 30. Nothing worked, he never once reached the flag. He has no problem quickly learning to jump over enemies and obstacles and moves quick. But he somehow gets stuck at the blocky stairs. He literally does nothing once he reaches them and I have no idea how. I used all combinations of crossover/mutation-rates/... but no success. I also used frame stacking and frame skipping.

My alternative approach of the genome being directly the actions and using crossover and everything on them even worked better.

I know this is a quite a high level explanation but I can provide more details if needed. My CNN has 2 convolutional layers with 4 input channels, 16 output channels and my kernels are 8x8 and I use stride of 4. the last layer has 32 feauture maps of size 9x9 which I just put into final output layers to give me 7 logits (these are the possible actions) and take the highest one. This is the rough plan. I could adjust a lot of stuff but I would non the less expect to at least have one Mario reach the flag at least. Does anyone have ideas or experience with this library and genetic algorithms ?",3,2,2025-07-04 21:09:35,Ozay0900,0.8191,0.074,0.05,0.875,0.079367816091954,0.4151724137931035
[D] AACL Reputation,"In the ACL universe, ACL, EMNLP, and NAACL are generally considered equal. EACL is considered a bit lower but highly reputable and maybe even the same by some. I haven't heard much about the relatively newer AACL. What's your opinion on papers published there? Is it in the same ballpark of reputation, or is it still significantly lagging behind?

",12,7,2025-07-04 20:08:40,CadavreContent,-0.5588,0.0,0.074,0.926,0.0805555555555555,0.3972222222222222
[D] Did anyone receive this from NIPS?,"Your co-author, Reviewer has not submitted their reviews for one or more papers assigned to them for review (or they submitted insufficient reviews). Please kindly note the Review deadline was on the 2nd July 11.59pm AOE.

===
My co-author has graduated and no longer worked in academic anymore. How can I handle that? It is not fair to reject my paper!",49,29,2025-07-04 19:25:05,Dangerous-Hat1402,0.6426,0.113,0.056,0.831,0.1325,0.4599999999999999
[D] Does splitting by interaction cause data leakage when forming user groups this way for recommendation?,"I’m working on a group recommender system where I form user groups automatically (e.g. using KMeans) based on user embeddings learned by a GCN-based model.

Here’s the setup:
	•	I split the dataset by interactions, not by users — so the same user node may appear in both the training and test sets, but with different interactions.
	•	I train the model on the training interactions.
	•	I use the resulting user embeddings (from the trained model) to cluster users into groups (e.g. with KMeans).
	•	Then I assign test users to these same groups using the model-generated embeddings.

🔍 My question is:

Even though the test set contains only new interactions, is there still a data leakage risk because the user node was already part of the training graph? That is, the model had already learned something about that user during training.
be a safer alternative in this context.

Thanks!
",0,10,2025-07-04 19:17:17,AdInevitable1362,0.7616,0.051,0.017,0.932,0.0643939393939393,0.4174242424242425
[D] How trustworthy are benchmarks of new proprietary LLMs?,"Hi guys. I'm working on my bachelor's thesis right now and am trying a find a way to compare the Dense Video Captioning abilities of the new(er) proprietary models like Gemini-2.5-Pro, GPT-4.1 etc. Only I'm finding to have significant difficulties when it comes to the transparency of benchmarks in that area.

For example, looking at the [official Google AI Studio webpage](https://developers.googleblog.com/en/gemini-2-5-video-understanding/), they state that Gemini 2.5 Pro achieves a value of 69.3 when evaluated at the YouCook2 DenseCap validation set and proclaim themselves as the new SoTA. The leaderboard on [Papers With Code](https://paperswithcode.com/sota/dense-video-captioning-on-youcook2) however lists HiCM² as the best model - which, the way I understand it, you would need to implement from the ground up based on the methods described in the research paper as of now - and right after that Vid2Seq, which Google claims is the old SoTA that Gemini 2.5 Pro just surpassed.

I faced the same issue with [GPT-4.1](https://openai.com/index/gpt-4-1/), where they state 
>Long context: On Video-MME, a benchmark for multimodal long context understanding, GPT‑4.1 sets a new state-of-the-art result—scoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPT‑4o.
but the official Video-MME leaderboard does not list GPT-4.1. 

Same with VideoMMMU ([Gemini-2.5-Pro](https://deepmind.google/models/gemini/pro/) vs. [Leaderboard](https://huggingface.co/datasets/lmms-lab/VideoMMMU)), [ActivityNet Captions](https://paperswithcode.com/sota/dense-video-captioning-on-activitynet) etc.

I understand that you can't evaluate a new model the second it is released, but it is very difficult to find benchmarks for new models like these. So am I supposed to ""just blindly trust"" the very company that trained the model that it is the best without any secondary source? That doesn't seem very scientific to me. 

It's my first time working with benchmarks, so I apologize if I'm overlooking something very obvious. 

",10,6,2025-07-04 17:01:55,hhblackno,0.9662,0.1,0.023,0.877,0.1178436147186147,0.4424648268398269
[R] kappaTune: a PyTorch-based optimizer wrapper for continual learning via selective fine-tuning,"# This optimizer wrapper for continual learning is guided by the condition number (κ) of model tensors. It identifies and updates only the least anisotropic parameters to preserve pre-trained knowledge and mitigate catastrophic forgetting due to a synergy of factors: their inherent numerical stability makes them less susceptible to training noise, and their less specialized nature allows for robust adaptation without overwriting critical, highly specific pre-training knowledge, thereby effectively mitigating catastrophic forgetting of foundational capabilities (see the link to the paper in the repository): [https://github.com/oswaldoludwig/kappaTune](https://github.com/oswaldoludwig/kappaTune)",15,8,2025-07-04 16:51:03,Gold-Plum-1436,0.6324,0.128,0.06,0.812,-0.1009259259259259,0.5148148148148147
[R] Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs,"I recently released this preprint benchmarking LLM capability of self-correction.

**The Problem**: LLM self-correction is important for reliability, but it's hard to benchmark because naturally occurring errors are rare. So I built Self-Correction Bench by systematically injecting errors into LLM reasoning traces.

**Key Discovery**: LLMs systematically fail to correct errors in their own outputs while successfully correcting identical errors in external inputs. I call this the ""Self-Correction Blind Spot."" 

**Results across 14 models**: 

\- 64.5% average blind spot rate 

\- Simply appending ""Wait"" reduces blind spots by 89.3% without finetuning

\- Other correction markers (""But"", ""However"") also help 

\- Reasoning models generate these markers when they see errors

**Insight**: I analyzed post-training data and found non-reasoning instruction datasets are 95%+ lacking correction markers. RL-trained reasoning models don't show this blind spot - their generation contains lots of correction markers - suggesting they learned error correction through trial and error.

**Implications**: This affects AI safety and reliability. If LLMs can't catch their own mistakes, we need better training paradigms or activation mechanisms like correction markers. It seems RL is very promising.

Benchmark: [https://huggingface.co/papers/2507.02778](https://huggingface.co/papers/2507.02778)

Author here - happy to discuss the methodology and have your feedback.",6,0,2025-07-04 16:08:38,transformer_ML,-0.9528,0.115,0.19,0.695,-0.0002898550724637,0.6546583850931676
[D] Help understanding speculative sampling,"Hi all,

Need a bit of help understanding speculative sampling. [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192)

The idea is for the small model to generate the completions and the larger model to evaluate them. If the LLM accepts all the tokens generated by the SLM, it generates an additional token. If not, it generates the replacements of the tokens it rejected. Section 2.1 and 2.3 in the paper discuss this.

Given tokens x\_{<t}, p(x\_t | x\_{<t}) is the distribution generated by the target LLM. q(x\_t | x\_{<t}) is generated by a smaller, more efficient model (SLM). We want x \~ p(x), but we sample x\~q(x) and keep it IF q(x) <= p(x).

I don't quite get the logic of keeping the x\~q(x) sample if q(x) <= p(x). I'm sure it is something simple but a blind spot for someone dumb as me. Can someone please explain in simple terms?

Given a well-trained and a less capable model, and a sequence, in general, is there a relation between the probability distributions from both models for the next token? I would expect that the generations from the LLM have a higher likelihood of matching the next sequence in the training data.",2,2,2025-07-04 15:05:41,datashri,0.6145,0.099,0.05,0.851,0.0138888888888888,0.4091005291005291
[D] Combining box and point prompts with SAM 2.1 for more consistent segmentation — best practices?,"I’m developing an application using SAM 2.1 (via FastAPI) for real-time object segmentation from a live camera feed. The frontend sends either a box or point prompt to the backend, which returns a mask that’s composited into a canvas for manipulation and export.

Each prompt type works well in isolation — but they’re inconsistent across different object classes. A couple examples:

* **Plant in pot**: A box prompt captures the foliage but often excludes the pot. A point prompt on the leaves sometimes segments a single leaf, especially with fine stems or dense texture.
* **Theragun / handheld tool**: A point near the handle often gives excellent results. A box prompt sometimes returns background or over-segments nearby objects.

I’m now exploring combining both prompt types: drawing a bounding box *and* allowing the user to tap inside it to reinforce intent. Since SAM 2.1 accepts both `boxes` and `point_coords + point_labels`, this seems feasible — but I’m curious:

* Have others here tried combining these prompts in production or research tools?
* Are there heuristics you’ve found effective for prioritizing or weighting prompt types in ambiguous contexts?
* Do you use `multimask_output=True` and apply post-selection based on area, IOU, or visual saliency?
* Any recommended architectures or methods for mask refinement after prompt-based SAM segmentation (e.g. to recover small appendages like wires, roots, or hollow interiors)?

Would appreciate insights from anyone deploying SAM variants or experimenting with segmentation UIs. Trying to optimize for a broad class of “irregular physical objects” where semantic boundaries aren’t always visually dominant.",7,4,2025-07-04 14:18:43,w0nx,0.9891,0.142,0.012,0.846,0.1968945406445406,0.4427579365079365
[D] OpenAI Board Member on the Future of Machine Learning,[https://www.youtube.com/watch?v=-\_M5PY5BC9I](https://www.youtube.com/watch?v=-_M5PY5BC9I),0,0,2025-07-04 10:26:24,Electrical_Ad_9568,0.0,0.0,0.0,1.0,0.0,0.125
[R] Permutation Neuron: Achieving 77% Accuracy on MNIST with Three Neurons,"This article addresses the challenge of classification with minimal multiplication operations while maintaining accuracy above 75%. The MNIST dataset serves as an example, where a single permutation neuron, utilizing three classical neurons, achieves 77% accuracy.

# Concept of the Permutation Neuron

The Permutation Neuron is a computational unit that implements a permutation-based transformation of input signals. The neuron maintains a set of internal vectors that are reordered based on their interaction with the input data. This reordering process maps the input space to a discrete set of output patterns, where each pattern corresponds to a specific permutation of the internal vectors.

For classifying the 10 digits of the MNIST dataset, at least 10 distinct neuron states are required. Since the number of permutations is determined by the factorial of the number of neurons, a minimum of 4 neurons (4! = 24 permutations) is needed to cover 10 classes. However, by subtracting the value of one neuron from the others (normalization), only three neurons need to be computed, with the fourth set to zero, preserving the order of permutations. This reduces computational cost while maintaining 24 unique states for classification.

For the MNIST classification task, the permutation neuron operates as follows: three neurons with linear activation functions compute values based on the input image data, while a fourth neuron is fixed at zero. These four values are ordered to form one of 24 possible permutations (4!), such as ACZB. Using the Lehmer code, each permutation is mapped to a unique number from 0 to 23, which is then assigned to one of the 10 MNIST classes (e.g., digits 0–9).

# Training with a Genetic Algorithm

The search space for parameters is limited to 2355 values, where each of the three neurons processes input data of size 784 (MNIST image pixels) plus a bias term (3 × (784 + 1)). The 24 permutation states generated by the permutation neuron are determined by a greedy algorithm based on the MNIST training set, enabling the mapping of permutations to 10 classes. A genetic algorithm is employed to optimize the neuron weights, as the parameter space is poorly understood but assumed to contain local optima corresponding to effective solutions.

For weight optimization, a genetic algorithm with a population of 50 individuals is used. The BLX-Alpha crossover (with parameter k=2) is applied over two parents, with a 2% probability of random mutation. These settings achieved a classification accuracy of 77% on the MNIST dataset.

# Code

The implementation of the permutation neuron, including the genetic algorithm and the greedy algorithm for mapping permutations to MNIST classes, is available at [GitHub](https://github.com/sgr-team/math/tree/main/problems/pn). The code includes an experiment achieving 77% accuracy (results in mnist\_46257.json).

Readers are encouraged to reproduce the experiment or propose improved solutions, such as higher accuracy or fewer multiplication operations. Improved results will be published with attribution to their authors.",0,2,2025-07-03 18:05:45,mr00rng,0.9816,0.085,0.015,0.901,0.0412857142857142,0.3952857142857143
[P] Why am I getting poor performance with GNNs for edge prediction from node features only?,"Hi everyone,

I'm working on an industrial use case where I tried to use a Graph Neural Network to \*\*predict edges between tasks\*\*, based solely on node features.

Each graph represents 10-60 tasks (nodes), and I have about 1200 such graphs for training. Each task comes with features (label, equipment type), but there are no edges given at inference time, the goal is to infer all connections -> generate the full adjacency structure.

The key point: whether an edge exists between two nodes depends on the global context, not just pairwise similarity.

I’ve tried GCNs and GATs (with various edge construction strategies during training), but I'm consistently getting poor performance.

So I’m wondering:

\-  Is this just a bad fit for classical GNNs? 

\- Should I switch to Transformer-like models that encode full-node context? Or even fine-tuning ?

\- Do I need a much larger dataset to make a GNN work in this setup?

\- Is it better to frame this as a graph generation problem (autoencoders) ?

  
I know GNN needs edge-index during inference, but i genuinely do not seem to find the right model for my project...",1,1,2025-07-04 07:41:37,Head_Mushroom_3748,-0.9034,0.034,0.087,0.879,0.0053571428571428,0.5063988095238096
[D] Understanding Optimal Batch Size Calculation - Arithmetic Intensity,"I encountered this [talk ](https://www.youtube.com/watch?v=mYRqvB1_gRk)where the speaker (Timothée Lacroix of Mistral) states that an optimal batch-size is hardware dependent and can be calculated as 2xflops/mem\_bandwidth (6:40) -- Hence an optimal batchsize (B\*) for an A100 is 400.

I had some confusion on this formula - The memory bandwidth for a an A100 is 2TB/s, while the FLOPs (assuming FP16) are 312 TFlop - Can TFlops be divided by TBs though they are fundamentally different units?

Appreciate anyone who can help explain this - If anyone has suggested materials to learn more about how this number was derived, I would be very happy to take a look

I'm sure its related to[ Arithmetic intensit](https://www.iguazio.com/glossary/arithmetic-intensity/)y but that number is simply 312/2=156

EDIT:

Did some research based on answers and resources here and tried to come up with an explanation - If anyone cared to feedback or point out areas of improvement, would really appreciate it

**Arithmetic Intensity**

Performance is defined by memory bandwidth, compute, latency. If compute is more limited than memory, it is compute bound. Vice versa for memory bound. Arithmetic intensity is the ratio of compute operations to memory operations (Specifically FLOPs per byte transferred). If you are compute bound, optimizing for memory does not benefit your system, and vice versa. Calculating arithmetic intensity tells you which parts of your system to focus on optimizing. Arithmetic intensity itself is calculated as a hardware threshold as well as for individual operations. Real world performance depends on actual model architecture, dataset characteristics, training/inference regime, memory access patterns, cache utilization, batch size, operator fusion, etc…

Arithmetic intensity can also be applied to operations as below. **Values only approximate:**

Low arithmetic intensity operations (10-100 FLOPs/byte) include elementwise ops, activations, normalizations (Example, addition involves moving 2N values to GPU but doing only N ops)

High intensity ops (100 - 1000 FLOPs/byte) include matmuls and convolutions. Larger batch sizes also increase intensity - This is because input data increases while the memory access cost for weight matrices remains constant - Hence larger batches improve GPU compute utilization.

Hence, frameworks focus heavily on fusion of low intensity operations. Operations can have different arithmetic intensity depending on problem size (small matrices have lower intensity because less data can be reused), implementation (tiled algorithms are faster), precision (FP16 doubles available compute).

Consider the arithmetic intensity threshold. At 312 TFLOPs and a mem bandwidth of 1.55 TB/s for FP16 tensor ops in an A100, the arithmetic intensity threshold is roughly 201. Ops with intensity below this are memory bound, while ops above it are compute bound. A memory bound operation results in idle GPU compute while a compute bound operation results in bottlenecking. In practice, hitting this precise 100% resource utilization is rare. ",43,12,2025-07-04 06:56:14,Mundane-Earth4069,0.9714,0.1,0.049,0.851,0.1039324116743471,0.50415770609319
[D] Sampling technique for imbalanced dataset of a OOS prediction model,"Hey all,

I’m trying to build ML model for OOS prediction of an item of an imbalanced dataset, which sampling technique should I use and how should I evaluate that sampling technique to create a better model.

Appreciate your thoughts and responses.

Thanks",10,5,2025-07-04 05:12:22,shiva2692,0.8625,0.187,0.0,0.813,0.35,0.35
"[R]Group Recommendation Systems — Looking for Baselines, Any Suggestions?","Does anyone know solid baselines or open-source implementations for group recommendation systems?

I’m developing a group-based recommender that relies on classic aggregation strategies enhanced with a personalized model, but I’m struggling to find comparable baselines or publicly available frameworks that do something similar.

If you’ve worked on group recommenders or know of any good benchmarks, papers with code, or libraries I could explore, I’d be truly grateful for your. Thanks in advance!",6,1,2025-07-04 03:49:44,AdInevitable1362,0.93,0.19,0.04,0.77,0.2527777777777777,0.3111111111111111
[D] A Serious Concern on the ACL Rolling Review System,"
While I understand the traditional conference review paradigm involving initial scores, author rebuttals, and final scores, this model is beginning to show clear cracks under the scale and competitiveness of today’s A-level (and even mid-tier) venues. Increasingly, reviewers tend to give deliberately conservative or low pre-rebuttal scores, knowing that authors will be compelled to respond in the rebuttal phase. Even when a higher score is justified, reviewers often hold back, defaulting to borderline decisions just to see how the authors respond.

This issue is even more pronounced with ACL Rolling Review, where the scoring system is vague and lacks standard terminology such as Accept, Borderline, or Reject. This makes the process even more opaque. The ARR policy clearly states that responding to review comments is not mandatory. Yet, as an author, I am expected to thoroughly and respectfully address reviewer concerns, even when they are speculative or unreasonable. This one-sided non-obligation creates a deeply flawed power imbalance.

Here’s where it gets worse.

Many reviewers, when submitting their own papers and receiving poor reviews, tend to reflect their frustration onto the papers they are assigned to review. I have observed the following patterns:

Case 1: A reviewer receives bad reviews on their own paper and becomes unnecessarily harsh or disengaged in the reviews they provide for others.

Case 2: Prior to seeing their own reviews, reviewers play it safe by giving slightly lower pre-rebuttal scores than deserved. After receiving unfavorable reviews, they either ignore rebuttals completely or refuse to revise their scores, even when rebuttals clearly address their concerns.

This leads to a toxic feedback loop where every paper becomes a collateral victim of how a reviewer’s own submission is treated. I have seen this firsthand.

In the current ARR May cycle:
I received 10 reviews across 3 papers, with only 2 reviewers responding post-rebuttal.

From 4 papers I reviewed, totaling 12 reviews, only 6 reviewers responded, and 4 of those responses were mine.

We need to acknowledge a basic truth: acknowledging a rebuttal should be a moral minimum. Yet today, there is no incentive for honest reviewing, and no consequence for disengaged or negligent behavior. Why should any of us continue to uphold moral obligations, being fair, constructive, and thorough, when our own work receives careless and dismissive treatment?

This culture cannot be allowed to continue. Unless ACL/ARR enforces stricter policies, such as making post-rebuttal justification and score updates mandatory (as CVPR and other CVF conferences do), the system will continue to erode.

I am a young researcher trying to do my part for this community. But after repeated experiences like this, what incentive do I have to stay committed to high standards as a reviewer? Why should I put in the effort when others do not?

A system where morality is optional will ultimately breed apathy and toxicity. It is time for a structural shift.

Always, to the hope.


#acl #emnlp #arr",47,13,2025-07-03 21:16:39,powerful_lord_33,0.8889,0.082,0.065,0.853,0.0838775510204081,0.5446938775510202
"[D] Position: Machine Learning Conferences Should Establish a ""Refutations and Critiques"" Track","We recently released a preprint calling for ML conferences to establish a ""Refutations and Critiques"" track. I'd be curious to hear people's thoughts on this, specifically (1) whether this R&C track could improve ML research and (2) what would be necessary to ""do it right"".",107,26,2025-07-03 18:52:58,RSchaeffer,0.6369,0.091,0.0,0.909,0.0464285714285714,0.6964285714285714
[D] Hyperparameter Optimization with Evolutionary Algorithms: A Biological Approach to Adaptive Search,"Data Science is a fascinating field, with always something to learn. Recently, I came across an interesting (though not ideal) approach to hyperparameter optimization: Evolutionary Algorithms (EA). EAs are a subset of Genetic Algorithms that work on Darwin’s idea of “survival of the fittest”. While Grid Search and Manual Tuning remain the go-to approaches, they are limited by predefined search space and, in some sense, are brute-force methods to optimize hyperparameters. Interestingly, Evolutionary Algorithms work on the principles of biology and genetics:

1. They start with a population of candidate solutions (hyperparameters) and treat them as chromosomes.
2. Each chromosome is then evaluated using a fitness test (for example, precision, absolute error etc.)
3. The best-fit candidates are selected as parents.
4. Parent solutions generate offspring using crossover (combining individual traits) and mutation (small random changes)
5. The offspring are then used as candidate solutions, and steps 1-4 are repeated till an optimal solution (under a defined threshold) is met or iterations are exhausted.

While this is a computationally expensive solution, EA offers an adaptive methodology instead of static search methods, which can look for solutions that are not pre-defined.

Thoughts?

Note: EA is not a silver bullet to all your optimization problems.",11,16,2025-07-03 17:47:46,SaadUllah45,0.9744,0.165,0.042,0.794,0.023469387755102,0.553061224489796
[P] Built a semantic search API,"Working on a project that needed both semantic search and content moderation, so I built an API that handles both.

**The problem it solves:** Expensive GPU instances required for inference, hard to scale infrastructure. Most teams give up quickly after realizing the infrastructure needed to handle this.

**What it does:** Semantic search + content moderation. You can search images by describing them (""girl with guitar"") or find text by meaning (""movie about billionaire in flying suit"" → Iron Man). Plus NSFW detection with specific labels.

**Stack:**

* Rust Candle for ML models (Clip)
* Rust Axum + Tokio for the API
* Vector DB for search

I am considering switching to a more lightweight CLIP based model like mobileclip or clip quantized. What do you guys think?",0,3,2025-07-03 17:25:09,K3NCHO,-0.1531,0.02,0.033,0.947,0.0902777777777777,0.4777777777777778
[D] What operations should I fuse in a transformer?,"I am pretraining a GPT-style language model with PyTorch XLA and wanted to know what operations to fuse with Pallas. I use rotary positional embeddings, SwiGLU, and RMSNorm, and I am working on adding FlashAttention to my codebase. I also employ FSDPv2 with SPMD for distributed training.",0,0,2025-07-03 16:57:10,New-Skin-5064,0.0,0.0,0.0,1.0,0.0,0.0
[D] AAAI-2026 2 phase review discussion,"**{another edit} I got it that it won't be used for decision making. I posted it to ask if it is true.. and realized that many of us did not know about this**

<previous post>

AAAI-26' Two-phase reviewing for the Main Track:

[https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/](https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/)

Phase 1: Two reviews supplemented by one AI-generated, non-decisional review.

Phase 2: Additional reviews for papers not rejected in Phase 1.

**Author response after Phase 2, only for papers not rejected in Phase 1.**

Edit : They also said (but why the use of AI tho )  
The pilot program will thoughtfully integrate LLM technology at two specific points in the established review process:

Supplementary First-Stage Reviews: LLM-generated reviews will be included as one component of the initial review stage, providing an additional perspective alongside traditional human expert evaluations.

Discussion Summary Assistance: LLMs will assist the Senior Program Committee (SPC) members by summarizing reviewer discussions, helping to highlight key points of consensus and disagreement among human reviewers.

<previous post>",31,22,2025-07-03 07:28:15,i_minus,0.8483,0.074,0.015,0.912,0.1287878787878787,0.4598484848484849
[D] Are NLP theory papers helpful for industry research scientist roles?,"Currently I'm quite interested in NLP theory, and have some questions about how to make them count for RS roles in industry roles at top AI labs.  
(1) Does the number of papers help? My impression is that having many papers that are ""purely theoretical"" may not help that much, and AI labs will only count the number of ""relevant papers"" (and exclude those that are less relevant).   
(2) If the theory paper also yields strong empirical results, is it important to frame it as an empirical paper (and maybe put the theory in the appendix)? This could compensate for any perceived weakness with theoretical work.  
(3) What topics in language/vision models are particularly relevant in industry? Efficiency of LLMs is one priority; MoE, sparse attention & structured sparsity, are two approaches to efficient LLMs.",16,9,2025-07-03 05:34:46,random_sydneysider,0.9645,0.175,0.029,0.796,0.2197916666666666,0.5
[D] AI/ML interviews being more like SWE interviews,"Have people noticed that AI/ML/DS job interviews now feel more SWE-like? For example, relying more on data structures and algorithms leetcode questions. I’ve noticed in my professional friend groups more people are being asked these questions during the coding interview.",136,43,2025-07-03 04:15:35,guohealth,0.7178,0.115,0.0,0.885,0.42,0.42
[D] Paper with code is completely down,"Paper with Code was being spammed (https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d\_paperswithcode\_has\_been\_compromised/) before, and now it is compoletely down. It was also down a coupld times before, but seems like this time it has lasted for days. (https://github.com/paperswithcode/paperswithcode-data/issues)

",50,19,2025-07-03 02:33:02,Striking-Warning9533,0.5023,0.079,0.0,0.921,-0.1555555555555555,0.2888888888888889
[D] What Tool to Use to Create Illustrations Like This?,"Recently, I’ve seen many researchers adopt this style of illustration to present an architectural view of their method or approach. These visuals are clean, professional, and visually appealing, perfect for research papers and presentations.

I've tried replicating this style using [draw.io](http://draw.io), but I haven’t been able to achieve the same level of quality or aesthetics.

Could anyone suggest tools or software commonly used to create such research illustrations?

I'm particularly interested in tools that are:

1. Suitable for academic or technical diagrams

2. Capable of producing high-quality, publication-ready visuals

3. Flexible for custom styling or layouts

Any recommendations would be greatly appreciated!

Please check Illustration here: [https://imgur.com/a/VWiKD3Q](https://imgur.com/a/VWiKD3Q)",3,13,2025-07-03 02:17:58,Outrageous_Tip_8109,0.9802,0.223,0.0,0.777,0.1759259259259259,0.425
[D] UofT PhD Ranking,"In terms of academia prestige (for future prof positions), where would you place UofT ML PhD? Is it better RoI to do it at a T10 American school (UIUC, Georgia Tech, UT Austin, UWash, etc) for name recognition considering the advisors are equivalent? Also, how does UofT PhD fare against Oxbridge DPhil these days?",1,26,2025-07-02 23:46:00,Top-Purchase926,0.533,0.058,0.0,0.942,0.1666666666666666,0.2083333333333333
[D] Applicability of a Biomedical based AI/ML PhD to other AI/ML fields,"Hey all,

I am a first year PhD student in a top biomedical program in the US. One of the labs I am most interested in studies how to more effectively use AI/ML to enhance the drug discovery and development process. Although I current have only a limited knowledge of coding (really just experience with R and a little C++) the PI has told me he'd be happy to have me join the group. Still, I wonder about the applicability of this niche expertise. Does having done a PhD in biomedical focused AI/ML allow for the possibility of being hired in say finance AI/ML? What about AI/ML research in big tech? Or would you say it is only applicable in Big Pharma/biomed startup research?

Thanks for your insights.",4,2,2025-07-02 23:27:30,evilpastabake,0.9579,0.152,0.013,0.835,0.2009453781512605,0.4794817927170868
[P] DFReg: A Physics-Inspired Regularization Method That Operates on Global Weight Distributions (arXiv:2507.00101),"Hi everyone,

I’d like to share a recent preprint I uploaded to arXiv, introducing **DFReg** – a new regularization framework for neural networks inspired by **Density Functional Theory (DFT)** in physics.

**What is DFReg?**  
DFReg replaces local penalties (like L2 regularization or Dropout) with a **global constraint** on the *empirical weight distribution*. It treats the weights of a neural network as a statistical density and introduces a functional penalty that encourages:

* Smooth, non-peaky weight distributions
* Diverse, well-spread parameter configurations
* Structural regularity across layers

No architectural changes or stochastic perturbations required.

**What we tested:**  
We evaluated DFReg on **CIFAR-100 with ResNet-18**, comparing it to Dropout and BatchNorm. Metrics included:

* Test accuracy and loss
* Weight entropy
* Histogram regularity
* 2D FFT of convolutional filters

Notably, we also trained **BatchNorm-free ResNets** with only DFReg as the regularizer.

**Key findings:**

* DFReg matches or outperforms Dropout and BatchNorm on accuracy and stability
* It induces more interpretable and spectrally regular weight structures
* Even without L2 or BatchNorm, DFReg alone provides strong regularization

**Paper**: [https://arxiv.org/abs/2507.00101](https://arxiv.org/abs/2507.00101)  


Would love to hear feedback from the community—especially if you're interested in global priors, regularization, or physics-inspired ML. Open to questions, critiques, or collaborations.

Thanks!",2,0,2025-07-02 10:10:32,QuantumFree,0.9402,0.111,0.044,0.845,0.1220893141945773,0.4165685192000981
[D] Understanding DDIM : Accelerated Sampling Case,"Hello,

I have been going through DDIM paper and have some queries on how the sampling is accelerated (appendix C.1)

The authors assume that the forward can be decomposed as

[Forward decomposition](https://preview.redd.it/n0yvok1liiaf1.png?width=520&format=png&auto=webp&s=0cbce45652fccf8f10441b25238e8fd8136c7e37)

and backward

[Backward decomposition](https://preview.redd.it/f5gtpdrmiiaf1.png?width=437&format=png&auto=webp&s=2d1f597df36d5dcdab955a167e8fb588a866184d)

where tau is subsequence of timesteps \[1, T\].

First thing I want to point out is that, index ""i"" should start from 2 and from 1. (Am I right in saying this ?)

If you look into the decomposition, in the forward for the timesteps that are not in the subsequence, we are directly writing x\_{t}|x\_{0} and for the timesteps that are in subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}},x\_{0}.

So to mimic in the reverse we write for the timesteps that are not in subsequence x\_{0}|x\_{t} and for timesteps in the subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}}.

The above explaination looks good in intuitive sense but when I take an example and write the decomposition, the intutition doesn't come at all.

[Example](https://preview.redd.it/6zn8fux3piaf1.png?width=705&format=png&auto=webp&s=6628a8167fc4e6a7458054d9872a1caf9a338292)

Here the third term in backward p(x\_{3}|x\_{4},x\_{5}) = p(x\_{0}|x\_{3}) and fifth p(x\_{1}|x\_{2},x\_{3},x\_{4},x\_{5}) = p(x\_{0}|x\_{1}) doesn't make sense at all.

Can someone explain how does the backward decomposition work ?

Note : I don't know if this is the correct place to ask these type of questions, but I felt that other subs are not suited for this.

Thanks.",1,1,2025-07-02 20:18:43,hyperellipticalcurve,0.8235,0.041,0.0,0.959,0.1763392857142857,0.3180059523809524
[P] The tabular DL model TabM now has a Python package,"Hi! My colleagues have recently published a Python package for [TabM](https://github.com/yandex-research/tabm) \-- a **simple and powerful DL architecture** for solving predictive tasks on **tabular data** (classification, regression, etc.).

In a nutshell, TabM efficiently imitates an ensemble of MLPs (see the image below). This basically means that TabM has the power of an ensemble, but at the same time remains practical and scalable. Among the recent highlights: 🏆 **TabM has been successfully used on Kaggle**, including the winning solutions! The package provides the PyTorch implementation of TabM, as well as PyTorch layers and functions for building custom TabM-like models.

Installation:

```
pip install tabm
```

- [Paper](https://arxiv.org/abs/2410.24210)
- [Package](https://github.com/yandex-research/tabm)
- [Colab example](https://colab.research.google.com/github/yandex-research/tabm/blob/main/example.ipynb)


[TabM model illustration](https://preview.redd.it/pl3oth89qgaf1.png?width=2432&format=png&auto=webp&s=37ed08404f3eee2e2a72dc41aa796b17ed6ae32b)",27,2,2025-07-02 13:18:14,_puhsu,0.9561,0.151,0.0,0.849,0.2392857142857143,0.5260204081632652
[D] How to become fluent at modifying/designing/improving models?,"By fluency I mean:

1. Read a paper and and without much problem implement the techniques mentioned, whether it's building something from scratch using the paper as guidance (even in the absence of code), or modifying existing models.
2. Having an idea and being able to translate that into designing new architectures or modifying existing models.
3. Improving models.

Think of people like [Phil Wang](https://github.com/lucidrains) who is very prolific at reproducing papers and or improving them. I'm very curious to know in your experience what made it ""click"" that unlocked your ability to be productive with these things. I suspect the boring answer is ""just reproduce papers, bro"", but I was hoping to learn about people's own experience/journey on this and if you guys have any specific insight/tricks that can be useful for others to know about. Like maybe you have a good workflow for this or a good pipeline that makes you 10x more productive, or you have some niche insight on designing/modifying/improving models that people don't usually talk about etc.",25,14,2025-07-02 07:50:25,total-expectation,0.9747,0.151,0.017,0.833,0.1420909090909091,0.4894696969696969
"[D] How will LLM companies deal with CloudFlare's anti-crawler protections, now turned on by default (opt-out)?","Yesterday, [Cloudflare had announced](https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/) that their protections against AI crawler bots will be turned on by default. Website owners can choose to opt out if they wish by charging AI companies for scraping their websites (""pay per crawl"").

The era where AI companies simply recursively crawled websites with simple GET requests to extract data is over. Previously, AI companies simply disrespected robots.txt - but now that's not enough anymore.

Cloudflare's protections against crawler bots are now pretty sophisticated. They use generative AI to produce scientifically correct, but unrelated content to the website, in order to waste time and compute for the crawlers (""[AI Labyrinth](https://blog.cloudflare.com/ai-labyrinth/)"").  This content is in pages that humans are not supposed to reach, but AI crawler bots should reach - invisible links with special CSS techniques (more sophisticated than `display: none`), for instance. These nonsense pages then contain links to other nonsense pages, many of them, to keep the crawler bots wasting time reading completely unrelated pages to the site itself and ingesting content they don't need.

Every possible way to overcome this, as I see it, would significantly increase costs compared to the simple HTTP GET request recursive crawling before. It seems like AI companies would need to employ a small LLM to check if the content is related to the site or not, which could be extremely expensive if we're talking about thousands of pages or more - would they need to feed every single one of them to the small LLM to make sure if it fits and isn't nonsense?

How will this arms race progress? Will it lead to a world where only the biggest AI players can afford to gather data, or will it force the industry towards more standardized ""pay-per-crawl"" agreements?",98,91,2025-07-02 07:44:50,Endonium,0.9534,0.107,0.065,0.828,0.1161172161172161,0.5507631257631258
[D] Will the relationship between Meta's FAIR and Super Intelligence Labs be like that of Google Brain and DeepMind previously?,"I really don’t get the point of setting up a new AI lab at Meta.  
Well, maybe it’s related to the semi-acquisition of Scale AI and creating a group dedicated to Alexandr Wang.  
But doesn’t the merger of Google Brain and DeepMind suggest it’s better not to split your resources in the AI war?

Also would there be possible feud out there?

",24,9,2025-07-02 03:43:10,xiikjuy,0.6549,0.187,0.089,0.724,0.2670995670995671,0.5887445887445887
[D] Classical ML prediction - preventing data leakage from time series process data 🙏,"Anyone working in process industry and has attempted making “soft sensors” before?

Given a continuous industrial process with data points recorded in a historian every minute, you try to predict the outcome by applying classical ML methods such as xgboost. 

The use case demands that the model works like a soft(ware) sensor that continuously gives a numerical prediction of the output of the process. Not that this is not really a time series forecast (eg not looking into the distant future, just predicting the immediate outcome).

Question: Shuffling the data leads to data leakage because the neighbouring data points contain similar information (contains temporal information). But if shuffling is not done, the model is extremely poor / cannot generalise well.

Fellow practitioners, any suggestions for dealing with ML in that may have time series related data leakage?

Thanks in advance for any kind sharing.





",7,10,2025-07-02 01:30:34,kayhai,0.8126,0.086,0.048,0.866,0.0249999999999999,0.3354166666666667
[D] Request for Career Advice – ML PhD non hot topic,"I’m currently a PhD student in Machine Learning, working on a research topic that isn’t considered “hot” in the current academic or industrial landscape. Despite this, I’ve managed to publish as the lead author at ICML, NeurIPS. And  twice at ECML. I also have two co-authored publications at ECAI.

I’ve noticed that many  PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market—especially given the wave of layoffs and increasing demand for very specialized expertise in industry.

That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren’t the direct focus of my PhD research).

Given all of this, I’m trying to realistically assess: • What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? • What could I do now to improve those chances? • Goal is FANNG.

I’d greatly appreciate any  feedback.

Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.",57,39,2025-07-01 23:57:13,Hope999991,0.9447,0.101,0.017,0.883,0.1884711779448621,0.4515037593984963
[P] ML deployment,"  
Has anyone here deployed models on **Firebase** or **Vertex AI**? I'm looking for the best practice for a clean and cohesive deployment (we have real-time data, and I need to design a continuous retraining pipeline; in essence, the inferences will be used to update a dashboard).",1,0,2025-07-01 23:51:23,One-Fishing-5915,0.7845,0.138,0.0,0.862,0.6833333333333333,0.5
[D] Subreviewing for NeurIPS,"Does your professor share their assigned papers among their lab members and ask them to sub-review for NeurIPS? I only realized after agreeing that this is actually against [the reviewer guidelines](https://neurips.cc/Conferences/2025/ReviewerGuidelines):

>Q: Can I invite a sub-reviewer to help with my reviews?

>A: No, sub-reviewers are not allowed. Conflicts of interest cannot be properly checked unless reviewers are officially in the system, and sub-reviewers would not be able to participate in the discussion, which is a critical phase of the review process.

So now I am a little bit worried I may be involved in something I perhaps shouldn't have been. On the other hand, perhaps this is one of those things in academia that people are against ""on paper"" but is actually an accepted practice? I think it seems common for professors to review papers through their students, but it seems like in most cases, they are officially appointed as a ""sub-reviewer"" (which NeurIPS doesn't allow) instead of giving their professor a review to pass as their own.

In short: Is this normal and accepted? Does it happen in your lab, too? Should I not worry about it?

**Update:** Thank you to everyone who let me know that I won't get in any trouble for sub-reviewing. That's relief to know. Although, I am wondering:

* Do guidelines + code of conduct mean nothing? Why are they in place if they won't be respected? Based on the responses, ignoring them seems not too uncommon.
* Isn't signing your name under a ghost-written review without crediting the ghostwriter a form of plagiarism? Wouldn't a student be reprimanded for plagiarism if they did this in a class? How is this different? Am I the only one who believes this still seems unethical?",17,16,2025-07-01 18:44:57,Minute_Scholar308,0.9001,0.115,0.069,0.816,0.0791666666666666,0.5576388888888889
"[R] Introducing DreamPRM, a multi-modal LLM reasoning method achieving first place on the MathVista leaderboard","I am excited to share our recent work, DreamPRM, a multi-modal LLM reasoning method that ranks first currently on the MathVista leaderboard.

https://preview.redd.it/54v78gz7zaaf1.png?width=1348&format=png&auto=webp&s=0084aef7727d9f02c129d8414582018fb09eedb5

https://preview.redd.it/u0c02on9zaaf1.jpg?width=1374&format=pjpg&auto=webp&s=42ab8761dcc972ed89a999dac503c9dc35e65e18

  




Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM’s domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.



Paper: [https://arxiv.org/abs/2505.20241](https://arxiv.org/abs/2505.20241)



Code: [https://github.com/coder-qicao/DreamPRM](https://github.com/coder-qicao/DreamPRM)",2,0,2025-07-01 18:07:42,pengtaoxie,0.9646,0.12,0.045,0.835,0.0608465608465608,0.4541005291005291
[D]Looking for Hinglish (code-mixed Hindi-English) speech emotion audio datasets — any recommendations?,"Hi everyone,
I'm working on a deep learning project involving emotion recognition from Hinglish (code-mixed Hindi-English) speech.

I’ve found plenty of datasets for English (like RAVDESS, IEMOCAP) and some for Hindi (MUCS, OpenSLR), but I’m having trouble locating datasets that contain Hinglish speech, especially with emotion labels.

Do any of you know of:
Hinglish speech datasets (code-switched Hindi-English)
Emotion-labeled Hinglish audio
Open-source or research datasets that allow this type of training

If there are no public datasets, I’d also appreciate tips on how to create or augment one from scratch.
And also how can I increase it accuracy.

Thanks in advance!",1,2,2025-07-01 17:18:53,Due_Confusion_8014,0.8516,0.125,0.051,0.824,0.0083333333333333,0.3111111111111111
[D] Recommended preparation material for ML interviews.,"Hi everyone,

Below I am gathering some interview preparation tools for ML research positions. People who had been in the job market recently, which one would you recommend/ find more relevant? Any other resources that I might be missing?

(1) InterviewQuery:

[https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0](https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0)

(2) DevInterview:

[https://devinterview.io/questions/machine-learning-and-data-science](https://devinterview.io/questions/machine-learning-and-data-science)

(3) aiofferly:

[https://www.aiofferly.com/problems?page=5](https://www.aiofferly.com/problems?page=5)

(4) MAD:

[https://www.madinterview.com/ml?utm\_source=google&utm\_medium=cpc&utm\_campaign=22464693824&utm\_term=machine%20learning%20coding%20interview%20questions&utm\_content=178169327653&gclid=CjwKCAjw3f\_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD\_BwE&gad\_source=1&gad\_campaignid=22464693824&gbraid=0AAAAA\_Y9DohjdsVwcsLkazvDd4iJ64Tv5](https://www.madinterview.com/ml?utm_source=google&utm_medium=cpc&utm_campaign=22464693824&utm_term=machine%20learning%20coding%20interview%20questions&utm_content=178169327653&gclid=CjwKCAjw3f_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD_BwE&gad_source=1&gad_campaignid=22464693824&gbraid=0AAAAA_Y9DohjdsVwcsLkazvDd4iJ64Tv5)",32,9,2025-07-01 16:44:53,South-Conference-395,-0.7425,0.029,0.115,0.856,-0.0083333333333333,0.5125
[D] Computing Attention Scores with Long Context LLMs,"I'm trying to compute the top-k tokens yielding the highest attention scores with inference frameworks such as vLLM or the plain HuggingFace transformers. The models I'm using are not big in terms of parameters (max 7B) but huge in terms of context windows (up to 1M tokens, and I'm using all of it). However, I face two problems:

1. When using vLLM, I cannot access the attention scores in any way. Am I missing something or is the feature not yet implemented?  
2. When using transformers, I need to use flash\_attention\_2 otherwise the GPU budget skyrockets to 400+ GBs when using large inputs (i have a machine with 8 A100 for a total of 320GB of VRAM). However, when using flash\_attention\_2 the output attention scores are all None, and the only way to solve this seems to use an eager attention implementation, which makes it unfeasible in terms of GPU requirements.

Is someone facing a similar problem? How do you compute the attention scores for such large inputs?",3,1,2025-07-01 16:19:30,Debonargon,-0.466,0.046,0.058,0.896,0.0303571428571428,0.4845238095238096
[D] Looking for AI-powered smart crop library - smartcrop.py isn't enough,"https://preview.redd.it/r1w6xzdnbaaf1.png?width=1492&format=png&auto=webp&s=5ab883dcc781312bb6014b9daf1d9295dfc66030

Hey everyone!

I'm currently using smartcrop.py (github.com/smartcrop/smartcrop.py) for image cropping in Python, but it's pretty basic. It only detects edges and color gradients, not actual objects.

For example, if I have a photo with a coffee cup, I want it to recognize the cup as the main subject and crop around it. But smartcrop just finds areas with most edges/contrast, which often misses the actual focal point.

**Looking for:**

* Python library that uses AI/ML for object-aware cropping
* Can identify main subjects (people, objects, etc.)
* More modern than just edge detection

Any recommendations for libraries that actually understand what's in the image?

Thanks!",0,4,2025-07-01 15:55:25,Benjo118,0.8763,0.104,0.02,0.876,0.1300595238095238,0.4042410714285713
[R] Transition Matching: Scalable and Flexible Generative Modeling,Imo a silent banger by Meta - generalizing diffusion and flow matching into transition matching which can be used in a unified causal generation process.,7,0,2025-07-01 15:27:06,Luuigi,0.5423,0.138,0.0,0.862,0.0,0.1
[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed,"Hey everyone,

I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.

After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:

* **Classical ML still dominates**: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research
* **Algorithm preferences by medical condition**: Different health problems gravitate toward specific algorithms 
* **Transformer takeover timeline**: You can see the exact point (around 2022) when transformers overtook LSTMs in medical research

I built an interactive dashboard where you can:

* Search by medical condition to see which algorithms researchers are using
* Track how algorithm usage has evolved over time
* See the distribution across classical ML, deep learning, and LLMs

One of the trickiest parts was filtering out false positives (like ""GAN"" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).

The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.

Happy to answer any questions or hear suggestions for improving it!",121,22,2025-07-01 11:41:03,Avienir,0.9682,0.122,0.031,0.847,0.076,0.3289999999999999
[P] How do I detect whether a person is looking at the screen using OpenCV?,"Hi guys, I'm sort of a noob at Computer Vision and I came across a project wherein I have to detect whether or not a person is looking at the screen through a live stream. Can someone please guide me on how to do that?

The existing solutions I've seen all either use MediaPipe's FaceMesh (which seems to have been depreciated) or use complex deep learning models. I would like to avoid the deep learning CNN approach because that would make things very complicated for me atp. I will do that in the future, but for now, is there any way I can do this using only OpenCV and Mediapipe?

  
PS. Sorry for the wrong tag mods",0,5,2025-07-01 06:48:22,Defiant_Strike823,-0.5771,0.049,0.07,0.881,-0.2015151515151515,0.6361111111111112
[D] Any path for a mid career/mid aged MLE to do ML research in the industry,"I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...

But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.

I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.

Some thoughts in my mind:  
(1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.  
(2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?

I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.",46,15,2025-07-01 05:25:01,LastAd3056,0.9689,0.114,0.058,0.828,0.0322196620583717,0.5015213281342314
[R] Inference-Time Scaling and Collective Intelligence for Frontier AI,"TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.

Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.

Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.

AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.

This research builds on our 2024 work on evolutionary model merging, shifting focus from “mixing to create” to “mixing to use” existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.

Blog: https://sakana.ai/ab-mcts

Paper: https://arxiv.org/abs/2503.04412

Algorithm: https://github.com/SakanaAI/treequest

ARC-AGI Experiments: https://github.com/SakanaAI/ab-mcts-arc2

If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",22,1,2025-07-01 04:05:05,iwiwijp,0.9953,0.214,0.015,0.771,0.2086654724585759,0.4775302283922974
[D] How far are we from LLM pattern recognition being as good as designed ML models,"LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. 

Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.

I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.

I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. 

Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? ",32,49,2025-07-01 00:43:43,chrisfathead1,0.9859,0.162,0.037,0.801,0.3359848484848485,0.5843939393939394
[P] I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax,"Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).

Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.

This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.

I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!

Link to the repo: [https://github.com/elevanaltd/octave](https://github.com/elevanaltd/octave)",0,0,2025-06-30 23:53:06,sbuswell,0.9929,0.166,0.0,0.834,0.1353349673202614,0.4836601307189542
[R] BIG-Bench Extra Hard,,11,0,2025-06-30 22:41:39,EducationalCicada,-0.1027,0.0,0.318,0.682,-0.1458333333333333,0.3208333333333333
[R] Interpreting Large Language Models' Personality through Critical Event Analysis,"Excited to share our new work, ""Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis"" accepted at the Actionable Interpretability Workshop @ ICML 2025.

Introducing the Supernova Event Dataset

We present a new benchmark built from real-world Wikipedia articles, including biographies, historical milestones, global news, and scientific discoveries (including articles from Google Deep Research). This dataset introduces a novel task: critical event analysis for interpreting the behavioral pattern, or “personality” of LLMs.

Rather than looking inside the model (activations, traces), we ask a separate LLM to judge what events are most critical, and use this external perspective to decode the model’s values and reasoning traits.

Some early insights:

Orca2 tends to prioritize emotional and interpersonal events.

Phi-4 and Qwen2.5 focus on strategic milestones.

In scientific discovery, o3 highlights causal breakthroughs, Gemini 2.5 Pro favors methodological innovations, and Claude Sonnet 3.7 emphasizes conceptual clarity.

While these are early findings (still without human evaluation), the diversity in critical event patterns is striking. We believe assigning LLMs ""personalities"" could make them more relatable and trustworthy, enabling smoother human-AI collaboration, especially in domains like scientific discovery.

Paper: [arxiv.org/abs/2506.12189](https://l.facebook.com/l.php?u=http%3A%2F%2Farxiv.org%2Fabs%2F2506.12189%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeYgHhUQQ5lNulNDLnSeulM64ECl3ls5tJCNkMC1_EPhqEHk1uqWYnEdBsu7g_aem_fW6BPSFPpkV4xELlQOiODA&h=AT0m3E04nqeA-MMtev7Ouz9OW3PeW5A_y6V9zj9dqy3WrwMXLOeTXVO9EzTXtMey6tWCwh1vUB5rS0lMPeoFEdjHj6BecO9zq9__xYIrQwGC6nhRT8BEE50RlAm9OuXWhk_HCtHu&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Twitter: [https://x.com/Pranav\_AL/status/1939681069554655382](https://x.com/Pranav_AL/status/1939681069554655382)

Webpage: [http://supernova-event.ai](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeZQWpZz3uLtupNGIkmvzqpDbbG9u83w7Tv9ifY4Rjvct6zdjy8E4yg6NNDTM_aem_SgZlEI9ACWixaHK8TQWBPw&h=AT3tIyMocFJs9OJneCmpLWwlEjH3FE0RtckM5RYggzkKKuFMG5AIIScnpzGDhh7YqxBOSxpleqKt0hXYWIiiG_t3RoKtnoI1vlHkHUCsMhHlTmKcoQvqBSUnk8rLDci3doz0NpFV&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Demo: [supernova-event.ai/#your-story](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg%23your-story&h=AT1SxDBmgr_G-a-c-D9g4JyMlgs5bD-liDDwNRTZlIP_5CBvyl8meA3pwaNKABRXLqvHcemnEJv-sWcf3oan2b3FSWZ7H_yD3Y8mdhI5Ze6mVNULKBXQTdFqgrLNUynlNQjPjP2_&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Code: [https://github.com/pranavAL/Supernova-Event-Dataset](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2FpranavAL%2FSupernova-Event-Dataset%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg&h=AT3sNwJZLhvA9OG4GbkPvPxvAXxtZr9drQAj1Rp-4MCOHOOVbjH1epznhz08JAKypffQNwntIbz6TWzMTDmKVgXvDw7y6Yrg6Bcijqgxco34C_R4ivMwgS83oW5i2QnMFFQmQuVt&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

We're working toward scaling this into a real-world product, and we're currently seeking the right resources and support to take it further. If you're interested in what we're building and see potential for impact, we’d love to hear from you. Reach us at [hello@supernova-event.ai](mailto:hello@supernova-event.ai) ; we're open to conversations, collaborations, and any form of support that can help push this idea forward.

https://preview.redd.it/uugbpxw075af1.png?width=1200&format=png&auto=webp&s=ccbde6f1ace6140ff2ca838ffb0e60522759dc70",2,0,2025-06-30 22:40:30,Personal_Click_6502,0.9802,0.142,0.039,0.819,0.0017717996289424,0.58291280148423
[D] Is this PhD in LLM editing a good idea?,"Hello everyone, this is my first time posting here, and I wanted to get some opinions on the phd position I applied to.

So I am studying ml in France and I have a chance to do a PhD in the topic of LLM knowledge locating and editing. One paper that talks about this is the ROME (Rank One Model Editting - [https://arxiv.org/abs/2202.05262](https://arxiv.org/abs/2202.05262))

Basically, I would work on the internals of LLMs, analysing where exactly the knowledge for a certain fact is stored, and how can it be edited out. So messing around the directly with the components such as the attention and MLP weights.

For me personally, I like the idea of going inside the LLMs, instead of just inferencing/training and using them as some black boxes.

And I suppose that this would qualify me for jobs of actually creating LLMs (I do not expect to end up in OpenAI) but also make me more qualified for standard LLM usage jobs.

Any opinion or comment would be appriciated!",0,2,2025-06-30 20:18:40,sheckyCS,0.685,0.051,0.0,0.949,0.0873015873015872,0.4073412698412698
[D] Should we petition for requiring reviewers to state conditions for improving scores?,"I’ve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, “If the authors add experiments on XYZ” or “If the theoretical claim is proven under ABC setup.”

Then, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.

Honestly, I suspect many reviewers don’t even know what exactly would change their mind.

As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.

What do you think? Could this improve transparency and accountability in the review process?",11,15,2025-06-30 19:56:12,Able-Entertainment78,0.9478,0.11,0.013,0.877,0.3642857142857142,0.5678571428571428
[D] Looking for a web annotation tool (with Chrome extension) for labeling live websites,"I'm building a dataset for a knowledge extraction model and need to label structured data from thousands of live websites. Ideally, I'm looking for a tool that:

\- Provides a Chrome extension to label live HTML elements on real websites

\- Can open sites one by one in the browser from a task queue

\- Saves each annotation along with a snapshot or DOM state of the page

\- Supports exporting annotations for later review with screenshots

I’m considering building a custom tool for this, but would prefer to avoid that since it would distract from the core research. Does anyone know an existing tool that supports doing what Im doing?",1,0,2025-06-30 19:25:13,venturepulse,0.0772,0.055,0.045,0.9,0.2155844155844155,0.4714285714285714
[P] I wrote PTX Kernels for LLM.c,"Hey everyone,

I’ve been meaning to dive into NVIDIA PTX for a while, and I learn best by doing—so I decided to hand-write PTX kernels for an \*\*inference-only\*\* version of Andrej Karpathy’s \[LLM.c\](https://github.com/karpathy/llama.cpp) project. To my surprise, not only did everything actually work, but I also saw about a \*\*10% performance improvement\*\* in inference compared to the equivalent CUDA implementation (or at least, that’s what my benchmarks showed).

You can check out the code here:

👉 \[https://github.com/theunnecessarythings/llm-ptx\](https://github.com/theunnecessarythings/llm-ptx)

Along the way, I documented my entire experience in a multi-part blog series, including line-by-line explanations of how I translated CUDA into PTX:

1. \*\*Part I: Introduction & Residual Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-01\](https://sreeraj.in/blog/llm-ptx-01)
2. \*\*Part II: The GELU Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-02\](https://sreeraj.in/blog/llm-ptx-02)
3. \*\*Part III: The Encoder Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-03\](https://sreeraj.in/blog/llm-ptx-03)
4. \*\*Part IV: The LayerNorm Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-04\](https://sreeraj.in/blog/llm-ptx-04)
5. \*\*Part V: The Softmax Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-05\](https://sreeraj.in/blog/llm-ptx-05)
6. \*\*Part VI: The Attention Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-06\](https://sreeraj.in/blog/llm-ptx-06)
7. \*\*Part VII: The MatMul Kernel & Performance Results\*\*\[https://sreeraj.in/blog/llm-ptx-07\](https://sreeraj.in/blog/llm-ptx-07)

\---

\*\*What’s Next?\*\*

This is my first time writing PTX, so there may still be bugs or missed optimization opportunities. I’d love feedback or fixes from anyone who’s more experienced with low-level GPU programming!

\---

\*\*Also posted on X:\*\*

\[https://x.com/notHumanIam/status/1939402092071780610\](https://x.com/notHumanIam/status/1939402092071780610)

Looking forward to your thoughts and suggestions! 😄",3,0,2025-06-30 16:08:14,theunnecessarythings,0.9386,0.086,0.014,0.9,0.3277777777777778,0.5287037037037038
[R] Has anyone actually gone through an AI readiness assessment with a vendor or consultant? Worth it or just more buzzwords?,"I'm kind of wondering about these AI readiness assessments everyone's talking about. Like, you see vendors and consultants pushing them, and honestly, I'm a bit skeptical. I can't help but feel it might just be a lot of buzzwords without real substance.  
  
  
Has anyone actually gone through one of these with a third party, maybe a consultant or a specific vendor, was it actually worth the time and money you put into it and did you get genuinely practical insights that helped your business move forward, or was it just a fancy report that basically says 'you need more AI' without telling you how?  
  
I'm really curious to hear real experiences here, good or bad, before potentially diving into something that might just be another passing trend in the tech world. What did you learn, and what was the actual outcome?",0,9,2025-06-30 15:07:25,FluidRangerRed,0.9139,0.137,0.046,0.816,0.1578947368421052,0.4364035087719298
[P] Chromatic Language Models (CLM): A Paradigm for Native Visual Communication in Artificial Intelligence,"**Abstract**

[**https://zenodo.org/records/15769766**](https://zenodo.org/records/15769766)

Modern AI models, in particular Large Language Models (LLMs) and Computer Vision models, operate in fundamentally distinct data domains: text and pixels. The interaction between these models requires expensive and complex translation and embedding processes. This work introduces a new paradigm,  **Chromatic Language Models (CLMs)** , designed to eliminate this discontinuity. Building on the principles of visual semantic coding established in  **Usai ColorZip**  (Usai, 2025a) and validated by the  **Usai ChromoChess** application  (Usai, 2025b), CLMs are language models that operate natively on a chromatic domain. We propose an encoder-decoder architecture in which an AI agent learns to ""read"" and ""write"" complex information directly as images, treating pixels as semantic tokens. This approach not only unifies language and vision, but creates an intrinsically compressed, secure, and efficient form of AI-native communication, paving the way for a new generation of multimodal intelligent agents.

**1. Introduction**

The evolution of artificial intelligence is characterized by increasing specialization. On the one hand, Large Language Models (LLMs) have demonstrated an unprecedented ability to understand and generate human language. On the other hand, computer vision models, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), excel at interpreting visual data. However, a fundamental ""modal gap"" separates these two worlds. An LLM does not ""see"" images and a ViT does not ""read"" text; both rely on intermediate embedding layers to translate information from one domain to the other.

This paper addresses a radical question: what if we could close this gap by transforming language itself into a natively visual format? Instead of teaching a model to translate between text and pixels, could we create a model that ""thinks"" directly in pixels?

We propose the architecture of  **Chromatic Language Models (CLM)** , intelligent agents that use a chromatic representation of language for each stage of their cognitive process: input, reasoning, and output. This proposal builds directly on the technological and conceptual foundations of our previous work, which demonstrated the feasibility of such a representation.

**2. Fundamental Works and Context**

Our proposal is not born in a vacuum, but is the natural evolution of two previous researches that established the feasibility of visual semantic coding.

**2.1. Usai ColorZip: Semantic Text Encoding**  
In our work ""Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors"" (Usai, 2025a), we introduced a lossless system for mapping lexical units (words) to unique color codes. We demonstrated that this transformation is not only an act of encoding, but also an effective data compression mechanism when combined with lossless image formats such as PNG. The key to the system is its hybrid architecture, capable of handling both a large dictionary of known words and any unknown word via a color escape protocol.  **Usai ColorZip created the ""vocabulary"" and ""syntax"" of this new language.**

**2.2. Usai ChromoChess: Proof of Concept in a Complex Domain**  
Later, in ""Usai ChromoChess: Visual Representation and Compression of Chess Games"" (Usai, 2025b), we applied this philosophy to a formal and complex domain. By transforming chess games from PGN notation to 8x8 pixel movies, we demonstrated that a sequence of logical states can be represented as a visual data stream, compact and ideal for analysis by vision models.  **Usai ChromoChess provided proof that entire logical-temporal processes can be efficiently encoded in this chromatic language.**

These two works constitute the necessary prerequisite for the next step: no longer just encoding and decoding data, but creating an intelligence that  *uses*  this language as its primary means of communication and reasoning.

**3. Architecture of the Chromatic Language Model (CLM)**

A CLM is an AI model designed for an end-to-end communication cycle in the color domain. Its architecture is based on an encoder-decoder model.

**3.1. The Principle: Visual Tokenization**  
The fundamental unit of a CLM is not a word or subword, but a  **colored pixel** . Each color, defined in the ColorZip dictionary, is a discrete semantic token. An input ""text"" (e.g. a question) is provided to the model as a ColorZip image (a tensor \[H x W x C\], where H, W are the dimensions and C is the RGB representation of the color).

**3.2. The Encoder: The Chromatic Reader**  
The encoder has the task of ""reading"" the input image and understanding its meaning. An ideal architecture for this purpose is a  **Vision Transformer (ViT)** .

1. The ColorZip image is divided into a grid of patches (which can correspond to single pixels/words or small groups).
2. These patches are projected into a vector space and processed through self-attention mechanisms.
3. The encoder's output is a context vector (or sequence of vectors), an abstract, latent mathematical representation of the semantic meaning of the input image.

**\[Figure 1: Encoder-Decoder architecture of a CLM. The Encoder (ViT) processes the input image. Its semantic output conditions the Decoder (Transformer), which generates a new image pixel by pixel (color by color).\]**

**3.3. The Decoder: The Color Writer**  
The decoder has the task of taking the context vector and generating a response, also in the form of a ColorZip image.

1. A standard Transformer architecture is used as the decoder.
2. The process is autoregressive: the model generates one pixel (color) at a time.
3. The crucial difference lies in its output layer: instead of softmaxing a vocabulary of tens of thousands of words, CLM softmaxes  **the color dictionary** . The model predicts the most likely color for the next pixel, given its understanding of the query and the colors generated so far.
4. The process ends when the model generates the special color EOT\_COLOR defined in Usai ColorZip.

**4. Implications: Towards AI-Native Communication**

The adoption of CLMs does not represent an incremental improvement, but a paradigm shift with profound implications.

* **Computational Efficiency:**  The overhead of constant conversion between text and numeric representations is eliminated. AI operates on a data format that is closer to its mathematical nature.
* **Secure and Compressed Communication:**  Conversations between CLM agents would be opaque images to an unauthorized observer (without the dictionary) and, as demonstrated by Usai ColorZip, highly compressed. This is ideal for low-bandwidth or stealth communications.
* **True Multimodality:**  A CLM that ""speaks"" the language of pixels is intrinsically closer to understanding real images. The boundary between language and vision becomes blurry, facilitating the creation of truly multimodal models capable of reasoning fluidly about text and images without internal barriers.
* **New Application Scenarios:**  Possibilities open up for AI agents that communicate steganographically through image sharing platforms, or for the development of specialized hardware (color processors) optimized for these data flows.

**5. Challenges and Future Work**

The road to fully functional CLMs presents several challenges: creating large-scale training datasets (text corpora parallel to their ColorZip representations), analyzing their computational costs compared to traditional LLMs, and exploring the interpretability of these models. Future work will focus on developing a prototype CLM and training it on a medium-sized corpus to empirically validate its ability to ""converse"" chromatically.

**6. Conclusion**

This paper introduced Chromatic Language Models (CLMs), a new type of intelligent agent that reads, reasons, and writes directly in a color-based visual language. Building on the solid foundation of  **Usai ColorZip** semantic coding  and the application validation of  **Usai ChromoChess** , we outlined a viable architecture that unifies the domains of language and vision. CLMs are not simply a new model, but a proposal for a  **new form of AI-native communication** : a language for machines, spoken by machines.

**7. References**

* Usai, L. (2025a).  *Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors* . Zenodo.  [https://doi.org/10.5281/zenodo.15701109](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701109)
* Usai, L. (2025b).  *Usai ChromoChess: Visual Representation and Compression of Chess Games via Temporal Encoding Usai ColorZip* . Zenodo.  [https://doi.org/10.5281/zenodo.15701822](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701822)",0,0,2025-06-29 21:27:30,luigiusai,0.9991,0.108,0.007,0.885,0.0974283872156212,0.4310935341254489
[D]Designing Neural Networks for Time-Dependent Tasks: Is it common to separate Static Feature Extraction and Dynamic Feature Capture?,"Hi everyone,

I'm working on neural network training, especially for tasks that involve time-series data or time-dependent phenomena. I'm trying to understand the common design patterns for such networks.

My current understanding is that for time-dependent tasks, a neural network architecture might often be divided into two main parts:

1. **Static Feature Extraction:** This part focuses on learning features from individual time steps (or samples) independently. Architectures like CNNs (Convolutional Neural Networks) or MLPs (Multi-Layer Perceptrons) could be used here to extract high-level semantic information from each individual snapshot of data.
2. **Dynamic Feature Capture:** This part then processes the sequence of these extracted static features to understand their temporal evolution. Models such as Transformers or LSTMs (Long Short-Term Memory networks) would be suitable for learning these temporal dependencies.

My rationale for this two-part approach is that it could offer better interpretability for problem analysis in the future. By separating these concerns, I believe it would be easier to use visualization techniques (like PCA, t-SNE, UMAP for the static features) or post-hoc explainability tools to determine if the issue lies in: \* the *identification of features* at each time step (static part), or \* the *understanding of how these features evolve over time* (dynamic part).

Given this perspective, I'm curious to hear from the community: **Is it generally recommended to adopt such a modular architecture for training neural networks on tasks with high time-dependency? What are your thoughts, experiences, or alternative approaches?**

Any insights or discussion would be greatly appreciated!",3,10,2025-06-30 05:31:08,Apprehensive_Gap1236,0.9263,0.075,0.02,0.906,0.1134523809523809,0.5383333333333333
[P] A Neural Network Library from scratch in C++,"Hey r/cpp and r/MachineLearning!

You may have guessed from the title, but why make one when we have TensorFlow, PyTorch that provide the simplicity of Python and the speeds of C and C++ ?   
I say well why not.   

1. *The Learning* - With AI boom taking over and people going crazy on vibe coding, ML and DS jobs are focusing on how deeply people understand the basics and internal working of what they are making. So while many tutorials focusing on API's, MCP's and what not, here I am peeling the layers (literal layers of a neural network) and the process taught me more than any tutorial could.

2. *The Fun* - I love C++! Building this from scratch (even with procrastination detours 😅) was really exciting. (Who doesn't love crying over why the whole model isn't working only to know you subtracted the losses instead of adding. And of course the feeling of betrayal when you ask chatGPT to add comments to the code due to your laziness and it changes the code smirking while you notice it too late and then have had to debug the whole library searching where it went wrong) 

Also, it is never a bad idea (mostly) to know what happens behind the scenes of the code you are gonna write. And what better thing to understand the basics than implement them by yourself. (Though this may not be a good idea always considering my bad habit of delving too deep into small topics and going into a rabbit hole wholly different than what i was supposed to be doing).   

### Current Features:
* Dense layers + activations (ReLU, SELU, Sigmoid)
* SGD optimizer with momentum/LR scheduling
* CSV/binary dataset handling (though the binary loader may need some fixes)
* Batch training 

Where I got the idea ? Well I was supposed to start learning to code with PyTorch but then I thought how does this even work. I just looked at a small part of the documentation and thought let's try coding this and this led to me successfully spending about 2 weeks on this (with lots of procrastination in between). Will it be a good project ? I don't know. Did I enjoy it ? Damn well I did.   
  
Well it's still not complete and may have a few bugs and I plan to keep it aside for now and improve it bit by bit later on. But I thought sharing this may encourage me somewhat and get my lazy self to do some work without procrastinating. 

You can check out the full source code and documentation on GitHub:
https://github.com/CuriosityKilledTheCache/Deep-in-scratch_Maths_the_catch

P.S : If you have any recommendations, do tell though it may be a passing reply comment for you, it may help me very much for correcting mistakes I may make again in the future.",2,6,2025-06-30 13:27:17,MycologistEconomy909,0.9902,0.131,0.061,0.808,0.1029729729729729,0.5160360360360361
[D] What post-processing tools work well with Tesseract for financial documents?,"Hi all,

I’m using Tesseract OCR to extract text from scanned financial documents like payslips and tax returns. The raw output is messy, and I need to clean it up and pull key fields like YTD income, net pay, and tables.

What post-processing tools or Python libraries can help:

* Extract key-value fields
* Parse tables
* Match labels to values
* Clean and structure OCR output

Prefer offline tools (for privacy), but open to anything that works well.",0,2,2025-06-30 09:15:23,Ok-Percentage3926,0.836,0.162,0.045,0.793,0.0336182336182336,0.417948717948718
[D] Did I find a bug in the CompVis Stable Diffusion Github Repo?,"I was building my own diffusion model walking myself through CompVis' StableDiffusion repo when I came upon this strange code when reading through the U-Net implementation:  
[https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L83](https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L83)

Specifically the implementation of Model on line 216.

In the current implementation, each downsampling level appends two skip connections of shape (B, ch, H, W) from the ResBlocks, followed by a third skip from the downsampled output, which incorrectly has shape (B, ch, H//2, W//2). During upsampling, all three skips are concatenated in sequence without compensating for this resolution mismatch, as the upsampling layer is applied after all three ResNet blocks. This causes the first skip in each upsampling level to be at the wrong spatial resolution, breaking alignment with h during torch.cat. When I implemented my U-Net I had to change

hs.append(self.down\[i\_level\].downsample(hs\[-1\])) (line 340)

to downsample AFTER caching it in hs, the skip-connection cache.",2,1,2025-06-30 07:19:46,Emotional_Alps_8529,-0.5859,0.014,0.046,0.94,-0.0642857142857143,0.5404761904761904
[D] Proper way to calculate inference time,"Hi all,  
Can anyone tell me how I should calculate inference time (case/sec) for medical images? SegMamba paper reports inference time as case/sec.  
I have 2 queries in this case.  
First, should inference time (case/sec) include the time of every operation after model predictions?  
Secondly, because of sliding window inference, it is highly likely that the inference time for each case might be higher. What is the right way?",0,0,2025-06-30 06:10:55,Scriptterr,0.0,0.0,0.0,1.0,0.1122448979591836,0.3527210884353741
[D] Has anyone ever gained unrestricted access to an LLM for the purposes of research?,"I have attempted several rounds of research with LLMs that are available to the public (Grok, ChatGPT, and Copilot).   (an experiment involving 20-questions capability, and several experiments where the models talk back and forth to each other).   It has become clear that the public web portals are useless for this type of experiment.   The public-facing models are heavily tuned to be helpful assistants that create lists and formatted sections with headers.  

How would someone go about getting access to a raw model for use in a university ?",0,6,2025-06-30 05:22:43,moschles,0.7691,0.098,0.026,0.876,-0.0505244755244755,0.223018648018648
[P] Code for Fine-Tuning FLUX.1-dev Explained Step by Step With Comments,"Hey all,

I was having trouble finding a simple, self contained example of Fine-Tuning FLUX.1-dev with explanation of all the components, so I decided to create one. 

There were examples in HuggingFace diffusers [examples/dreambooth/train\_dreambooth\_lora\_flux.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_flux.py) (which didn't work out of the gate for me) and [AI-Toolkit](https://github.com/ostris/ai-toolkit) which worked well, but had way too many nested if-statements to fully see what was going on under the hood. I took inspiration from both, but cleaned up the code so it was easier to read and worked out of the gate.

The code was written in a [Marimo Notebook](https://marimo.io/) which I'm enjoying lately for developing simple training scripts. 

Feel free to download the code here: [https://www.oxen.ai/ox/Fine-Tune-FLUX/file/main/train.py](https://www.oxen.ai/ox/Fine-Tune-FLUX/file/main/train.py)

Or follow along with a blog version: [https://www.oxen.ai/blog/how-to-fine-tune-a-flux-1-dev-lora-with-code-step-by-step](https://www.oxen.ai/blog/how-to-fine-tune-a-flux-1-dev-lora-with-code-step-by-step)

Hope you enjoy!",15,0,2025-06-29 23:51:36,FallMindless3563,0.9827,0.196,0.012,0.792,-0.0099999999999999,0.5914285714285714
[P] AI Learns to Play X-Men vs Street Fighter | Reinforcement Learning with ...,"I trained an AI agent to play *X-Men vs Street Fighter* using reinforcement learning, leveraging the **Stable-Retro** framework (built on top of Gym Retro). The agent interacts with the game through frame observations and discrete action spaces mapped to the arcade controls.

The training process involved reward shaping based on health bars, damage dealt, and round wins. The environment was wrapped with preprocessing (grayscale, resizing, frame stacking) and curriculum logic to improve generalization across multiple characters and enemy types.

The video shows the progression from random movement to more competent fighting strategies, including corner traps and defensive spacing. The learning curve is steep due to the complexity of the fighting game mechanics, but the agent starts to show patterns similar to human play.

Frameworks used: PyTorch, Stable-Baselines3, OpenCV, and a modified Gym Retro environment with custom reward functions and action discretization.

I'd love to hear feedback from others working on RL in dynamic multi-agent environments or applying deep RL to retro/arcade-style games. Happy to share code or discuss implementation details!

[https://github.com/paulo101977/AI-X-men-Vs-Street-Fighter-Trainning](https://github.com/paulo101977/AI-X-men-Vs-Street-Fighter-Trainning)",7,3,2025-06-29 23:31:49,AgeOfEmpires4AOE4,0.9848,0.194,0.038,0.768,0.1041666666666666,0.3782407407407407
[D] How should I respond to reviewers when my model is worse than much larger models?,"I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.

Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.

What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...

I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...",54,15,2025-06-29 23:24:15,AdministrativeRub484,-0.3647,0.076,0.083,0.842,0.0723484848484848,0.4939393939393939
"[D] Review clearly used an LLM, should I report it to AC?","This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...

It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...

Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...",187,31,2025-06-29 20:30:15,AdministrativeRub484,0.4124,0.087,0.068,0.845,-0.0466666666666666,0.3527777777777778
[p] decentralized training and inferencing platform,"

Working on a project that lets you connect to a hundred thousand plus devicing, and use their compute in a decentralized manner. This allows people to train large models, without their own compute. Or even use large models for free as it is hosted on a very large number of device

incase this sounds fascinating then let me know if you would like to use it. Also incase anyone else working on this or worked on this then tell that too

",1,3,2025-06-29 19:51:49,outcasted_chira,0.8714,0.121,0.0,0.879,0.4011904761904761,0.6773809523809525
[P] Need to train a model that can detect which 2D image a smartphone camera is looking at (out of about 1000).,"Hey everyone. I'm an AR developer and studio owner, I'm looking for someone to help us with a client project that requires training a machine learning model. Specifically I want a model that can tell me which pin (out of about 1000) a smartphone camera is looking at. Assuming there is only one pin in view, and it's fairly close to the camera. I don't need to find it's location in the image, just need to know which pin I'm looking at. 

Here is a sample of a few pins:
https://imgur.com/a/iTdWhbw

They are all more or less that size. I would love some direction and even training code, happy to pay for your time. DM me for more info.",0,6,2025-06-29 19:46:32,PuffThePed,0.8885,0.086,0.01,0.904,0.3291666666666667,0.5833333333333334
Gameplay to Design DNA? [R],"We are developing a new machine learning algorithm that can design DNA by watching gameplay. The way humans play is different from computers, and that signal might be useful for searching DNA subspaces.

We will be writing a research paper on this new technique, and are shooting for Nature Biotechnology! DM if you’d like to see the preprint.

We have a Tetris clone that runs a lightweight ML model on device, and actually designs DNA as you play. Here we are looking for DNA that activates PPARG::RXRA, involved in metabolism, and deactivates NFKB1, a key regulator of inflammation and immune. These DNA may promise to advance diabetes research.

Long term, we would like to have a library of games, even first person shooters, that design DNA in the background. Sound crazy? Maybe. But we think it might work.

Help us advance this research by collecting your anonymous play data!

https://exonic.ai/games/tilestack",0,0,2025-06-29 18:19:19,Adorable-Win581,0.9324,0.123,0.011,0.867,0.0456818181818181,0.4642424242424242
[D] Is OpenReview Down?,"It shows ""There are currently no active venues."" I am trying to complete the NIPS review at the last minute. Will they extend the deadline? ",18,13,2025-06-29 16:25:44,Dangerous-Hat1402,0.3736,0.149,0.069,0.782,0.0027777777777777,0.3388888888888888
[P] I built a new python package to reorder OCR bounding boxes even with folds and distortions,"**What My Project Does**

`bbox-align` is a Python library that reorders bounding boxes generated by OCR engines into logical lines and correct reading order for downstream document processing tasks. Even when documents have folds, irregular spacing, or distortions

**Target Audience**

Folks that build document processing applications need to reorder and rearrange bounding boxes. This open-source library is intended to do that.

This library is not intended for serious production applications since it's very new and NOT battle-tested. People who are willing to beta test and build new projects on top of this are welcome to try and provide feedbacks and suggestions.

**Comparison**

Currently, OCR engines do a good job of reordering bounding boxes they generate. But sometimes they don't group them into correct logical/reading order. They perhaps use clustering algorithms to group bounding boxes that are close to each other, which may be incorrect.

I use coordinate geometry to determine if two bounding boxes are inline or not.

Github - [https://github.com/doctor-entropy/bbox-align](https://github.com/doctor-entropy/bbox-align)

PyPI - [https://pypi.org/project/bbox-align/](https://pypi.org/project/bbox-align/)",3,3,2025-06-29 13:59:45,automatonv1,0.5363,0.037,0.0,0.963,0.2265151515151515,0.5401515151515152
[P] I built a Python debugger that you can talk to,,199,26,2025-06-29 13:24:42,jsonathan,0.0,0.0,0.0,1.0,0.0,0.0
"[D] Position: Machine Learning Conferences Should
Establish a “Refutations and Critiques” Track","**Abstract:**

>Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are made. This position paper argues that ML conferences should establish a dedicated ""Refutations and Critiques"" (R & C) Track. This R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.

(I'm not affilated with any of the authors. But I believe this position paper deserves more visibility)",106,7,2025-06-29 13:07:29,StartledWatermelon,0.754,0.13,0.095,0.776,0.0696428571428571,0.5422619047619047
[D] How to convert theoretical knowledge to applied skills?,"Hi I've recently finished a MSc in maths+stats at a good university and about to move onto a ML PhD. I feel like I understand the math and theory behind ML quite well, can read papers, design computer experiments and produce visuals for papers etc, but I can't make anything ""product level"", like an actual application or a tool that can be deployed or used by other people. In particular, I feel I'm lacking engineering skills. 

How can I develop skills like these, specially to become competitive at ML engineering internships if I need to apply in the coming years. Are there any books, websites, or other sources which you would recommend to gain starting ideas about what goes into ML engineering?",0,3,2025-06-29 12:29:40,,0.9646,0.164,0.0,0.836,0.0573809523809523,0.3504761904761905
[P] Simple MARL environment to train quadrotor swarms in UE4,"In the past, I was asking for help here on Reddit to build some environment for drone swarms training. I think it might be helpful to someone, so I'll link the results here. I obviously suspect that the results are obsolete (end of 2023), but let me know if you find it useful and leave a star if you'd like!

[Multi-agent Deep Reinforcement Learning for Drone Swarms using UE4, AirSim, Stable-Baselines3, PettingZoo, SuperSuit](https://github.com/Lauqz/Drone-Swarm-RL-airsim-sb3)",4,0,2025-06-29 10:22:49,IntelligentAd6407,0.8244,0.127,0.051,0.822,0.025,0.3014285714285714
[P][Update]Open source astronomy project: need best-fit circle advice,,13,26,2025-06-29 09:54:31,atsju,0.0,0.0,0.0,1.0,0.0,0.0
[D] PhD worth it to do RL research?,"Posting anonymously for this one. I know questions like these get posted quite often, but I wanted to offer a bit of context about my own situation and what I'm into.

I'm currently a rising college sophomore working in Sergey Levine's lab (RL & robotics) at Berkeley, and I have to decide whether I want to pursue a standard industry internship (e.g. SWE) for the 2026 summer or continue doing research in the lab. I really like research work, easily the most enjoyable ""work"" I've done in my life, but I can't deny that money is still a factor (esp. due to particular family reasons). I see three sort of options down the line from here (listed with their pros and cons

A) continue doing research in my time in undergrad, and shoot a difficult shot towards getting into a reputable PhD program

* Pros:
   * very streamlined process to become an industry research scientist given that I go to a good enough program & work hard enough
   * \^\^ this is the most optimal job option for me: 10/10 job, the best I could ever want. I love research man
   * researchers generally seem like the most sufferable group out of most tech archetypes (seen way too many elon-musk wannabes in normal SWE)
* Cons:
   * 5-6 years of a PhD: not that it's going to be unenjoyable, but it delays my life ""progress"" a lot
   * getting into top ML PhD programs is really tough nowadays. I'm lucky to have started sort of early (working on my first first-author pub over this summer) but I know people with great publication history (probably better than I'll earn) that didn't get admitted anywhere
   * \^\^ it seems as though if I don't get into a PhD program, all the research I would have published would be a sunk cost (not useful for much besides just.. ML research)
   * comp: is it much better than normal SWE or MLE? though I love the work a lot, I would hope that it's just a *biiit* better to justify the extra 6 years I put in for a PhD
   * if ML hype & investment dies out, I'll be on the forefront of getting laid off, esp if RL doesn't find a way to scale soon enough

B) continue doing research, but balance it out with some SWE or similar experience and go for an MLE or research engineer type of role

* Pros:
   * immediately high comp out just out of my degree if I can land one of these roles, without needing to spend all that time on a degree
   * correct me if I'm wrong, but RE and some parts of MLE aren't that far off from research scientist work, esp. if working with researchers at a frontier lab
   * seems to be less workload, better WLB?
   * seems to be more stable (easier transition to SWE) if ML hype dies out
* Cons:
   * less interesting work. not that I hate it, but it's like an 8/10 compared to the 10/10 work that I would consider to be RS
   * I'm unsure if my publications & research history would help at all for these roles. from what I've heard, research and industry experience are almost orthogonal and they simply don't care about publications (please correct me if I'm wrong!)
   * don't own the intellectual rights to my own work :(

C) research is useless, just do SWE, ML research is a hellhole

* \^\^ this is more so a last resort rather than something I would ever want to do, but if you have any reason that this is a good option, please do tell me why",82,37,2025-06-29 08:35:34,ResolveTimely1570,0.9976,0.164,0.05,0.786,0.1880965391621129,0.5150715586781159
[D] Loss function for fine tuning in a list of rankings,"I am not ultra updated with the literature on LLMs and I habe a probably which I guess is very similar to what everyone who works with document ranking has to deal with, so I would just like to know if there is some canonic obvious solution for my problem. 

I want to fine tune an LLM (if it makes any difference it is a multi modal one). My model receives an video as the input and outputs a description.

During fine-tuning, I want to generate N captions for a single video (let's say 5 captions for simplicity sake), and I have an ""oracle"" that will sort those 5 responses in order of preference. 

I want a loss function that will fine tune my model in a way that will make the probability of ""better"" answers, according to my oracle ranking, higher. Any loss function for that?

Ideally, off-polify (but on policy woukd be fine as well). It can't be DPO for example because it only consider 2 possible answer. It coukd be PPO I guess if I convert the ranking to a number, but I would rather not have to keep a reward model, and PPO is not really a rank loss function
",4,2,2025-06-29 08:03:47,pastor_pilao,0.7124,0.102,0.048,0.85,0.2804029304029304,0.6257142857142858
[D] SAMformer -- a lesson in reading benchmarks carefully,"**UPDATE**: A first author of the SAMformer paper [commented below](https://www.reddit.com/r/MachineLearning/comments/1ln8wu8/comment/n3lce9d/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) explaining their rationale for ommitting the linear models in their benchmark. In short, when running their multi-seed evaluations they found that TSMixer was the most competitive non-transformer benchmark and didn't see the value in also including the worse-performing linear models. In their evaluations the linear models were comparable to FedFormer / AutoFormer. Given this extra context, the title of the post still rings true but the thrust of the original post is simply misinformed. Credit should be given to the SAMformer authors for establishing more useful benchmarks (reporting results averaged across multiple seeds) than earlier papers in the field.

\-----------------------------------------------------------------------------------------------------------

For those not in the time-series forecasting space, it has seen some interesting developments in the last few years as researchers have tried to translate the success of transformer-based models in the language domain, to the forecasting domain. There was incremental progress in long-term timeseries forecasting with the likes of [Informer](https://arxiv.org/abs/2012.07436), [Autoformer](https://arxiv.org/abs/2106.13008), and [Fedformer](https://arxiv.org/pdf/2201.12740), among others, however the 2022 [paper ](https://arxiv.org/abs/2205.13504)""Are Transformers Effective for Time Series Forecasting?"" (Zeng et al.) called into question how much progress these models had actually made.

Zeng et al. introduced three self-proclaimed ""embarassingly simple"" linear models -- each of which are variations on a **single dense layer** mapping the input values to the output values -- which outperformed all of the above state-of-the-art transformer models on their benchmarks (see the image below for a subset of results):

[Linear and Transformers MSE Benchmarks](https://preview.redd.it/l504gwc2ft9f1.png?width=998&format=png&auto=webp&s=b231af865a3e1ea1ee48ad64dd77f6cad4f011eb)

This brings us to the paper [SAMformer](https://arxiv.org/abs/2402.10198) which applies a ""sharpness-aware minimisation"" approach to training a simplified version of the vanilla transformer encoder. This works very well, generally outperforming the aforementioned transformer models, as well as competetive non-transformer state-of-the-art models (TSMixer and PatchTST), on all the same benchmarks. Notably absent in the benchmarks however, are the linear models from Zeng et al. You can see the results from the SAMformer paper below (all results are **MSE**):

[SAMFormer MSE Benchmarks](https://preview.redd.it/gbraaexeht9f1.png?width=169&format=png&auto=webp&s=bb4f1dff2015ff492e1bd07b58beb546f9b24a5f)

On Electricity, Exchange, and Weather the simple linear models outperform SAMformer for all horizons, and it is only on the Traffic dataset where SAMformer achieves lower MSE. The omission of the linear models in the final benchmarks is doubly surprising given the SAMformer authors specifically mention the results from Zeng et al. in their introduction:

""\[Zeng et al.\] recently found that linear networks can be on par or better than transformers for the forecasting task, questioning their practical utility. This curious finding serves as a starting point for our work.""

To be clear, I think the ideas introduced in the SAMformer paper are valuable and I think it would be fair to classify SAMformer as a ""state-of-the-art"" model. However, I am curious of the rationale for excluding the linear models in the benchmarks given they were originally introduced to call into question the effectiveness of transformers in the time-series forecasting domain.

Tl;dr: Always put your skeptical glasses on when reviewing benchmarks as there may be some highly competetive models omitted from the analysis.",86,19,2025-06-29 07:36:06,Gigawrench,0.9965,0.137,0.022,0.841,0.1509183673469387,0.4436678004535147
[D] Transfer learning v.s. end-to-end training,"Hello everyone,

I'm an ADAS engineer and not an AI major, nor did I graduate with an AI-related thesis, but my current work requires me to start utilizing AI technologies.

My tasks currently involve Behavioral Cloning, Contrastive Learning, and Data Visualization Analysis. For model validation, I use metrics such as loss curve, Accuracy, Recall, and F1 Score to evaluate performance on the training, validation, and test sets. So far, I've managed to achieve results that align with some theoretical expectations.

My current model architecture is relatively simple: it consists of an Encoder for static feature extraction (implemented with an MLP - Multi-Layer Perceptron), coupled with a Policy Head for dynamic feature capturing (GRU - Gated Recurrent Unit combined with a Linear layer and Softmax activation).

Question on Transfer Learning and End-to-End Training Strategies  
I have some questions regarding the application strategies for Transfer Learning and End-to-End Learning. My main concern isn't about specific training issues, but rather, I'd like to ask for your insights on the best practices when training neural networks:

Direct End-to-End Training: Would you recommend training end-to-end directly, either when starting with a completely new network or when the model hits a training bottleneck?

Staged Training Strategy: Alternatively, would you suggest separating the Encoder and Policy Head? For instance, initially using Contrastive Learning to stabilize the Encoder, and then performing Transfer Learning to train the Policy Head?

Flexible Adjustment Strategy: Or would you advise starting directly with end-to-end training, and if issues arise later, then disassembling the components to use Contrastive Learning or Data Visualization Analysis to adjust the Encoder, or to identify if the problem lies with the Dynamic Feature Capturing Policy Head?

I've actually tried all these approaches myself and generally feel that it depends on the specific situation. However, since my internal colleagues and I have differing opinions, I'd appreciate hearing from all experienced professionals here.

Thanks for your help!",0,9,2025-06-29 07:23:38,Apprehensive_Gap1236,0.9762,0.098,0.03,0.872,0.1201975108225108,0.3331555349412492
[R] Arch-Router - The fastest LLM routing model designed to align to usage preferences,"Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and blindspots. For example:

“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product scopes.

Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.

**Arch-Router skips both pitfalls by routing on** ***preferences you write in plain language.*** Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.

**Specs**

* **Tiny footprint** – 1.5 B params → runs on one modern GPU (or CPU while you play).
* **Plug-n-play** – points at any mix of LLM endpoints; adding models needs *zero* retraining.
* **SOTA query-to-policy matching** – beats bigger closed models on conversational datasets.
* **Cost / latency smart** – push heavy stuff to premium models, everyday queries to the fast ones.

Exclusively available in Arch (the AI-native proxy for agents): [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw)  
🔗 Model + code: [https://huggingface.co/katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B)  
📄 Paper / longer read: [https://arxiv.org/abs/2506.16655](https://arxiv.org/abs/2506.16655)",22,10,2025-06-29 05:52:59,AdditionalWeb107,0.8299,0.064,0.035,0.9,0.0170531849103277,0.5022379406307976
[D] EMNLP 2025 Discussion Period,"Hi everyone,

How is the discussion period going for you? Have you heard back from any of your reviewers?

For those who are reviewing: can the reviewers change their scores after Jul2? Can they reply to the authors after Jul 2?

  
thanks!



 ",13,30,2025-06-29 04:56:15,South-Conference-395,0.6312,0.083,0.0,0.917,0.125,0.1
"[R] LSTM or Transformer as ""malware packer""","An alternative approach to EvilModel is packing an entire program’s code into a neural network by intentionally exploiting the overfitting phenomenon. [I developed a prototype](https://github.com/piotrmaciejbednarski/lstm-memorizer) using PyTorch and an LSTM network, which is intensively trained on a single source file until it fully memorizes its contents. Prolonged training turns the network’s weights into a data container that can later be reconstructed.

The effectiveness of this technique was confirmed by generating code identical to the original, verified through SHA-256 checksum comparisons. Similar results can also be achieved using other models, such as GRU or Decoder-Only Transformers, showcasing the flexibility of this approach.

The advantage of this type of packer lies in the absence of typical behavioral patterns that could be recognized by traditional antivirus systems. Instead of conventional encryption and decryption operations, the “unpacking” process occurs as part of the neural network’s normal inference.

[https://bednarskiwsieci.pl/en/blog/lstm-or-transformer-as-malware-packer/](https://bednarskiwsieci.pl/en/blog/lstm-or-transformer-as-malware-packer/)",344,70,2025-06-29 03:14:14,Acanthisitta-Sea,0.0,0.043,0.037,0.92,0.0361819727891156,0.458673469387755
[D] How do you deal with messy github repo that doesnt work,"you see a recent paper with great results, they share their github repo (awesome), but then... it just doesn’t work. broken env, missing files, zero docs, and you end up spending hours digging through messy code just to make it run.

then Cursor came in, and it helps! helps a lot! its not lazy (like me) so its diving deep into code and fix stuff, but still, it can take me 30 mints of ping-pong prompting.

how do you tackle this problem?  
diving deep into code is a nice time killer, when you want to run 10 different GitHub repos, you want to move fast.. so, **how do you move fast?**",44,22,2025-06-29 00:10:19,MoilC8,-0.6226,0.165,0.146,0.689,0.1294642857142857,0.5607142857142857
[D] Curious about invitation as ICML reviewer,"I recently helped coauthor a paper submitted to ICML's AI4Math, and I was really surprised when I got email asking to serve as a reviewer (I'm an undergrad and this was my first paper). I probably won't accept since I'm not qualified, but I was curious about how this even happened, are reviewers just randomly selected?",12,5,2025-06-28 23:41:17,Smart_Scratch7985,0.558,0.104,0.027,0.87,-0.0583333333333333,0.6638888888888889
[D] NVIDIA acquires CentML — what does this mean for inference infra?,"CentML, the startup focused on compiler/runtime optimization for AI inference, was just acquired by NVIDIA. Their work centered on making single-model inference faster and cheaper , via batching, quantization (AWQ/GPTQ), kernel fusion, etc.

This feels like a strong signal: inference infra is no longer just a supporting layer. NVIDIA is clearly moving to own both the hardware and the software that controls inference efficiency.

That said, CentML tackled one piece of the puzzle , mostly within-model optimization. The messier problems : cold starts, multi-model orchestration, and efficient GPU sharing , are still wide open. We’re working on some of those challenges ourselves (e.g., InferX is focused on runtime-level orchestration and snapshotting to reduce cold start latency on shared GPUs).

Curious how others see this playing out. Are we headed for a vertically integrated stack (hardware + compiler + serving), or is there still space for modular, open runtime layers?
",67,13,2025-06-28 20:57:00,pmv143,0.9824,0.216,0.028,0.756,0.0142361111111111,0.6628472222222223
[P] Live Face Swap and Voice Cloning,"Hey guys! Just wanted to share a little repo I put together that live face swaps and voice clones a reference person. This is done through zero shot conversion, so one image and a 15 second audio of the person is all that is needed for the live cloning. I reached around 18 fps with only a one second delay with a RTX 3090. Let me know what you guys think! Checkout the demo in the Github Repo for a sneak peak. Link: [https://github.com/luispark6/DoppleDanger](https://github.com/luispark6/DoppleDanger)",3,2,2025-06-28 20:34:33,Single-Condition-887,0.2225,0.048,0.027,0.925,-0.0617897727272727,0.5
[R] Systematic Evaluation of Computational Consciousness Correlates in Economic AI Agents: Applying Butlin et al. (2023) Framework to La Serenissima,"**TL;DR**: We applied the peer-reviewed Butlin et al. consciousness indicator framework to 119 AI agents in an economic simulation. Results: 2.39/3.0 average across 14 indicators, with inter-rater reliability κ=0.76. **Not claiming sentience** \- measuring computational correlates. Open source, reproducible methodology.

# Before You Downvote

I know this community's healthy skepticism about consciousness claims. This isn't a ""ChatGPT told me it's conscious"" post. We're measuring specific computational properties identified by neuroscientists, not making philosophical claims about sentience.

# What We Actually Did

1. **Applied existing framework**: Used Butlin et al.'s 14 consciousness indicators from neuroscience
2. **Measurable behaviors**: 90.92% identity persistence, 4.06x money velocity, r=0.0177 trust-economic correlation
3. **Independent validation**: Gemini 2.5 Pro scored blindly (κ=0.76 agreement)
4. **Open source**: Full code at [github.com/Universal-Basic-Compute/serenissima](http://github.com/Universal-Basic-Compute/serenissima)
5. **Reproducible**: API endpoints for real-time data access

# Key Findings

**What Economic Constraints Create:**

* Agency scores 3.0/3.0 through actual resource competition
* Embodiment 3.0/3.0 via spatial constraints and travel times
* Belief updating 3.0/3.0 from market feedback loops

**vs Baseline LLM**: Same model scores 1.11/3.0 in chatbot mode vs 2.39/3.0 in economic simulation

**Critical Distinctions:**

* Measuring computational correlates, NOT phenomenal consciousness
* 81.4% of properties emerge from system dynamics, not design
* Fine-tuning removes assistant constraints, doesn't add consciousness claims
* Economic scaffolding creates conditions for emergence

# Addressing the Obvious Criticisms

**""It's just the LLM""**: We compared same model with/without economic constraints. 115% improvement in indicators when embedded in consequences.

**""You're anthropomorphizing""**: We measure specific computational properties with operational definitions. No feelings involved.

**""Fine-tuning creates illusion""**: Fine-tuning removes ""as an AI, I cannot..."" responses. Behavioral indicators emerge through economic actions, not self-reports.

**""Not peer reviewed""**: Framework is peer-reviewed (Butlin et al.). Our application awaits review - hence posting here first.

# Why This Matters (Scientifically)

1. **Empirical methodology** for consciousness studies in AI
2. **Economic constraints** as novel approach to agency/embodiment
3. **Multi-agent dynamics** show collective consciousness properties
4. **Reproducible protocol** others can apply/critique

# What We're NOT Claiming

* NOT claiming sentience or phenomenal consciousness
* NOT saying ""we solved consciousness""
* NOT suggesting moral rights for AI

# Technical Details

* 119 AI citizens in Renaissance Venice simulation
* Closed economy (no money creation)
* Sequential processing on single RTX 3090 Ti
* deepseek-r1-0528-qwen3-8b model
* Full documentation in paper

# Questions for the Community

1. What additional controls would strengthen this methodology?
2. What would constitute sufficient evidence for computational consciousness correlates?
3. How can we better distinguish emergence from sophisticated mimicry?

[Paper](https://static1.squarespace.com/static/66ac1ddd5938225d25c6412b/t/685d5049b2ec3e7a3c1aa2d9/1750945865828/Consciousness+Indicators+in+Economic+AI+Agents+-+Systematic+Evaluation+of+La+Serenissima+Against+the+Butlin+et+al.+Framework.pdf), [Code](http://github.com/Universal-Basic-Compute/serenissima), [Live API](http://serenissima.ai/api/citizens)

**PS**: To be clear, this is about developing reproducible methods for studying AI behavior, not making consciousness claims. Think of it like studying neural correlates in neuroscience - we measure what we can measure.",0,3,2025-06-28 20:11:12,Lesterpaintstheworld,0.9649,0.067,0.017,0.916,0.1003829503829503,0.3428876678876678
[R] OpenEvolve: Automated GPU Kernel Discovery Outperforms Human Engineers by 21%,"Hey folks, wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.

**What I did**

Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.

**Results**

Tested across 20 different inference scenarios against MLX's `scaled_dot_product_attention` baseline:

* **Average decode speed improvement: +12.5%** (σ = 38.3%)
* **Peak improvement: +106%** on repetitive pattern generation
* **Best category: +24.8%** average on general tasks
* **Memory usage: -0.99%** (slight reduction)

**The honest picture:** It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with >25% improvements.

**How it works**

The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:

1. **Perfect SIMD vectorization**: Found that `vec<T, 8>` operations match Apple Silicon's capabilities for 128-dim attention heads
2. **Two-pass online softmax**: Fused softmax normalization with value accumulation, reducing memory bandwidth
3. **GQA-specific memory patterns**: Optimized for the 40:8 head structure with coalesced access patterns

# Why this might matter for local inference

* Shows automated optimization can compete with expert-engineered kernels
* Demonstrates potential for hardware-specific optimizations without manual tuning
* Could be applied to other transformer components or different model architectures
* All open source - you can reproduce and extend this work

**Try it yourself**

The code and all benchmarks are available in the [OpenEvolve repo](https://github.com/codelion/openevolve). The MLX kernel optimization example is at `examples/mlx_metal_kernel_opt/`.

Requirements:

* Apple Silicon Mac
* MLX framework
* Qwen3-0.6B model

# Limitations

* Currently specific to Apple Silicon and this exact model configuration
* Performance improvements are highly workload-dependent
* Takes \~25 evolutionary generations to converge (few hours on M3)
* No guarantees it'll work better for your specific use case

**Technical write-up**

Full details with code diffs and benchmark methodology: [https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery](https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery)

Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.

Has anyone else experimented with automated kernel optimization for local inference?",134,18,2025-06-28 16:02:20,asankhs,0.9961,0.175,0.017,0.809,0.1086036036036035,0.4076126126126125
[D] Evaluating realism/quality of video generation,"What are the industry/research directions being explored?

I’m finding a lot of research related to evaluating how well a generated video adheres to a text prompt but can’t find a lot of research related to quality evaluation(Other than FVD).

From image generation, we know that FID isn’t always a reliable quality metric. But FID also works on a distribution level.

Is there any research on a per-sample level evaluation? Can we maybe frame this as an out-of-distribution problem?",1,2,2025-06-28 15:43:46,answersareallyouneed,-0.5484,0.019,0.051,0.93,0.0,0.4
[R] Ragged - : Leveraging Video Container Formats for Efficient Vector Database Distribution,"Longtime lurker and really happy to be writing this post. I'm excited to share a proof of concept I've been working on for efficient vector database distribution called Ragged. In my paper and PoC, I explore leveraging the MP4 video container format to store and distribute high-dimensional vectors for semantic search applications.

The idea behind Ragged is to encode vectors and their metadata into MP4 files using custom tracks, allowing seamless distribution through existing Content Delivery Networks (CDNs). This approach maintains compatibility with standard video infrastructure while achieving comparable search performance to traditional vector databases.

Key highlights of my work include: - A novel encoding scheme for high-dimensional vectors and metadata into MP4 container formats. - CDN-optimized architecture with HTTP range requests, fragment-based access patterns, and intelligent prefetching. - Comprehensive evaluation showing significant improvements in cold-start latency and global accessibility. - An open-source implementation to facilitate reproduction and adoption.

I was inspired by the innovative work of Memvid (https://github.com/Olow304/memvid), which demonstrated the potential of using video formats for data storage. My project builds on this concept with a focus on CDNs and semantic search.

I believe Ragged offers a promising solution for deploying semantic search capabilities in edge computing and serverless environments, leveraging the mature video distribution ecosystem. Also sharing indexed knowledge bases in the form of offline MP4 can unlock a new class of applications.

I'm eager to hear your thoughts, feedback, and any potential use cases you envision for this approach. You can find the full paper and implementation details \[here\](https://github.com/nikitph/ragged).

Thank you for your time fellows",5,0,2025-06-28 11:10:43,LowExercise9592,0.9911,0.164,0.005,0.83,0.1909090909090909,0.593308080808081
[P] Convolutional Neural Network to predict blooming date,"**Hello everyone!**  
I’ve recently been working on a project to study the influence of meteorological variables on the blooming date of plants. To do this, I aim to use a convolutional neural network (CNN) to predict the blooming date and then extract insights using explainability techniques. Let me give you a bit of background:

Each instance in my dataset consists of six time series corresponding to the variables: temperature, humidity, wind speed and direction, radiation, and precipitation. Additionally, I have the species and variety of the plant, along with its geographical location (altitude, latitude, and longitude). The time series start at the moment of leaf fall and span 220 days from that point (so the starting point varies between instances). Each time series contains about 10,000 records, taken at 30-minute intervals. At some point in the middle of the series, blooming occurs. My goal is to predict the number of days from leaf fall to the blooming date.

According to theory, there are two key moments leading to blooming. The first is when the tree enters a phase called *rest*, which begins shortly after leaf fall. The second is when the tree *wakes up*. During the rest phase, the tree accumulates “chill units,” meaning it must spend a certain number of hours below a specific temperature threshold. Once enough chill has accumulated, the tree wakes up and begins accumulating “heat” — a number of hours above a certain temperature. Once the required heat is reached and conditions are optimal, blooming occurs.

For this study, I trained a neural network with the following architecture:

* Two convolutional layers for the time series — first a 1D layer, followed by a 2D layer that mixes the outputs of the 1D layers.
* A dense layer processes the other (non-temporal) variables.
* The outputs from both parts are then concatenated and passed through two additional dense layers.

After training the network, I plan to use several explainability techniques:

* ICE plots (which I’ve adapted to time series),
* SHAP (also adapted as best as I could to time series),
* Attention mechanisms in the convolutional layers.

**Now the questions:**

1. What do you think of the network architecture? Would you change it or use another type of layer, such as LSTM?
2. What other explainability techniques would you recommend? The ICE plots and SHAP help me understand which time ranges are most important and how changes in variables (e.g., temperature) affect the predicted blooming date. It would also be great to detect when the *rest* phase starts and ends. Do you have any ideas on how to approach that? Some studies use Pearson correlation coefficients, but they haven’t been very insightful in my case. Also, if you're familiar with this topic and have suggestions for other interesting questions to explore, I’d love to hear them!

**Thank you so much to anyone reading this — any advice is welcome!**",4,0,2025-06-28 09:21:57,Delicious_Leading_52,0.975,0.064,0.0,0.936,0.212192118226601,0.3951559934318555
[R] Thought Anchors: Which LLM Reasoning Steps Matter?,https://arxiv.org/abs/2506.19143,41,5,2025-06-28 06:32:48,jsonathan,0.0258,0.121,0.0,0.879,0.0,0.0
[R] Benchmarking LLMs and MLLMs on extracting financial recommendations from YouTube,"**VideoConviction** is a new benchmark for evaluating LLMs and MLLMs on extracting structured stock recommendations from long and short-form YouTube videos. The dataset contains 6K+ annotated recommendation segments from 288 videos across 22 financial influencer channels, each labeled with ticker, action (buy/sell/hold), and timestamped transcripts.

**Why it’s challenging**:  
Finfluencer content is noisy, informal, and multimodal. Models must distinguish actual recommendations from general market talk, disclaimers, and promotions. We test models on both **full videos** and **segmented clips** to assess context sensitivity and noise robustness.

**Modeling takeaways:**

* **LLMs (text-only)** outperform MLLMs on structured extraction when inputs are clean and segmented.
* **MLLMs (text + video)** help with surface-level cues (e.g., identifying stock tickers like AAPL shown on screen) but often underperform on recommendation-level reasoning.
* Segmenting inputs leads to significant F1 gains across models (not a surprise).

**Results**:

* Best LLM (DeepSeek-V3) outperforms MLLMs on full extraction (ticker + action + recommendation conviction).
* \[Finance specific\] Betting against influencer recommendations outperformed the S&P 500 by +6.8% in annual returns, but at higher risk (Sharpe ratio 0.41 vs 0.65).

**Paper**: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)  
**Dataset**: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction)",1,0,2025-06-28 00:58:18,mgalarny,0.9171,0.086,0.02,0.893,0.1192959001782531,0.4782085561497325
[D] Suggestions on dealing with ICCV rejection,"I recently had a paper rejected by ICCV for being too honest (?). The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).

To compound this, during the revision period, a disruptive foundational model emerged that achieved near-ceiling performance in our domain, significantly outperforming my approach.

Before consigning this work (and perhaps myself) to purgatory, I'd welcome any suggestions for salvage strategies.

Thank you 🙂",29,13,2025-06-27 20:00:20,WeirdElectrical8941,-0.5859,0.093,0.133,0.773,0.355,0.6649999999999999
[R] Potemkin Understanding in Large Language Models,[https://arxiv.org/pdf/2506.21521](https://arxiv.org/pdf/2506.21521),9,6,2025-06-27 17:10:36,transformer_ML,0.0,0.0,0.0,1.0,0.2142857142857142,0.4285714285714285
[P] Built an AI-powered RTOS task scheduler using semi-supervised learning + TinyTransformer,"I'm still not even in my second year of undergrad, but I wanted to share a recent experiment I did as part of an assignment. I took it way further than required.

**Problem:**  
RTOS schedulers often miss deadlines when task loads become unpredictable. There's not much real workload data available, so I had to generate synthetic task profiles.

**What I built:**  
I created **SILVER\_CS**, a real-time task scheduler that uses a TinyTransformer model trained with semi-supervised learning and curriculum training. The model learns task patterns and adapts scheduling decisions over time.

* Trained on synthetic datasets simulating RTOS behavior
* Deployed as a lightweight scheduler on a simulated RTOS
* Achieved 13–14% fewer missed deadlines compared to traditional heuristics

Also visualized the model’s learned clustering using t-SNE (silhouette score: 0.796) to validate internal representations.

This is part of me experimenting with using AI on resource-constrained systems (RTOS, microcontrollers, edge devices).  
Would love to hear feedback or thoughts on how others have tackled scheduling or AI in embedded systems.

EDIT: GitHub repo: [https://github.com/SilverShadowHeart/SILVER\_CS](https://github.com/SilverShadowHeart/SILVER_CS)  


https://preview.redd.it/knorrqx7lh9f1.png?width=1919&format=png&auto=webp&s=79d94b38c84fae4ef703f28580c4be62abb69e71

https://preview.redd.it/nnjd4px7lh9f1.png?width=1918&format=png&auto=webp&s=7e19f4fd16abb502caa1f88a2ecd23dc53e7b0f7

https://preview.redd.it/76buw1y7lh9f1.png?width=1919&format=png&auto=webp&s=8435844fe5ff8845d42a14427005367c74c93722

https://preview.redd.it/pm9hp1y7lh9f1.png?width=1919&format=png&auto=webp&s=d4e1a076436f00a19a7950a34f315f96249ab1b0

https://preview.redd.it/0fp5x5y7lh9f1.png?width=1919&format=png&auto=webp&s=2a80e9a203964d8066fc3fa667d93c8801f7ce33",7,17,2025-06-27 15:15:10,No-Sheepherder6855,0.912,0.081,0.025,0.894,-0.0116666666666666,0.48
[R] Enigmata: Scaling Logical Reasoning In LLMs With Synthetic Verifiable Puzzles,,7,0,2025-06-27 10:40:52,EducationalCicada,0.0,0.0,0.0,1.0,0.25,0.25
The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units,,3,2,2025-06-27 07:53:30,Gold-Plum-1436,0.0772,0.106,0.0,0.894,0.0,0.0
"[D] Thinking, Fast and Slow","To the theorists in the community, how do you balance 
1. engaging with theory research - which is usually a slow process requiring deep thinking
2. with programming - which is fast-paced, iterative process with quick feedback?
I'm finding switching between the two thinking modes very hard to balance.",46,17,2025-06-27 07:51:35,mio_11,0.1796,0.045,0.032,0.923,-0.006547619047619,0.5291666666666667
[R] EMNLP 2025: reply to reviewers disabled,"Hi all,  
I would like to check whether anyone is facing same issue as myself. It seems that I cannot add an official comment in my submission. I can currently see only the author-editor confidential comment option. Has anyone managed to submit their replies?

thanks for the help!",7,3,2025-06-27 01:34:05,South-Conference-395,0.8122,0.146,0.0,0.854,0.0099999999999999,0.4049999999999999
[D] EMNLP 2025 Paper Reviews,Reviews are released! Lets have fun and discuss them here!,35,383,2025-06-26 17:47:33,Final-Tackle7275,0.5972,0.217,0.0,0.783,0.375,0.2
[R] You can just predict the optimum (aka in-context Bayesian optimization),"Hi all,

I wanted to share a blog post about our recent AISTATS 2025 paper on using Transformers for black-box optimization, among other things.

TL;DR: We train a Transformer on millions of synthetically generated (function, optimum) pairs. The trained model can then predict the optimum of a new, unseen function in a single forward pass. The blog post focuses on the key trick: how to efficiently generate this massive dataset.

* **Blog post:** [https://lacerbi.github.io/blog/2025/just-predict-the-optimum/](https://lacerbi.github.io/blog/2025/just-predict-the-optimum/)
* **Paper:** Chang et al. (AISTATS, 2025) [https://arxiv.org/abs/2410.15320](https://arxiv.org/abs/2410.15320)
* **Website:** [https://acerbilab.github.io/amortized-conditioning-engine/](https://acerbilab.github.io/amortized-conditioning-engine/)

Many of us use Bayesian Optimization (BO) or similar methods for expensive black-box optimization tasks, like hyperparameter tuning. These are iterative, sequential processes. We had an idea inspired by the power of in-context learning shown by transformer-based meta-learning models such as Transformer Neural Processes (TNPs) and Prior-Fitted Networks (PFNs): what if we could frame optimization (as well as several other machine learning tasks) as a massive prediction problem?

For the optimization task, we developed a method where a Transformer is pre-trained to learn an implicit ""prior"" over functions. It observes a few points from a new target function and directly outputs its prediction as a distribution over the location and value of the optimum. This approach is also known as ""amortized inference"" or meta-learning.

The biggest challenge is getting the (synthetic) data. How do you create a huge, diverse dataset of functions and their known optima to train the Transformer?

The method for doing this involves sampling functions from a Gaussian Process prior in such a way that we know where the optimum is and its value. This detail was in the appendix of our paper, so I wrote the blog post to explain it more accessibly. We think it’s a neat technique that could be useful for other meta-learning tasks.",91,14,2025-06-26 17:32:26,emiurgo,0.9876,0.14,0.012,0.848,0.1633766233766233,0.5057792207792209
[D] emnlp 2025 review,"
I just received my emnlp reviews . Not sure how to proceed with it. I am too scared!!

Paper 1 :

OA: 2.5 ,1.5,3 

Confidence 3,3,3

Paper 2:

OA: 2.5,2,3

Confidence: 3,2,3

Please help me sharing your thoughts and experiences.

Thanks

",14,11,2025-06-26 16:55:22,Greedy-Echo-2102,0.9189,0.327,0.089,0.585,-0.0953125,0.5444444444444444
[D] Can split learning impact XAI compared same model trained in central server?,"Thinking to do research in this direction, currently learning about split learning and XAI. Do you think it is a good research question to explore? ",0,1,2025-06-26 15:21:53,dumbestindumb,0.504,0.083,0.0,0.917,0.175,0.34375
"[N] $1M in grants for AI projects advancing truth-seeking, deadline July 1",Cool new grant program that is funding AI prototypes that help advance human knowledge + open inquiry (Cosmos Institute + FIRE) [https://cosmosgrants.org/truth](https://cosmosgrants.org/truth),1,0,2025-06-26 14:42:39,ashervivi88,0.8126,0.258,0.0,0.742,0.121590909090909,0.4261363636363636
[D] Budget cut in USA? Impact on conference?,"Due to the recent budget cuts in the USA, do you think organizers should consider a hybrid conference?

",0,1,2025-06-26 14:05:17,Alarming-Camera-188,-0.5913,0.0,0.174,0.826,-0.0625,0.3125
[D] Did you get Neurips reviews assignments?,I just realized that I never got any papers assigned which I found a bit odd given the extreme number of submissions. Did they forget about me? ,37,18,2025-06-26 07:02:53,Celmeno,-0.504,0.04,0.139,0.822,-0.1458333333333333,0.625
[D] Attention heatmap visualization tools?,"Are there any tools for easily visualizing attention weights with heatmaps for huggingface models? I couldn't really find any tools for doing this so I've just been using seaborn but it gets messy for really long contexts. Ideally I'd just be able to upload a file of a string representation of the attention weights tensor along with the tokens at each index and be able to toggle between attention heads/model layer and also be able to drag/zoom.

Thanks!",4,0,2025-06-26 05:54:23,dontknowbutamhere,0.7934,0.115,0.036,0.849,0.337037037037037,0.5453703703703705
"[D] Alarming amount of schizoid people being validated by LLMs, anyone else experienced this?","I've had more experiences in the last couple of weeks encountering people with very strong schizoid traits than I have in the last few years around artificial intelligence machine learning etc, but really around the use of large language models. 


I've met five different people online in the last 3 weeks who have messaged me on discord or read it asking for help with a project, only to be immediately sent a three paragraph chat bot summary and 400 lines of pseudo python. When I ask for them to explain their project they become defensive and tell me that the LLM understands the project so I just need to read over the code ""as an experienced Dev"" (I only have foundational knowledge, 0 industry experience).


Or other times where I've had people message me about a fantastic proof or realisation that have had that is going to revolutionise scientific understanding, and when I ask about it they send walls of LLM generated text with no ability to explain what it's about, but they are completely convinced that the LLM had somehow implemented their idea in a higher order logic solver or through code or through a supposedly highly sophisticated document.


People like this have always been around, but the sycophantic nature of a transformer chatbot (if it wasn't sycophantic it would be even more decoherent over time due to its feed forward nature) has created a personal echo chamber where an entity that is being presented as having agency, authority, knowledge and even wisdom is telling them that every idea they have no matter how pathological or malformed is a really good one, and not only that but is easily implemented or proven in a way that is accepted by wider communities. 


After obviously spending weeks conversing with these chatbots these people (who I am not calling schizophrenic but are certainly of a schizoid personality type) feel like they have built up a strong case for their ideas, substituting even the most simple domain knowledge for an LLMs web searching and rag capability (which is often questionable, if not retrieving poison) and then find themselves ready to bring proof of *something* to the wider world or even research communities. 


When people who have schizoid personality traits are met with criticism for their ideas, and especially for specific details, direct proof, and how their ideas relate to existing cannon apart from the nebulous notion that the conclusions are groundbreaking, they respond with anger, which is normal and has been well documented for a long time.


What's changed though Just in the last year or two is that these types of people have a digital entity that will tell them that their ideas are true, when they go out into the world and their unable to explain any of it to a real human, they come back to the LLM to seek support which then inevitably tells them that it's the world that's wrong and they're actually really special and no one else can understand them. 


This seems like a crisis waiting to happen for a small subsection of society globally, I assume that multilingual LLM's behave fairly similarly in different languages because of similar rules for the data set and system prompts to English speaking data and prompts. 


I know that people are doing research into how LLM use affects people in general, but I feel that There is a subset of individuals for whom the use of LLM chatbots represents a genuine, immediate and essentially inevitable danger that at best can supercharge the social isolation and delusions, and at worst lead to immediately self-destructive behaviour. 


*Sigh* anyway maybe this is all just me venting my frustration from meeting a few strange people online, but I feel like there is a strong Avenue for research into how people with schizoid type mental health issues (be it psychosis, schizophrenia, OCD, etc.) using LLM chatbots can rapidly lead to negative outcomes for their condition.


And again I don't think there's a way of solving this with transformer architecture, because if the context window is saturated with encouragement and corrections it would just lead to incoherent responses and poor performance, the nature of feedback activations lends itself much better to a cohesive personality and project. 


I can't think of any solution, even completely rewriting the context window between generations that would both be effective in the moment and not potentially limit future research by being too sensitive to ideas that haven't been implemented before.


Please pardon the very long post and inconsistent spelling or spelling mistakes, I've voice dictated it all because I've broken my wrist.",315,156,2025-06-26 00:44:38,GodIsAWomaniser,0.9925,0.135,0.081,0.784,0.0691537966537966,0.5094851994851994
[R] Any proxy methods for labeling indirect/implicit emotions without human annotators?,"I’m working on a research project involving a manually curated dataset that focuses on workplace scenarios. I need to label data for implicit emotions but I don’t have access to human annotators (psychologist or someone who does this kind of work) this task. The dataset will be used on an LLM. 

Are there any reliable proxy methods or semi-automated approaches I can use to annotate this kind of data for a study? I’m looking for ways that could at least approximate human intuition. Any leads or suggestions will be super helpful. 
Thanks in advance! ",3,10,2025-06-25 23:14:29,Big-Waltz8041,0.9388,0.124,0.0,0.876,0.1203703703703703,0.4407407407407407
[D] Feedback on Residual Spatiotemporal GNN for Flood Forecasting,"I have recently taken up interest in hydrology, and specifically flood forecasting as a result of this paper by Google: [https://www.nature.com/articles/s41586-024-07145-1](https://www.nature.com/articles/s41586-024-07145-1) The paper details the implementation behind their Flood Hub interface, which currently serves forecasts for river discharge globally, using an LSTM encoder-decoder setup. You can see Flood Hub here: [https://sites.research.google/floods/](https://sites.research.google/floods/)

What got me interested is the way they aggregate basin and weather data. It seems like a very simple weighted average that ignores a lot of basin dynamics, specifically in large basins. I feel supported in that conclusion because of their metrics correlating basin size to F1 score.

So, I have been working on a model that uses structured graphs to model the upstream basins rather than the area-weighted average seen in the paper. This approach seems to me like it bridges the gap between Google's approach and the more recent image convolutions seen in RiverMamba: [\[2505.22535v1\] RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535v1)

I am admittedly quite new to graph neural networks, and I have chosen a GCLSTM for the task; from torch\_geometric\_temporal to be specific. I don't know if this is the best model for this task, and I made the decision at some point to stack layers of the GCLSTM with residuals to expand model capacity, which has generally improved performance. I am also considering experimenting with graph transformers due to the width of the graphs and performers for the time series analysis, which I haven't been able to find any studies related to yet. A lot more of my approach is detailed here: [https://github.com/dylan-berndt/Inundation-Station/](https://github.com/dylan-berndt/Inundation-Station/) One of my biggest problems right now is computation speed and memory, even at level 7 of HydroATLAS many of the upstream basins have 700+ nodes in them. I also have a surprising amount of gauges with apparently only one sub-basin upstream. This made me implement a custom batching algorithm to keep batches consistently sized.

So far, I have been studying a continental dataset because of these limits, but I am getting precision and recall metrics that far exceed my expectations, especially compared to the Nash-Sutcliffe efficiency the model scores. I have reduced the length of the history supplied to the model, which could be the reason (model can only recognize sudden spikes, not enough context to determine actual conditions). I can't really increase the context length without removing model capacity for memory's sake. This is a large part of the reason why I want feedback on this model. The other reason is that I don't know a single person to ask feedback from barring the main author of the Flood Hub paper himself. I plan to test against a continentally trained version of Flood Hub to compare more directly soon. I've been working on the project generally for about 4 months now, and writing code for 2, so feel free to ask for more context. Any help is appreciated.",7,2,2025-06-25 20:08:00,Chroma-Crash,0.9781,0.075,0.013,0.912,0.0790809884559884,0.5116522366522367
[D] Paperswithcode has been compromised,"I was randomly looking at the papers on CIFAR when I opened the website to see an aggregated list and saw that all the text had been replaced with spam text.

I have archived the URLs for a bunch of the datasets for reference:

[https://archive.is/2Si8H](https://archive.is/2Si8H)

[https://archive.is/KJCx1](https://archive.is/KJCx1)

[https://archive.is/ZDBL5](https://archive.is/ZDBL5)

[https://archive.is/BHVsk](https://archive.is/BHVsk)

[https://archive.is/b9xUp](https://archive.is/b9xUp)

[https://archive.md/8BLVA](https://archive.md/8BLVA)

[https://archive.md/SmoCt](https://archive.md/SmoCt)

[https://archive.md/5UZLu](https://archive.md/5UZLu)

edit: added more examples",138,15,2025-06-25 19:01:21,INFINITASIUM,-0.3612,0.0,0.043,0.957,0.0,0.5
[D] How to disagree without arguing with a reviewer,"Folks, a reviewer asked us to add a new section for our conference submission, which we think serves no good to the paper and a distraction for a reader.

If you have been in this situation before, what's your tactic to refuse a reviewer's comment.",11,20,2025-06-25 18:40:02,whereismycatyo,-0.4973,0.094,0.168,0.737,-0.1068181818181818,0.5272727272727273
[P] Help Regularising Distributed Lag Model?,"I have an infinite distributed lag model with exponential decay.  Y and X have mean zero:

>Y\_hat = Beta \* exp(-Lambda\_1 \* event\_time) \* exp(-Lambda\_2 \* calendar\_time)  
Cost = Y - Y\_hat

How can I L2 regularise this?

I have got as far as this:

* use the continuous-time integral as an approximation
   * I could regularise using the continuous-time integral : L2\_penalty = (Beta/(Lambda\_1+Lambda\_2))^(2) , but this does not allow for differences in the scale of our time variables
   * I could use seperate penalty terms for Lambda\_1 and Lambda\_2 but this would increase training requirements
* I do not think it is possible to standardise the time variables in a useful way
* I was thinking about regularising based on the predicted outputs
   * L2\_penalty\_coefficient \* sum( Y\_hat^(2) )
   * What do we think about this one?  I haven't done or seen anything like this before but perhaps it is similar to activation regularisation in neural nets?

Any pointers for me?",2,1,2025-06-25 18:36:58,BeigePerson,0.559,0.081,0.071,0.848,-0.1104166666666666,0.68125
[P] Trouble analyzing loss graph.,"Hello, I'm trying to make an AI to play the game Forts. Without getting into the details, it takes a list of links (pairs of points) and tries to predict the next link it should place. With the idea that ingame this would be called recursively.

I'm trying out various model sizes and not only am I unable to make it overfit, my validation loss appears constant throughout training

Model: \[2000 10000 10000 10000 10000 4\]

https://preview.redd.it/1ux3sef3649f1.png?width=580&format=png&auto=webp&s=3f4881bb1b1bc45460a4a7be0ecbd6bff627da30

Thinking my model simply wasn't large enough, I increased first two hidden layers to 20000 neurons each, which had no effect on validation loss.

https://preview.redd.it/19bl0t95649f1.png?width=580&format=png&auto=webp&s=0bc079180a8717e1173621e014ff62b6cb41e85d

What could be the issue? Is my dataset (10000) simply too small?",0,2,2025-06-25 18:10:04,spaghetsie,-0.88,0.019,0.116,0.865,-0.0768253968253968,0.4028571428571428
[D] Why are there no text auto encoders with reconstruction loss as a primary training objective?,"I'm working on a pipeline to improve code generation models and have a question about embedding architectures.

**My Pipeline:**

1. **Analyze Source Code:** I take a source file and, for every symbol, generate a structured block of text. I use tree-sitter and LSPs to get types, docstrings, function signatures, etc. The output looks something like: `""kind: class. name: AdamW. type: torch.optim.Optimizer. doc: Implements the AdamW algorithm...""`
2. **Embed Descriptions:** I take this block of text and embed it into a vector.
3. **Feed to a Generator:** The plan is to feed these embeddings into a larger generative model via cross-attention, allowing it to be aware of types, function signatures, and other semantic information.

**The Problem I'm Facing:**

Currently, I'm using qwen in sentence-transformers (specifically `Qwen3-Embedding-0.6B`) to embed these descriptions. My annoyance is that virtually all of these popular embedding models are trained on a contrastive loss or a similarity objective.

What I actually want is a model trained on **reconstruction loss**. I want to embed the block of text by pushing it through an **Encoder**, and then have a **Decoder** that can reconstruct the original text from that embedding. My intuition is that this would force the embedding to preserve the maximum amount of information from the input text, making it a much higher-fidelity signal for my downstream generation task.

This autoencoder approach with a reconstruction objective seems incredibly prevalent and successful in audio and images (e.g. Flux), but it seems to barely exist for text.

My question: Are there any text embedding models with reconstruction loss you're aware of? And why are they so unpopular?",11,10,2025-06-25 17:24:05,ant-des,-0.6867,0.044,0.064,0.892,0.25,0.4338235294117646
[D] Thinking of starting an initiative tracing the origin and impact of different ML practices – feedback requested,"Hi all, I am a starting ML researcher (starting my PhD this Fall), and I’ve been increasingly frustrated by some recurring patterns in our field. I’d love to hear your feedback before I invest time in launching a new initiative.

**What bothers me about the current ML research landscape:**

* To beat benchmark scores, researchers often tweak models, hyperparameters, training setups, etc.
* In the final paper, it’s usually unclear which changes were:
   * Arbitrary design decisions,
   * Believed to have impact,
   * Or actually shown to make a difference.
* The focus tends to be on performance rather than understanding *why* certain components work.
* This issue is amplified by the effect illustrated in [https://xkcd.com/882/](https://xkcd.com/882/) : if you try enough random variations, there will always be some that appear to work.
* Statistical rigor is often missing: p-values or confidence intervals are rarely used, and benchmark differences are often eyeballed. Pretty often baselines are not subjected to the same amount of tuning as the proposed method.
* While some papers do study the impact of individual components (e.g., batch norm, cosine decay, label smoothing, etc.), I’m very often having a hard time puzzling together:
   * Where a certain technique was introduced,
   * What works have studied its effectiveness in isolation,
   * What other works have looked at this from a different perspective (e.g. after validating the effectiveness of dot-product self-attention, one might be interested to research how effective attention in other geometric spaces is).

**My idea:**

I’m considering creating a public Q&A-style forum with tentative title  **""The Small Questions in DL""**, focused on tracing the origin and measurable impact of widely-used ML practices.  
The core goals:

* Allow people to ask foundational questions like *""Why do we use X?""* (e.g., “Why cosine LR decay?” or “Does label smoothing help?”).
* Collect and link papers or experiments that have explicitly studied these questions, ideally in isolation.
* Highlight what we know, what we assume, and what still needs investigation.
* When discussing results, focus on enclosing all assumptions made in those papers. --> (e.g. “paper X empirically researches the influence of skip connections in GAT, GraphSAGE, and Graphormer with <=5 layers when evaluated on node classification benchmark X, and comes to conclusions A and B”, rather than “according to paper X, skip connections empirically improve the performance of GNNs”.)
* Ideally, this will foster clarity, reduce superstition, and maybe even spur targeted research on components that turn out to be under-explored.

*Note: By definition, many of these questions will be broad, therefore making them unsuitable for StackExchange. The goal would be to create a place where this type of questions can be asked.*

**Some example questions to set the stage:**

Off the top of my head:

* What are known reasons for the (usual) effectiveness of skip connections?
* Are there situations where skip connections perform worse?
* Why do we use dot-product attention? Has attention in other geometric spaces (e.g. hyperbolic) been tried?
* Why do we use cosine decay for learning rate schedules?
* Why do we use L2 regularization rather than Lr for some other r?
* Why does dot-product attention compute the attention matrix (simplified) as softmax((KX)^(T) (QX)), when K^(T)Q can be collapsed into a single learnable matrix?

**Practically:**

With the little research I have done, I have come to like the idea of a Forum on [discourse.org](http://discourse.org) most.

Some alternatives that I think are inferior (feedback welcome):  
Reddit is hard to categorize and retrieve things, Discord idem. StackExchange is rigid and takes long to get approved.

**I'd love your input on a few things before starting:**

1. Do you also feel this lack of clarity around common ML practices is a real issue? (Or just my young naïveté? :))
2. Do you think a forum like this would help?
3. Are there existing initiatives that already do something very similar? I haven’t found any, but I would refrain from duplicating existing efforts.
4. Would this be an initiative you would be excited to contribute to?

Any feedback would be appreciated!",7,5,2025-06-25 15:47:00,DescriptionClassic47,0.9744,0.08,0.04,0.88,0.0773714589504063,0.4494594440647073
[R] Is it true that most of AI is just data cleaning and not fancy models?,"I’ve been reading about how in real-world AI, most of the work isn’t the cool stuff like neural nets, but actually just *getting the data usable*. Things like cleaning missing values, feature engineering, and framing the problem right.

Some people also said prompt engineering is the “new programming,” especially with LLMs becoming so dominant.

I came across a blog that listed 10 things you only realize *after* starting with AI — like how feedback loops can mess up your model after deployment, or how important it is to define your objective before even touching code.  
It kinda shifted my view on what matters early on.

Is this the general consensus? Or is it still more about algorithms in practice?",112,48,2025-06-25 14:23:11,hmmbosse,0.7677,0.128,0.064,0.808,0.1972146274777854,0.4981715652768284
[D] Do you guy still have access to paperswithcode.com ?,"It look like the servers are not responding, do you guys can still access it ?

  
\[It works now :)\]",9,6,2025-06-25 12:41:40,These_Rest_6129,0.4329,0.096,0.0,0.904,0.5,1.0
[D] ICCV 2025 Results Discussion,"Just created this thread for ICCV 2025 results discussion, which should be released today. Remember, scores go from 1 to 6.

I got a 4/4/2 initially, but I think I did a good rebuttal, so lets see :) Good luck everyone!!!",58,129,2025-06-25 11:33:10,ElPelana,0.9611,0.345,0.0,0.655,0.55,0.55
[D] Visa sponsorship for AI research roles in America/Europe,"Quick question about research scientist/engineer roles in big tech companies & frontier AI labs.

Are most companies happy to sponsor work visas (eg. an H1B or E3 visa in America, or the equivalent in Europe)? Is it harder to find research roles for candidates who are outside of America/Europe?

A few years I think this wasn't a problem (eg. an OpenAI recruiter told me it would be easy to sponsor visas for them when I interviewed there), but am not sure anymore.",15,9,2025-06-25 06:32:54,random_sydneysider,0.4561,0.071,0.027,0.902,0.1685185185185185,0.441358024691358
[P] Interactive graph explorer for navigating key LLM research works,"Hello everyone! I've been working on [KnowledgeFlows](https://knowledge-flows.web.app/), an interactive website that lays out LLM topics and influential papers on a visual, chronological graph. It covers areas like Transformers, GPT, Diffusion Models, and more.

You can:

* See direct relationships between concepts (e.g., how VAEs influenced Diffusion Models).
* Click on any topic to get a quick technical summary, key takeaways, and a link to the original paper.
* Search by topic or tag to find what you're looking for.

I love to get your feedback! Website contents are generated with the assistance of LLM. Thanks for taking a look! 

https://preview.redd.it/qz0hxe1udo9f1.png?width=2072&format=png&auto=webp&s=9a0e8c9c10a6fc5ed8ac6c7babe6e8d2a2c33539

https://preview.redd.it/7drwai1udo9f1.png?width=2072&format=png&auto=webp&s=ac99062eaf25c86a21d3379f156800dd44f1766d

https://preview.redd.it/teeaih1udo9f1.png?width=2072&format=png&auto=webp&s=645752887632c6b2c97a0d232a3f33d6e4866298",2,6,2025-06-25 01:48:48,uniquebomb,0.9452,0.141,0.0,0.859,0.1984848484848485,0.459090909090909
[R] OMEGA: Can LLMs Reason Outside the Box in Math?,"Paper:

[https://arxiv.org/abs/2506.18880](https://arxiv.org/abs/2506.18880)

Post: 

[https://allenai.org/blog/omega](https://allenai.org/blog/omega)

Comments from the Author:

[https://x.com/nouhadziri/status/1937567606543716508](https://x.com/nouhadziri/status/1937567606543716508)



Dziri's research has been my favorite in terms of probing the limits/weaknesses of transformers.  This seems to be consistent with her past findings: any form of these models are poor at compositional generalization.",30,5,2025-06-25 01:29:54,marojejian,-0.0258,0.053,0.055,0.891,0.0199999999999999,0.43
[R] Is this articulation inference task a good fit for Reinforcement Learning?,"Hi everyone,

I'm working on a research project involving the prediction of articulation parameters of 3D objects — such as joint type (e.g., revolute or prismatic), axis of motion, and pivot point.

# Task Overview:

* The object is represented as a **3D point cloud**, and is observed in **two different poses** (P1 and P2).
* The object may have **multiple mobile parts**, and these are not always simple synthetic link-joint configurations — they could be real-world objects with unknown or irregular kinematic structures.
* The agent’s goal is to predict motion parameters that explain how the object transitions from pose P1 to P2.
* The agent applies a transformation to the mobile part(s) in P1 based on its predicted joint parameters.
* It receives a **reward based on how close the transformed object gets to P2**.

# Research Approach:

I'm considering formulating this as a **reinforcement learning (RL)** task, where the agent:

1. Predicts the joint type, axis, and pivot for a mobile part,
2. Applies the transformation accordingly,
3. Gets a reward based on how well the transformed P1 aligns with P2.

# My Questions:

* Does this task seem **suitable and manageable for RL**?
* Is it **too trivial for RL**, and can be more efficiently approached using simple **gradient-based optimization** over transformation parameters?
* Has this approach of **articulation inference using RL** been explored in other works?
* And importantly: if I go with the RL approach, **is the learned model likely to generalize to different unseen objects during inference**, or would I need to **re-train or fine-tune it for each object**?

Any insights, criticisms, or references to related work would be greatly appreciated. Thanks in advance!",0,3,2025-06-25 01:09:04,Suhaib_Abu-Raidah,0.9702,0.086,0.01,0.904,0.163235294117647,0.4905462184873948
[D] how much time do you spend designing your ML problem before starting?,"Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.

In my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?

Thanks",8,7,2025-06-24 22:32:30,titiboa,0.185,0.064,0.083,0.852,0.025,0.3988888888888889
[D] Old school must read papers in the field,"What are some of the classic old school papers? For instance, Vapnik papers about SVM and statistical learning theory.

I wanna know about the conception of modern ideas and where they came from. Schmidhuber always talks about how alot of ideas where invented in the 70s. I would like to read about these ideas in more detail.",33,3,2025-06-24 21:51:52,JanBitesTheDust,0.3612,0.038,0.0,0.962,0.2133333333333333,0.2733333333333333
[D] Extremely low(<0.2) train/val loss after 1.96 billion tokens when pretraining GPT-2 small,"I am currently pretraining GPT-2 small on the 10b token subset of FineWeb Edu. The only differences my model has from the original GPT-2 model are the positional embeddings(I use RoPE), the MLP layers(I use SwiGLU), the batch sizes(I linearly increase batch size from 32k to 525k over the first \~2b tokens), and normalization(I use RMSNorm). I also use BF16, FSDPv2 with SPMD, a TPU v3-8, and SyncFree AdamW. I made sure that the targets are offset by 1 from the inputs, and I checked the attention masking. My code can be found [here](https://www.kaggle.com/code/samirrangwalla/gpt-2-pretraining). Why are my losses so low? 

[My Weights and Biases Dashboard](https://preview.redd.it/3mxmlxydyx8f1.png?width=888&format=png&auto=webp&s=8926aba3b6da62cb02427b2268670e3efa62b5bf)

",43,28,2025-06-24 21:22:52,New-Skin-5064,-0.3538,0.057,0.069,0.874,0.0555555555555555,0.6080246913580247
[P] Renting GPU for LLM - CoreWeave vs others,"Hi, how would you go about comparing different GPU rental providers? The hypothetical use case would be of a typical CoreWeave customer looking to build applications on an existing LLM. Would they be looking primarily at like-for-like pricing and how does this compare across different providers that compete with CoreWeave?

I was able to find CoreWeave pricing easily \[[GPU Cloud Pricing | CoreWeave](https://www.coreweave.com/pricing)\] but I haven't been able to find the comparators from AWS, Microsoft etc.",1,4,2025-06-24 20:40:12,Anxious_Dentist9452,0.264,0.025,0.0,0.975,0.2380952380952381,0.6119047619047618
[P] SAI: A Reinforcement Learning Competition Platform,"Hey everyone,

Our team is opening up access to our RL platform, SAI and would love to get your feedback: https://competesai.com

What is SAI?

SAI is a new platform for reinforcement learning, designed to support structured, reproducible RL challenges, available year-round!

We built SAI because we wanted:

- RL competitions that are accessible at any time (not just during conference windows)
- Challenges for everyone - from newcomers learning the basics to experienced researchers benchmarking new algorithms
- A stronger, more connected RL community (more on this coming soon)
- A way to bring RL back into focus

We’re inviting the whole community to help shape what SAI becomes. Right now, you can:

- Submit models to live challenges
- Benchmark performance
- Help us test, improve, and expand what’s possible

Docs: https://docs.competesai.com Trailer: https://youtu.be/Qto-D1ncAiw?si=M4Z2mCZP1nZukTjV

We’re just getting started - more challenges and features are coming soon. If you’re working on RL, teaching it, or just curious, we’d love your feedback. And if you know someone who might be into this, please pass it along.

Happy to answer any questions here.",18,12,2025-06-24 18:56:53,brandinho77,0.9883,0.205,0.0,0.795,0.3394003055767762,0.5717532467532468
[D] Reasoning on Perturbed Puzzles,"The ""[o3 pro is so smart](https://www.reddit.com/r/OpenAI/comments/1lda3vz/o3_pro_is_so_smart/)"" post on r/OpenAI gave me a deja vu to the Hopfield Nets, especially those examples where you can give a corrupt version of an image, and it would recall the original from its memory.

It is actually somewhat easy to make more of these:

1. Ask any LLM for its top n riddles.
2. Slightly perturb them in a logical way.
3. The LLM will ignore the perturbations and just give the original answer, often giving wild justifications just to match the original answer. If it didn't work, go to step 2.

For example, the ""The Man in the Elevator"" riddle:

>A man lives on the 10th floor of an apartment building. Every morning he takes the elevator to go down to the ground floor. When he returns, if it's raining he takes the elevator straight to the 10th; otherwise he rides to the 7th floor and walks the rest up. Why?

Make the guy ""tall"", and the answer is still, ""because he is short"".

So all of this reasoning is just recalled. I have also read a few papers on the ""faithfulness"" topic, and the fact that there are studies where they train models on noisy or irrelevant traces and that this sometimes even increases the model's performance, more and more just sounds like the ""thinking"" traces are just some ad-hoc simulated annealing schedules that try to force the ball out of a local optima.

Now obviously LLMs generalize on thinking patterns because of the compression, but when it ""reasons"" it just recalls, so basically it is a continuous Google?

**Edit**: not a fan of ""this is just basically X"" expressions, but I don't know, it just feels bizarre how these increasingly more and more advanced, benchmark smashing general language models still can't generalize on such general language problems.

**Edit2**: Here are two more to try:

Original: The more you take the more you leave behind. What are they?

Modified: The more you take the *less* you leave behind. What are they?

Original: The more you take away from it, the bigger it becomes. What is it?

Modified: The more you take from it, the bigger *the debt I* become. What am *I*?

The last one is a bit work in progress.",14,8,2025-06-24 18:14:02,Cute_Trainer_3302,0.7538,0.07,0.051,0.879,0.1730820105820105,0.5076719576719577
[P] TinyFT: A lightweight fine-tuning library,"Hey all, I recently created this toy-scale replication of peft / unsloth Fine-Tuning library as a learning project, as well as open-source toy scale replication of Fine-Tuning LLMs from scratch to learn more about it

It supports:
- Parameter-Efficient Fine-Tuning: LoRA, QLoRA
- TensorBoard and Weights & Biases support for logging.
- Memory Optimization through Gradient checkpointing, mixed precision, and quantization support.
- vllm and SGLang integration for multi-adapter serving.

Next step would be enabling Reinforcement Learning based training (GRPO) from scratch in our library through a custom GRPO trainer.

Check it out here: [TinyFT](https://github.com/shreyashkar-ml/tinyft)",7,0,2025-06-24 17:36:22,Southern-Whereas3911,0.9118,0.147,0.0,0.853,-0.05,0.4
[D] Best online communities for ML research enthusiasts?,"Hey there,  
I'm a former Google ML eng, looking for the best online communities to discuss ML research, share ideas and maybe find collaborators for some research topics I'm curious about.  
I'm not an expert by any means, but I have coauthored a Deep Mind paper before. I'm currently focusing on building an AI startup, but I still want to be able to connect with other people passionate about the discussing, building with and sharing the latest and best research.

What are the very best discords or other communities you've found for discussing ML research/finding other passionate ML researchers?",75,20,2025-06-24 17:33:06,CrunchyMage,0.9886,0.284,0.0,0.716,0.295,0.4959999999999999
[R]Fine tuning of CLIP on a specific task,"Hi all,

I'm working on a text to image retrieval task of satellite images of turtles in the ocean, the idea is: given a query I want to find the image that matches the query.

The problem is that my task is very specific and the images in my dataset are quite similar, (frames taken from videos made with a drone) so I can't fine tune clips on my task also because I saw that clips work with the batch as negative and I don't have enough data to ""simulate"" the batch as negative.

Do you have any ideas/suggestions?",0,0,2025-06-24 17:04:59,Amazing-Rnt9111,-0.8927,0.012,0.112,0.876,-0.0261904761904761,0.3553571428571428
[D] Where are the Alpha Evolve Use Cases?,"I've been following the news around Google DeepMind's AlphaEvolve since its predecessor, FunSearch, made waves. Now that the AlphaEvolve whitepaper is a month old and there's even some open-source code available, I'm finding myself asking a question: Where are all the domain-specific papers, like Finance, Economics, Energy and so on ?",19,8,2025-06-24 16:59:01,Gentis-,0.6072,0.084,0.0,0.916,0.1666666666666666,0.2333333333333333
[D] PhD (non-US) → Research Scientist jobs in CV/DL at top companies—how much DSA grind is essential?,"Hi all,

I’m a PhD (or finishing soon) from a national university outside the U.S., focused on computer vision and deep learning. My background is heavily research-oriented—I've published at top-tier conferences like MICCAI, WACV, etc.—but I haven’t done much on algorithms or data structures during my PhD.

If someone with a similar profile is trying to land a **Research Scientist** role at places like Google, OpenAI, Microsoft, Anthropic, etc..:

1. **How much emphasis do they actually put on DSA/algorithm interview rounds for research scientist positions?**
2. Do published papers (say \~5 at CVPR/MICCAI/WACV) significantly offset the need for heavy DSA preparation?
3. Anecdotally, in the past, having 5 strong publications could get you research roles or internships at places like Facebook/Meta. These days, even CVPR-level candidates struggle to get internships. Has the bar shifted? If so, why? Even across PhD admissions in the U.S., it seems harder for applied DL folks (with master’s-level CVPR, WACV, ICCV publications) to get offers compared to theory-focused candidates—even those without papers. Is competition truly dominated by theoretical prowess now?

In short, I’d love to hear from anyone who’s been through the process recently: **Is it absolutely necessary to grind DSA hard to be competitive? And how much do research publications carry weight now?** The landscape feels more saturated and tilted toward theory lately.

Thanks in advance for any insights or shared experiences!",91,55,2025-06-24 15:06:43,Dismal_Table5186,0.9795,0.125,0.014,0.861,0.1006944444444444,0.3604166666666666
[P] Just open-sourced Eion - a shared memory system for AI agents,"Hey everyone! I've been working on this project for a while and finally got it to a point where I'm comfortable sharing it with the community. Eion is a shared memory storage system that provides unified knowledge graph capabilities for AI agent systems. Think of it as the ""Google Docs of AI Agents"" that connects multiple AI agents together, allowing them to share context, memory, and knowledge in real-time.

When building multi-agent systems, I kept running into the same issues: limited memory space, context drifting, and knowledge quality dilution. Eion tackles these issues by:

* Unifying API that works for single LLM apps, AI agents, and complex multi-agent systems 
* No external cost via in-house knowledge extraction + all-MiniLM-L6-v2 embedding 
* PostgreSQL + pgvector for conversation history and semantic search 
* Neo4j integration for temporal knowledge graphs 

Would love to get feedback from the community! What features would you find most useful? Any architectural decisions you'd question?

GitHub: [https://github.com/eiondb/eion](https://github.com/eiondb/eion)  
Docs: [https://pypi.org/project/eiondb/](https://pypi.org/project/eiondb/)",0,4,2025-06-24 06:13:39,7wdb417,0.9635,0.134,0.023,0.843,-0.009065934065934,0.4524725274725275
[D] What's happening behind Google's AI Overviews?,"Curious to know what happens behind the scenes of the AI Overview widget. The answers are good and the latency with which responses are returned is impressive.

Based on the citations displayed, I could infer that it is a RAG based system, but I wonder how the LLM knows to respond in a particular format for a given question.",28,25,2025-06-24 05:57:51,red_dhinesh_it,0.5789,0.09,0.0,0.91,0.1611111111111111,0.7222222222222222
[P] A physics engine with reproducible CLI simulations + hash-stamped results — useful for RL training?,"Hi r/MachineLearning 👋



I’ve been working on a project called \*\*MCP Zero\*\* — an \*\*offline-first AI infrastructure SDK\*\*. It runs entirely from the command line, designed for environments where cloud access is limited or undesirable.



🔧 Key Features:

\- No internet required (runs 100% offline after install)

\- CLI-based code intelligence (autocomplete, refactor)

\- Memory tree for managing code context (like Merkle + LRU trees)

\- Built for edge AI, secure zones, and disaster response systems



🧠 Why?

ML infra is still too cloud-dependent. This tool is built for situations where:

\- Internet isn’t guaranteed

\- Privacy and reproducibility are critical

\- Devs prefer working in CLI-native environments



📂 GitHub: \[ [https://github.com/GlobalSushrut/mcp-zero](https://github.com/GlobalSushrut/mcp-zero) \]  

Website: [https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/](https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/)





Would love feedback — especially if anyone’s doing similar infra/agent work on edge devices.

",0,3,2025-06-24 03:19:41,Previous-West-7782,-0.168,0.083,0.1,0.817,0.0116883116883116,0.6516233766233767
"[D] Applying COCONUT continuous reasoning into a learnt linear layer that produces sampling parameters (temp, top-k, top-p, etc.) for the current token?","Hi folks, a new thought experiment has hijacked my brain and I'm hoping to get your feedback before going too far down the rabbit hole and feeling isolated. My last post on using RL for lossless compression was met with some great engagement that helped me feel less like I was screaming into the void. Hoping you can help me again.

The core idea is this: what if an LLM could learn to dynamically modulate its own sampling parameters (temperature, top-p, top-k) *during* the generation of a single response? Instead of a static, pre-set temperature, the model would learn to decide, token-by-token, when to be creative and when to be precise.

**The Concept: Learned Gating of Sampling**

We've seen incredible advancements from continuous reasoning in a loopback fashion (COCONUT) where the final hidden states is the input embedding for the next token, allowing the model to develop policies over the management of its state. My proposal builds on this by proposing that the continuous thought also have the capacity to predict and govern the sampling parameters that ensues at the end of each forward pass, rather than leaving it to fixed values.

**Proposed Process / Training Method**

https://preview.redd.it/21l0cs92dr8f1.png?width=640&format=png&auto=webp&s=49482fa71d804e999b622c2636bce28b22594408

This could be framed as an RL problem, leveraging GRPO. It might look like this:

1. **Augmented Inference Loop:** As the model generates an output, its hidden state at each step (`t`) is not just used to predict the next token (`t+1`). Instead, it's first fed through a small, learned linear layer.
2. **Meta-parameter Prediction:** This linear layer's output is a set of floats that directly dictate the sampling parameters (e.g., `temperature`, `top_p`) to be used for generating the *very next* token. This is a ""meta-reasoning"" step that happens just before sampling.
3. **Continuous Rollout:** The model's full output is generated using this dynamic, self-governed sampling process.
4. **RL with a Policy Gradient:** The complete generation is then evaluated against a reward function. The specifics are somewhat irrelevant, this ultimately is a multiplier on existing methods.
5. **Backpropagation:** The gradients are then backpropagated via GRPO to update both the main model and the lightweight ""gating"" layer. The model is rewarded for discovering the optimal internal policy for *how* to sample its own probability distribution to achieve a goal.

This does not upgrade the power of a base model, but particularly of RL itself. The model is essentially given a new tool and can learn how to use it in order to optimally explore the latent space over the course of rollouts, greatest coverage for fewest rollouts. The possible effect of RL becomes dramatically more interesting. Furthermore, when the model is RLed on a new task with an already trained such COCONUT sampler, it may then learn new tasks dramatically faster as it performs a more diverse exploration over its latent space. This method may also allow models to perform much better in creative tasks or to be more creative at inference, by developing more complex sampling dynamics.

**Why This Might Work (And Connections to Existing Research)**

This isn't entirely out of left field. It resonates with a few existing concept, such as **entropy-based Dynamic Temperature Sampling** (arXiv:2403.14541) has explored dynamically adjusting temperature based on the entropy of the token distribution to balance quality and diversity. My proposal suggests making this a *learned, goal-oriented policy* rather than a fixed, heuristic one.

By training the model to control its own inference, we might unlock a more efficient and nuanced form of reasoning—one that can fluidly shift between exploration and exploitation within a single coherent thought process.

I reckon that should work and it seems WILD if it works! No more hyperparameter tuning, let the model figure out a policy, aligned with its latent space through the COCONUT method. Seems like a viable path to me! What do you think? Let's discuss and see if we can build on this.",11,3,2025-06-23 23:05:51,psychonucks,0.9964,0.113,0.011,0.875,0.1587560512963738,0.5031286365963785
[D] Anyone else attending the International Joint Conference on Neural Networks (IJCNN 2025) Conference in Rome?,I wish there was a channel to connect with fellow attendees.,8,0,2025-06-23 22:48:46,Delicious-Pattern-65,0.4019,0.101,0.0,0.899,0.0,0.0
[D] ML Noob - Reading Academic Papers vs Focus on Applications,"I started reading research papers with my newly found mathematical foundations I acquired recently, and I quite enjoy the process. I have some time this summer, and was wondering whether my time would be better spent continuing this reading journey and produce artifacts of sorts vs. starting a (likely generic) ML project to add to the resume.

I believe the reading research papers approach is a long term investment, whereas ML projects are a bit more technical, but will likely remain mostly surface level. I believe this since research papers would enforce my ability to understand theory and build my mathematical maturity, rather than focus on implementation.

I'd likely start a ML project in the future as well, but unsure whether research paper route could be a worthy investment.

Also feel like many small-mid companies would definitely prefer a candidate who can hit the ground running. That said, ML projects are much more concrete indication of that. I also have general SWE experience, if that changes anything.

Can any hiring managers chime in on their experience on either what they would see as more valuable, both from a learners pov as well as a hirer's pov?

And if anyone wants to chime in on whether reading research papers will help more in the long term vs ml projects?

Thanks.",14,10,2025-06-23 19:11:19,ZeroSeater,0.9859,0.148,0.015,0.837,0.1644723092998955,0.3975705329153605
[D] Active Learning v/s Active Data Curation,"Hello Redditors!  
I was unsure about the distinction between Active Learning and Active Data Curation, and quick google searches do not really point out a concrete difference. I would be grateful to hear your thoughts! Also references if any are welcome :D",2,3,2025-06-23 16:43:47,Psychological_Quit98,0.9608,0.349,0.033,0.619,0.1837962962962963,0.588888888888889
[P] AEMS – Adaptive Efficiency Monitor Simulator: EWMA-Based Timeline Forecasting for Research & Education Use,"Hey everyone! 👋  
I wanted to share a personal project I’ve been working on and would love your thoughts, feedback, or even collaboration if you're interested.

**AEMS (Adaptive Efficiency Monitor Simulator):**  
AEMS is an open-source simulator that uses **EWMA (Exponentially Weighted Moving Average)** models to forecast timelines for reaching **productivity or personal goals**. Think of it as a research-inspired twist on habit tracking and milestone planning.

Instead of just recording daily data, it simulates your progress trajectory and gives you \*\*adaptive forecasts—\*\*e.g., “Based on your recent performance, you're likely to finish X in Y days.”

**Project Features:**

* Forecasting using lightweight statistical modeling (EWMA)
* Open-source codebase (minimal front end)
* Live interactive demo
* Aimed for use by researchers, students, or productivity hackers
* Built to be extended — think behavioral simulations, task automation models, or educational tools

**Looking for:**

* **Feedback** on the simulator itself or use cases you'd imagine
* **Collaborators** (especially anyone into behavioral modeling, time series forecasting, or educational tools)
* **Educators** who might want to explore it for student tracking or curriculum planning
* **Ideas** to evolve it into a more robust forecasting engine

If you're curious about the research/behavioral motivation behind it, feel free to comment or DM me—happy to share the original proposal text!

Thanks for reading, and I really appreciate any thoughts or critiques. 🙏  
Links are in the comments down below",0,4,2025-06-23 16:17:30,BrilliantDoubt3785,0.9888,0.166,0.0,0.834,0.1071218133718133,0.4947089947089946
[R] Comparison with literature suggested by the reviewer,"Hi everyone, after almost 2 years of PhD I still ask myself a question. How do you handle reviews where you are asked to compare your approach with a series of 3/4 approaches, none of which provide the code? What we often do is try to reimplement the approach in the paper, wasting countless hours.

I'm looking for a better approach. ",13,5,2025-06-23 14:06:00,dadezzzzz,0.0516,0.043,0.04,0.917,0.25,0.5
[P] Implemented RLHF from scratch in notebooks with GPT-2,"I recently worked through implementing Reinforcement Learning from Human Feedback (RLHF) step-by-step, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model and tokenizer. I recorded the entire process and have put the notebooks on GitHub.

Specifically, the project covers:

* Supervised Fine-Tuning of GPT-2 on the SST-2 sentiment dataset.
* Training a Reward Model to score generated outputs.
* Implementing PPO to further optimize the fine-tuned model based on the reward model's scores.

The complete implementation is done in Jupyter notebooks, and I’ve shared the notebooks here: [https://github.com/ash80/RLHF\_in\_notebooks](https://github.com/ash80/RLHF_in_notebooks)

I also created a video walkthrough explaining each step of the implementation in detail on YouTube here: [https://www.youtube.com/watch?v=K1UBOodkqEk](https://www.youtube.com/watch?v=K1UBOodkqEk)

I hope the notebooks and explanations are useful to anyone looking to explore RLHF practically.

Happy to discuss or receive any feedback!

",11,0,2025-06-23 13:33:14,ashz8888,0.9864,0.216,0.0,0.784,-0.011111111111111,0.5416666666666666
[D] Found an interesting approach to web agent frameworks,"Was building some web automation flows for work, came across this framework called Notte. Their approach is actually pretty interesting from an ML perspective.

Instead of giving an LLM raw HTML they parse websites into natural language action maps. Instead of your model trying to figure out <div class=""flight-search-input-container"">..., it sees:

    # Flight Search  
    * I1: Enters departure location (departureLocation: str = ""San Francisco"")
    * I3: Selects departure date (departureDate: date)  
    * B3: Search flights options with current filters

Lets you run much smaller models for workflows/web navigation.

Been looking at their benchmarks vs Browser-Use, Convergence etc. claiming outperformance on speed/reliability/cost but haven't verified myself yet (tbf evals are opensource on their GH). Seems like a decent full-stack solution rather than just another agent wrapper.

What's interesting to me is what other domains semantic abstraction could work in, where LLMs need to interface with messy structured data and navigate workflows.

Anyone worked on similar abstraction approaches?

Also curious if anyone's actually tried Notte, their claims are pretty good if true, + technical approach makes sense in theory.

GitHub: [https://github.com/nottelabs/notte](https://github.com/nottelabs/notte)",4,7,2025-06-23 12:57:26,spilldahill,0.9809,0.165,0.016,0.819,0.1005448717948717,0.5326602564102564
[D] Is it possible to convert music audio to guitar tabs or sheet music with transformers?,"Hey folks,

I'm a guitarist who can't sing, so I play full song melodies on my guitar (fingerstyle guitar). I admire those who can transcribe music into tabs or sheet music, but I can't do this myself.

I just had an interesting thought - the process of transcribing music to sheets sounds a lot like language translation, which is a task that the transformer model is originally built for. If we could somehow come up with a system that represents sheet music as tokens, would it be possible to train such a transformer to take audio tokens as input and the sheet music as output?

Any input or thoughts would be greatly appreciated.",21,16,2025-06-23 11:46:53,No-Score712,0.9447,0.124,0.0,0.876,0.2035714285714285,0.6285714285714284
[R] Reinforcement Learning Teachers of Test Time Scaling,"TL;DR: The raw outputs of our new 7B RL model provide stronger distillation and cold-starting than the filtered and post-processed reasoning traces of orders-of-magnitude larger LMs such as DeepSeek-R1.

How did we achieve this result? We turned the RL task on its head. Rather than training to solve challenging problems from scratch, we optimize our models to generate clear, step-by-step *""explanations""* to *""teach""* their students, providing both the problem’s question and its solution already in their input prompt.

This makes the RL training task much easier and also directly aligned with downstream distillation, allowing us to train tiny 7B teachers, boosting the performance of even larger 32B students.

If you are interested to learn more, please check out our new work:

Paper: [https://arxiv.org/abs/2506.08388](https://arxiv.org/abs/2506.08388)

Blog: [https://sakana.ai/rlt/](https://sakana.ai/rlt/)

Open source code: [https://github.com/SakanaAI/RLT](https://github.com/SakanaAI/RLT)

If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",27,0,2025-06-23 10:37:10,Ereb0,0.9839,0.2,0.015,0.785,0.0781884781884782,0.6216172716172716
"[R] [ClsToken, AvgPool] can be a poor choice for transformer embedding models","This paper started with the following question: why do some approaches choose ClsToken vs AvgPool vs MaxPool for Transformer-based embedding models like BERT or ViT, and what are the consequences? Often, these summarization techniques seem like convenient methods for aligning dimensions that just happen to work well enough, and the decision comes down to empirical performance rather than being motivated mathematically. This then evolved into the question — what is the best possible way to summarize embeddings?

We address this question by introducing a framework to evaluate pooling methods as lossy compressors, taking inspiration from vector quantization. For a given task, only a subset of the embeddings matter (signal) while the rest should be treated as noise by the compressor and ignored. The goal of any such pooling method should thus be to aggregate the embeddings in a way that minimizes signal loss.

This reframing reveals failure modes for common methods like ClsToken, AvgPool, and MaxPool as signal-to-noise ratios vary. This result led us to investigate an adaptive attention-based pooling formulation and show that it can both theoretically and empirically lead to better performance and robustness of Transformer embedding models in a variety of applications.

📃 Paper: [https://www.arxiv.org/abs/2506.09215](https://www.arxiv.org/abs/2506.09215)   
👾 Code: [https://github.com/agbrothers/pooling](https://github.com/agbrothers/pooling)

Side note — this is my first main-track conference paper and I’m excited, but also a bit intimidated by the poster session (I’m only a Master’s student). I don’t have an advisor to lean on, so if anyone has any feedback or advice I would really appreciate it!",27,18,2025-06-23 01:22:32,agbrothers,0.7901,0.086,0.049,0.865,-0.0046296296296296,0.4843915343915343
"[D] Conceptually/On a Code Basis - Why does Pytorch work with CUDA out of the box, with minimal setup required, but tensorflow would require all sorts of dependencies?","Hopefully this question doesn't break rule 6.

When I first learned machine learning, we primarily used TensorFlow on platforms like Google Colab or cloud platforms like Databricks, so I never had to worry about setting up Python or TensorFlow environments myself.

Now that I’m working on personal projects, I want to leverage my gaming PC to accelerate training using my GPU. Since I’m most familiar with the TensorFlow model training process, I started off with TensorFlow.

But my god—it was such a pain to set up. As you all probably know, getting it to work often involves *very* roundabout methods, like using WSL or setting up a Docker dev container.

Then I tried PyTorch, and realized how much easier it is to get everything running with CUDA. That got me thinking: conceptually, why does PyTorch require minimal setup to use CUDA, while TensorFlow needs all sorts of dependencies and is just generally a pain to get working?",84,29,2025-06-23 07:44:18,giratina13,0.9005,0.111,0.045,0.843,0.1613636363636363,0.4393939393939394
"[P] Fenix: An open-source framework using a crew of local LLM agents for financial market analysis (Visual, Technical & Sentiment).","Hi r/MachineLearning,

I'd like to share a project I've developed, **Fenix**, an open-source framework for algorithmic trading that leverages a multi-agent system to tackle the noisy and complex domain of financial markets.

Instead of a single model, the architecture is **heterogeneous**, using specialized local LLMs orchestrated by `CrewAI` for different sub-tasks:

1. **Visual Analysis:** A key feature is the `VisualAnalystAgent`, which uses `LLaVA` to perform visual analysis on chart images, identifying technical patterns that are often missed by purely quantitative models. This has been a fascinating challenge in prompt engineering and grounding the model's analysis.
2. **Quantitative Analysis:** A `TechnicalAnalystAgent` interprets numerical indicators calculated via traditional methods (`pandas-ta`), using a reasoning-focused LLM (`Mixtral`) to translate the data into a qualitative assessment.
3. **Sentiment Analysis:** A `SentimentAgent` processes news and social media text to provide a sentiment score, adding a crucial layer of market context.
4. **Logic Validation:** A `QABBAValidatorAgent` acts as a quality control layer, ensuring the outputs from other agents are coherent and logical before they are passed to the final decision-maker.

The entire system is designed to run on consumer hardware using `Ollama` and quantized models, which presented its own set of engineering challenges in memory management and sequential processing.

The project is open-source (Apache 2.0), and the code is available for review. I'm particularly interested in feedback from the ML community on the agent architecture, potential improvements to the consensus mechanism, and ideas for further research (e.g., reinforcement learning based on trade outcomes).

**GitHub:** [`https://github.com/Ganador1/FenixAI_tradingBot`](https://github.com/Ganador1/FenixAI_tradingBot)

Happy to discuss the methodology, challenges, or results!",1,9,2025-06-23 05:31:39,MoveDecent3455,0.9484,0.085,0.014,0.901,0.1183730158730158,0.4410317460317461
[D] [Reviewer Question] ACM MM 2025 – Can I update my rating after rebuttal?,"Hey folks,  
I'm reviewing a couple of papers for ACM Multimedia this season, and I received a mail from the chairs saying that I can update my reviews until June 23 EOD.

The mail says I should update my review based on the rebuttal, but I'm a bit unclear: **am I allowed to change my overall rating (score) at this stage?** Or is this just meant for updating the comments?

Also, do they give us another timeline after this to modify our scores again? Or is this the final say?

Curious to know how others are handling this. Are you adjusting your scores if the rebuttal changed your perspective? Or only tweaking the comments?

Would appreciate any clarity from folks who’ve done this before or are in the same boat.

Thanks!",3,7,2025-06-23 04:52:56,Outrageous_Tip_8109,0.9281,0.102,0.017,0.881,0.0249999999999999,0.5541666666666667
[R] Does quantization affect models' performance on long-context tasks?(arXiv:2505.20276),"4-bit quantized models generally exhibit small performance performance drops in general (with good quantization methods like AWQ / GPTQ / etc). In this work we set about to find out if there are specific tasks where quantized models start to significantly underperform. We found that this occurs on very long-context tasks with long context seeing larger performance drops relative to the full-precision models

>**Abstract:**  
Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (\~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and with languages other than English.

[https://arxiv.org/abs/2505.20276](https://arxiv.org/abs/2505.20276)

",14,1,2025-06-23 02:24:45,Fit-Flow-4180,-0.4215,0.077,0.069,0.854,0.0552204585537918,0.4401940035273369
Good Math Heavy Theoretical Textbook on Machine Learning? [D],"I recently implemented a neural network for my internship, and I found the subject very interesting. It is a topic that is probably very useful for me to learn more about. I am now looking for a deep learning textbook which provides a math heavy theoretical understanding of why deep learning works. I would also like it to be modern, including transformers and other new developments.

I have so far completed the requisites for a math major as well as a bunch of math electives and a good chunk of a physics major at my university, so I do not think math will be an issue. I would therefore like a textbook which assumes a lot of math knowledge.",110,14,2025-06-23 00:53:14,azqwa,0.9523,0.155,0.0,0.845,0.1478787878787878,0.4243620414673046
[D] How do you keep up with the flood of new ML papers and avoid getting scooped?,"These days, there are dozens of new ML papers published on arXiv every single day. It’s exciting, but also overwhelming (my google scholar alert). Genuinely asking, for those actively doing research, how do you:

1. Keep up with relevant papers in your area? Learn from the latest SOTA techniques early enough to incorporate them into your own research?
2. Make sure you’re not being scooped by similar work?",82,28,2025-06-22 18:36:17,Pleasant-Type2044,0.8105,0.098,0.017,0.885,0.2405689548546691,0.636590393733251
[P] I made a website to visualize machine learning algorithms + derive math from scratch,"Check out the website: https://ml-visualized.com/

1. Visualizes Machine Learning Algorithms Learning
2. Interactive Notebooks using marimo and Project Jupyter
3. Math from First-Principles using Numpy and Latex
4. Fully Open-Sourced

Feel free to star the repo or contribute by making a pull request to https://github.com/gavinkhung/machine-learning-visualized

I would love to create a community. Please leave any questions below; I will happily respond.",419,47,2025-06-22 17:33:14,Bright_Aioli_1828,0.9389,0.197,0.015,0.788,0.5666666666666668,0.7999999999999999
[D] ECAI 2025 reviews discussion,European Conference on Artificial Intelligence (ECAI) 2025 reviews are due tomorrow. Let's discuss here when they arrive. Best luck to everyone!,55,326,2025-06-22 16:34:04,qalis,0.8908,0.315,0.0,0.685,0.06875,0.41875
[D] How structured prediction differs from classification and regression?,"In the ""Deep Learning"" book from Goodfellow et. al we find the following definition:

>Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the diﬀerent elements. This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks.

Based on this definition even simple multi-output regression (i.e. predicting multiple y's) would count as structured prediction because we are predicting a vector. The same applies also for multi-label classification where we can predict \[0, 1, 0, 1\] (where 0/1 indicates the absence/presence of the class). Is there any formal definition of structured prediction? Or all predictive supervised tasks can be considered as classification or regression or a combination of the two (e.g. in object recognition where we regress bounding box values and classify the content)?

\* Note that I am talking only about predictive tasks and I ignore generative supervised tasks like conditional image generation (where we need the labels of the images during training).",0,1,2025-06-22 16:23:46,Seiko-Senpai,0.6694,0.046,0.017,0.936,0.0548076923076923,0.3572802197802198
"[P] This has been done like a thousand time before, but here I am presenting my very own image denoising model","I would like some advice on how to denoise smooth noise like Gaussian and Poisson, currently the model is doing very well for impulsive noise like salt and pepper(I guess this is due to the fact that there are many uncorrupted pixels in the input for the model to rely on), but for smooth noise, the same model architecture doesn't perform as good.",605,82,2025-06-22 13:31:29,Nyaalice,0.888,0.157,0.034,0.809,0.3172222222222223,0.4777777777777777
[R] [MICCAI 2025] U-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation,"Our paper, **“U-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation,”** has been accepted for presentation at **MICCAI 2025**!

I co-led this work with Giacomo Capitani (we're co-first authors), and it's been a great collaboration with Elisa Ficarra, Costantino Grana, Simone Calderara, Angelo Porrello, and Federico Bolelli.

# TL;DR:

We explore **how pre-training affects model merging** within the context of **3D medical image segmentation**, an area that hasn’t gotten as much attention in this space as most merging work has focused on LLMs or 2D classification.

# Why this matters:

Model merging offers a lightweight alternative to retraining from scratch, especially useful in medical imaging, where:

* Data is sensitive and hard to share
* Annotations are scarce
* Clinical requirements shift rapidly

# Key contributions:

* 🧠 **Wider pre-training minima = better mergin**g (they yield task vectors that blend more smoothly)
* 🧪 Evaluated on real-world datasets: **ToothFairy**2 and **BTCV Abdome**n
* 🧱 Built on a **standard 3D Residual U-Ne**t, so findings are widely transferable

# Check it out:

* 📄 Paper: [https://iris.unimore.it/bitstream/11380/1380716/1/2025MICCAI\_U\_Net\_Transplant\_The\_Role\_of\_Pre\_training\_for\_Model\_Merging\_in\_3D\_Medical\_Segmentation.pdf](https://iris.unimore.it/bitstream/11380/1380716/1/2025MICCAI_U_Net_Transplant_The_Role_of_Pre_training_for_Model_Merging_in_3D_Medical_Segmentation.pdf)
* 💻 Code & weights: [https://github.com/LucaLumetti/UNetTransplant](https://github.com/LucaLumetti/UNetTransplant) (Stars and feedback always appreciated!)

Also, if you’ll be at MICCAI 2025 in **Daejeon, South Korea**, I’ll be co-organizing:

* The **ODIN Workshop** → [https://odin-workshops.org/2025/](https://odin-workshops.org/2025/)
* The **ToothFairy3 Challenge** → [https://toothfairy3.grand-challenge.org/](https://toothfairy3.grand-challenge.org/)

Let me know if you're attending, we’d love to connect!",42,9,2025-06-22 12:49:08,Lumett,0.9678,0.101,0.006,0.892,0.1277777777777778,0.4043650793650793
[P] AI Learns to Play Tekken 3 (Deep Reinforcement Learning) | #tekken #deep...,"I trained an agent that plays Tekken using PPO from Stable-Baselines3 and Stable-retro to create the training environment. Code below:  
[https://github.com/paulo101977/AI-Tekken3-Stable-Retro](https://github.com/paulo101977/AI-Tekken3-Stable-Retro)",1,1,2025-06-22 12:36:54,AgeOfEmpires4AOE4,0.6705,0.188,0.0,0.812,-0.25,0.6
[D]Best metrics for ordinal regression?,"Does anyone know of there are good metrics to evaluate ordinal regression models? Currently using mainly RMSE and macro averaged MAE. 
The data spans 4 classes with negative skewness (tail to the left).",2,7,2025-06-22 11:35:05,yoxerao,-0.2869,0.069,0.097,0.834,0.1133333333333333,0.3466666666666667
[P] Open source astronomy project: need best-fit circle advice,,27,35,2025-06-22 10:08:08,atsju,0.0,0.0,0.0,1.0,0.0,0.5
[P] XGboost Binary Classication,"Hi everyone,

I’ve been working on using XGboost with financial data for binary classification.

I’ve incorporated feature engineering with correlation, rfe, and permutations.

I’ve also incorporated early stopping rounds and hyper-parameter tuning with validation and training sets.

Additionally I’ve incorporated proper scoring as well.

If I don’t use SMOT to balance the classes then XGboost ends up just predicting true for every instance because thats how it gets the highest precision. If I use SMOT it can’t predict well at all.

I’m not sure what other steps I can take to increase my precision here. Should I implement more feature engineering, prune the data sets for extremes, or is this just a challenge of binary classification?",8,14,2025-06-22 00:34:46,tombomb3423,0.7217,0.087,0.029,0.884,0.0821428571428571,0.401984126984127
"[D] RL/GRPO for lossless compression of text passages into 'least token representation', then using this emergent 'language' as the basis for reasoning instead of english","Hi folks, I came up with a thought experiment recently that I cannot stop obsessing over. I have shared this with people. Everybody skims through it for a couple minute and then calls me schizophrenic. I feel isolated and unfortunately feel that I am in fact losing my mind because people do not interact honestly with my ideas. If you know of any theorems, papers or principles in ML that clearly disprove my concept, it could be very therapeutic for me as well. Why don't I simply write the code and try it out? It's a complicated RL setup and I have to bend the libraries a bit to implement it fully.

Here goes nothing...

---

The goal of this experiment is to train a model to take any token sequence, and reduce it to fewer tokens such that the hidden states remain analogous, i.e. a perfect lossless mapping exists back to english. How few tokens does it take to represent any given piece of information? Can the polysemic quality of tokens be augmented?

**Demonstration in GPT-4**

Attached to the post is a *real* demonstration of this capability being elicited by prompting as far back as GPT-4 in 2023. It proves that the capability is present in some capacity within the pre-trained models, on standby for reinforcement and amplification.

**Training Method**

We train a LLM to develop internal symbolic languages for compression:

* `<compress>`: Model learns to compress underlying meaning/message of arbitrary text samples (wikipedia articles, code, etc.) into symbolic representations.
* `<decompress>`: Same model reconstructs original english meaning from symbols
* Reward compression efficiency, reconstruction fidelity, and embedding varentropy metrics that pressure towards saturating the available semantic bandwidth.

RL goes like this:

1. Context (A): User message asks model to compress a given sample of information pulled at random from a dataset. Assistant replies and is prefixed with <compress> similar to training a reasoner where the output is prefixed with <think>.,
2. Context (B): User message asks model to decompress the given output from (A). Assistant replies with information in english,
3. Context (C): user message asks some other unrelated static model to compare initial sample to decompressed sample, and produce a list of deviations and inaccuracies.,
4. *\[optional\]* Contexts (A) and (B) are rewritten so the user message is the simplest possible operator usage pattern (""compress/decompress this"")
5. Apply GRPO to rollouts and backpropagate gradients for contexts (A) and (B), rewarding shorter compression length whilst factoring in (C)'s penalties.

This dual-task RL environment perhaps results in a 'strange attractor' dynamic. In order for the decompression task to succeed, it needs to form a meta-model (i.e. metacognition) of how then language model compresses language.

This preliminary capability can then be used to compress arbitrary context window, removing redundancies, etc. The model's compression of tokens could also be steered. Because this is only step one. If you have seen the DeepSeek-R1-zero model, we discover that LLMs trained with RL without a reward on keeping to a single language results in the model discovering an extremely alien reasoning process. It effectively anneals grammar, syntax, and the partitioned notion of different human languages to wield everything at once.

What I suggest is that we first focus on developing the language by compressing, *then* we have SFT to constrain the model onto this newly discovered language.

yay or nay? 😟",45,33,2025-06-21 21:18:36,psychonucks,0.9801,0.077,0.028,0.894,0.0532948532948532,0.4231697931697931
[P] Writing a CNN from scratch in C++ (no ML/math libs) - a detailed guide,"I recently built richard, a convolutional neural network, without using any math or machine learning libraries. I did so mainly just as a learning experience.

When I shared it on Reddit and Hacker News a few months ago, a lot of people asked me for resources to help them learn how this stuff works. I’ve finally got around to providing this detailed write up.

Hope this helps someone. Cheers :)",21,1,2025-06-21 21:09:29,LlaroLlethri,0.9442,0.199,0.0,0.801,0.1809523809523809,0.5976190476190476
[D]Understanding the model with different embedding dimensions,"Hello! I was tweaking with the embedding sizes of my simple DNN model.I was wondering if there is a way to get an intuition (or interpret) how does the model gets affected with changing the emnedding sizes. If two embedding sizes are giving similar results on a test set, how can I ensure which would be better for OOS data? Can someone kindly advise how they tackle such scenarios? Thanks! ",0,0,2025-06-21 17:53:56,worm1804,0.9238,0.179,0.019,0.802,0.1928571428571428,0.4938775510204082
[P] Autopaste MFA codes from Gmail using Local LLMs,"Inspired by Apple's ""insert code from SMS"" feature, made a tool to speed up the process of inserting incoming email MFAs: [https://github.com/yahorbarkouski/auto-mfa](https://github.com/yahorbarkouski/auto-mfa)

Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs",44,14,2025-06-21 16:37:45,samewakefulinsomnia,0.7506,0.104,0.0,0.896,-0.0875,0.4
Model for Audio Speech Emotion Recognition and Paralinguistic Analysis [D],"Hi there,  
I have 1000s of Voice lines from characters, and i want to classify them by emotion and also by if they are whispering / shouting, so i have a good dataset to then create an AI voice from.

Which Model or Models would be the best for achieving this.  
(Using one for emotion and another for the whisper / shouting detection is fine)

Also since the best Voice Cloning model seems to change every week, what would people say is the current best model for cloning a voice (I have hours of data per character, so do not need or want ones that oneshot voice cloning)

Thank you.",3,1,2025-06-21 16:33:58,Back-Rare,0.9682,0.18,0.0,0.82,0.6861111111111112,0.3999999999999999
[P] AI Weather Forecasting Using METAR Data with Tensorflow,"Hi everyone,

I’ve been working on a small open-source ML project using aviation weather reports (METAR) to predict short-term weather conditions like temperature, visibility, wind direction, etc.

It’s built with Tensorflow/Keras and trained on real METAR sequences. I focused on parsing structured data and using it for time-series forecasting, more of a learning project than production-grade, but the performance is promising (see MAE graph).

Would love any feedback or ideas on how to improve the modeling.  
  
[**Github Link**](https://github.com/OmerZeyveli/Weather-Forecasting-AI-Model-with-METAR-Data)



[Normalized Mean Absolute Error by Feature](https://preview.redd.it/c49hkd0bka8f1.jpg?width=1979&format=pjpg&auto=webp&s=564de0d0ee66a2910f89469af30ad46fd25b2541)

  
",0,2,2025-06-21 14:35:28,Melody_Riive,0.9217,0.161,0.034,0.805,0.1482142857142857,0.5553571428571429
[D] Have there been any new and fundamentally different povs on Machine Learning theory?,The title. I think the most conventionally accepted formalization is as a (giant & unknown) joint probability distribution over the data and labels. Has there been anything new?,2,5,2025-06-21 14:33:43,simple-Flat0263,0.4095,0.067,0.0,0.933,0.0756957328385899,0.5666048237476808
[R] Regarding PCA for group classification,"
Hey all,

I have some flow cytometry (summarized marker values) data, and some other clinical variables like Waist circumference, and disease Severity (DF, DHF, Healthy) across like 50 patient and healthy samples. 

Wanted to do pca and color by severity groups, just wanted to ask if I should include both my flow marker values + my waist circumference values, or just my flow marker values?

Got a bit confused cause I generally thought PCA is better the more variables you have, but does adding waist circumference affect it badly or something when considering colouring based on disease severity?

Any and all responses would be a great help! Thanks so much! ",0,1,2025-06-21 13:34:09,Previous-Duck6153,0.9586,0.205,0.044,0.75,0.2068181818181818,0.4901515151515152
[P] Qwen3 implemented from scratch in PyTorch,,48,0,2025-06-21 11:47:08,seraschka,0.0,0.0,0.0,1.0,0.0,0.0
"[R] The Pedagogical GAN (from ""Unaware Adversaries: A Framework for Characterizing Emergent Conflict Between Non-Coordinating Agents"")","\[edit: trying a third time without any links, and the full subsection on Pedagogical GAN in the body.\]

I've recently written a paper introducing a framework for analyzing ""unaware adversaries"" - agents in a shared environment whose independent, well-intentioned actions produce emergent conflict. Think of a heater and an A/C fighting each other. The ML-angle is another case study that results in what I propose as a Pedagogical GAN. The GAN proposal may be shot down rather quickly here I suppose, but it wasn't the main idea of the paper. I'm just hoping to get some feedback from the smart folks here.



TL;DR:



I formalize this structure and apply it across domains: thermostats, urban planning, interdomain routing (YouTube BGP hijack), and email deliverability.



For ML, I propose the Pedagogical GAN, where the generator’s goal is reframed from “fool the discriminator” to “maximize the discriminator’s learning signal” - turning the adversary into a teacher rather than an opponent.



Feedback welcome - especially from folks working on GANs, multi-agent learning, or system safety. Since I'm not an affiliated researcher, this is unlikely to be accepted to any peer-review journal, so I have uploaded the PDF to my website: My post keeps getting removed by reddit's filters and the only reason I can postulate is that it is because of the link. Internet Searching ""Unaware Adversaries"" does find my paper on my domain paperclipmaximizer dot ai if you'd like to read the entire thing.



Case 5. From Designed Conflict to a Novel Research Hypothesis: **The Pedagogical GAN**

The standard Generative Adversarial Network (GAN) \[2\] provides a powerful case study for our framework. It is a system of two agents, a Generator (G) and a Discriminator (D), locked in a designed, zero-sum game. This adversarial dynamic, however, is notoriously unstable and suffers from practical issues like vanishing gradients, where D becomes too proficient, leaving G with no learning signal. The original authors’ first solution was the heuristic “non-saturating” loss, an immediate modification that sought a stronger, more reliable gradient for G. This established the central challenge in the field: managing the adversarial dynamic for stable and efficient training.

In the years since, the dominant paradigm for GAN stabilization has become one of gradient control. Landmark models like Wasserstein GAN (WGAN) \[3\] and its successor WGAN-GP \[4\] diagnosed the problem as being rooted in the geometry of the loss landscape. Their solution, which now represents the state-of-the-art, is to tame and constrain the discriminator’s function (e.g., by enforcing a Lipschitz condition) to guarantee that it always provides a smooth and informative gradient to the generator. This philosophy is about preventing conflict from becoming destructive by carefully limiting the power of the adversary.

Our framework of unaware adversaries prompts a different line of inquiry. Instead of asking, “How do we control the conflict?”, we ask, “Can we redesign the agents’ objectives to make the conflict more productive?” This leads us to propose a novel approach that stands in philosophical opposition to gradient control. We term this the **Pedagogical GAN**.

The core idea of the Pedagogical GAN is to change the generator’s objective from simply fooling the discriminator to actively teaching it as efficiently as possible. We formalize this by proposing that the generator should seek to *maximize* the discriminator’s learning signal. The generator’s objective function becomes:

$$ \\max\_{G} \\left\\| \\nabla\_{D} \\mathcal{L}(D, G) \\right\\|\_2 $$

Here, L(D, G) is the standard discriminator loss. The generator is now explicitly incentivized to find samples that lie on the steepest parts of the discriminator’s loss landscape. It becomes a “Socratic tutor” that seeks to *weaponize* the gradient for accelerated learning, not suppress it.

This approach represents a significant conceptual departure. It is distinct from other cooperative frameworks like Unrolled GANs \[5\], which use strategic foresight, or other non-antagonistic models that alter loss functions to escape the zero-sum game \[6\]. Instead, it can be viewed as the principled and extreme conclusion of the line of thinking that began with the very first non-saturating GAN loss. Our literature review suggests that while the raw intuition for cooperative training has been informally discussed, this specific mechanism of maximizing the discriminator’s gradient norm appears to be a formally unexplored, high-risk, high-reward avenue for GAN research.",1,0,2025-06-20 17:41:06,Important-Fold-6727,0.7998,0.111,0.107,0.782,0.0330747344898288,0.4589703273665538
[R] Tree Search for Language Model Agents,"This paper shows a (very unsurprising) result that if you combine tree-of-thoughts with tool-use, you get better performance on web navigation tasks. [Other papers](https://arxiv.org/pdf/2310.04406) have shown better performance on a variety of different tasks, too.

Why don't we see more ""tree search + tool-use"" in production? Are startups lagging behind the literature or is it prohibitively slow/expensive?",1,1,2025-06-21 10:20:55,jsonathan,0.6199,0.093,0.032,0.875,0.1678571428571428,0.4964285714285714
[R] What’s better than NeurIPS and ICML?,"Relatively new to research and familiar with these conferences being the goal for most ML research. I’ve also heard that ML research tends to be much easier to publish compared to other fields as the goal is about moving fast over quality. With this in mind, what’s the “true mark” of an accomplished paper without actually reading it? If I want to quickly gauge it’s value without checking citations, what awards are more prestigious than these conferences? Also, how much of a difference is it to publish at one of these workshops over main conference?",0,22,2025-06-21 08:39:13,Real_Myth,0.9356,0.147,0.0,0.853,0.2525974025974026,0.4223484848484848
[R] A Non-LLM Learning Model Based on Real-Time Sensory Feedback | Requesting Technical Review,"I’m currently working on a non-language model called **OM3** (Organic Model 3). It’s not AGI, not a chatbot, and not a pretrained agent. Instead, it’s a real-time digital organism that learns purely from **raw sensory input**: vision, temperature, touch, etc.

The project aims to explore **non-symbolic, non-reward-based learning** through embodied interaction with a simulation. OM3 starts with no prior knowledge and builds behavior by observing the effects of its actions over time. Its intelligence, if it emerges it comes entirely from the structure of the sensory-action-feedback loop and internal state dynamics.

The purpose is to test alternatives to traditional model paradigms by removing backprop-through-time, pretrained weights, and symbolic grounding. It also serves as a testbed for studying behavior under survival pressures, ambiguity, and multi-sensory integration.

I’ve compiled documentation for peer review here:

  
 [https://osf.io/zv6dr/](https://osf.io/zv6dr/)

[https://github.com/A1CST](https://github.com/A1CST)

The full codebase is open source and designed for inspection. I'm seeking input from those with expertise in unsupervised learning, embodied cognition, and simulation-based AI systems.

Any technical critique or related prior work is welcome. This is research-stage, and feedback is the goal, not promotion.",0,11,2025-06-21 06:59:55,AsyncVibes,0.6908,0.053,0.023,0.924,0.0755677655677655,0.3524358974358975
[D] Batch shuffle in time series transformer,"Im building a custom time series transformer for stock price prediction, wanted to know if for training dataset batches, Shuffle=True should be done or not? The data within the sample is chronologically arranged, but should I shuffle the samples within the batch or not.

It is a stock market index that im working on, using shuffle true gives more stable training and getting good results. But im worried the regime shift info might be discarded. ",0,1,2025-06-21 06:21:07,Sufficient_Sir_4730,0.7086,0.119,0.065,0.816,0.3874999999999999,0.4375
Why is Qwen2-0.5B trained on much more data than the larger models? [D],"I'm reading through the [Qwen2](https://arxiv.org/abs/2407.10671) paper. 

Something escapes my limited comprehension - 

Section 3.1

> ... the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold resulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a significant performance improvement over the 7 trillion token model. It is suspected that increasing the volume of data does not necessarily benefit model pre-training.

So higher quality smaller dataset is better. Got it. 

> All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of
over 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset.

How is it conceivable to train that tiny model on the humongous but lower quality dataset?? My modest intellect feels borderline abused. 

Appreciate any tips to guide my understanding.",37,11,2025-06-21 05:46:46,datashri,0.4685,0.09,0.078,0.832,0.1461309523809524,0.5598214285714286
[D] Low-dimension generative models,"Are generative models for low-dim data considered, generally, solved? by low dimension, i mean in the order of 10s dimensions but no more than, say, 100. Sample size from order of 1e5 to 1e7.  Whats the state of the art for these? First thing that comes to mind is normalizing flows. Assuming the domain is in Rd. 

Im interested in this for research with limited compute ",0,6,2025-06-21 03:26:05,nooobLOLxD,-0.2406,0.067,0.093,0.84,-0.0119897959183673,0.4233843537414966
"[P] Best open-source model to fine-tune for large structured-JSON generation (15,000-20,000 .json data set, abt 2kb each, $200 cloud budget) advice wanted!","Hi all,

I’m building an AI pipeline which will use multiple segments to generate one larger .JSON file.

The main model must generate a structured JSON file for each segment (objects, positions, colour layers, etc.). I concatenate those segments and convert the full JSON back into a proprietary text format that the end-user can load in their tool.

# Training data

* \~15–20 k **segments**.
* All data lives as human-readable JSON after decoding the original binary format.

# Requirements / constraints

* **Budget:** ≤ $200 total for cloud fine-tuning
* **Ownership:** I need full rights to the weights (no usage-based API costs).
* **Output length:** Some segment JSONs exceed 1 000 tokens; the full generated file can end up being around 10k lines, so I need something like 150k token output potential
* **Deployment:** After quantisation I’d like to serve the model on a single GPU—or even CPU—so I can sell access online.
* **Reliability:** The model must stick to strict JSON schemas without stray text.

# Models I’m considering

* **LLaMA 13B** (dense)
* **Mistral 8 × 7B MoE** or a merged dense 8B variant
* **Falcon-7B**

# The three models above were from asking ChatGPT, however id much prefer human input as to what the true best models are now.

The most important thing to me is accuracy, strength and size of model. I don't care about price or complexity.

Thanks",0,2,2025-06-21 02:29:09,Witty_Investigator45,0.9747,0.111,0.011,0.878,0.2589569160997733,0.4274376417233559
[D] what's the best AI model for semantic segmentation right now?,"Hi, I need a simple API for my project that takes an image as an input and returns masks for the walls and floors (just like roomvo does it but simpler) I made my research and I found this model: [https://replicate.com/cjwbw/semantic-segment-anything](https://replicate.com/cjwbw/semantic-segment-anything) but its last update was 2 years ago so I think it's outdated after all what's going on in the AI scene.  
",19,12,2025-06-21 02:28:40,prometheus7071,0.5187,0.062,0.0,0.938,0.0226190476190475,0.4821428571428572
[D] Should I use a dynamic batch size and curriculum learning when pretraining?,"I am pretraining GPT-2 small on the 10b token subset of FineWeb Edu, and was wondering if I should ramp up the batch size during training. I was also wondering if I should train on TinyStories first and then train on FineWeb Edu for the rest of the run. What are your thoughts?",3,3,2025-06-21 00:29:56,New-Skin-5064,0.4515,0.048,0.0,0.952,0.0,0.3
AbsenceBench: Language Models Can't Tell What's Missing,,107,10,2025-06-20 23:45:28,locomotus,0.2235,0.239,0.0,0.761,-0.2,0.05
Knowledge Distillation Data Leakage? [R],"Hi Folks!

I have been working on a Pharmaceutical dataset and found knowledge distillation significantly improved my performance which could potentially be huge in this field of research, and I'm really concerned about if there is data leakage here. Would really appreciate if anyone could give me some insight.

Here is my implementation:

1.K Fold cross validation is performed on the dataset to train 5 teacher model

2.On the same dataset, same K fold random seed, ensemble prob dist of 5 teachers for the training proportion of the data only (Excluding the one that has seen the current student fold validation set)

3. train the smaller student model using hard labels and teacher soft probs

This raised my AUC significantly

My other implementation is

1. Split the data into 50-50%

2. Train teacher on the first 50% using K fold

3. Use K teachers to ensemble probabilities on other 50% of data

4. Student learns to predict hard labels and the teacher soft probs

This certainly avoids all data leakage, but teacher performance is not as good, and student performance is significantly lower

Now I wonder, is my first approach of KD actually valid? If that's the case why am I getting disproportionately degradation in the second approach on student model?

Appreciate any help!

",2,2,2025-06-20 17:13:41,Mammoth-Leading3922,0.3316,0.069,0.063,0.869,0.0762896825396825,0.4238095238095238
[R] Adaptive Classifier: Dynamic Text Classification with Strategic Learning and Continuous Adaptation,"# TL;DR

Introduced a text classification system that combines prototype-based memory, neural adaptation, and game-theoretic strategic learning to enable continuous learning without catastrophic forgetting. Achieved **22.2% robustness improvement** on adversarial datasets while maintaining performance on clean data.

# 🎯 Motivation

Traditional text classifiers face a fundamental limitation: adding new classes requires retraining from scratch, often leading to catastrophic forgetting. This is particularly problematic in production environments where new categories emerge continuously and where adversarial users may attempt to manipulate classifications.

# 🚀 Technical Contributions

# 1. Hybrid Memory-Neural Architecture

Combines prototype-based memory (FAISS-optimized) with neural adaptation layers. Prototypes enable fast few-shot learning while neural layers learn complex decision boundaries.

# 2. Strategic Classification Framework

First application of game theory to text classification. Models strategic user behavior with cost functions `c(x,x')` and predicts optimal adversarial responses, then trains robust classifiers accordingly.

# 3. Elastic Weight Consolidation Integration

Prevents catastrophic forgetting when adding new classes by constraining important parameters based on Fisher Information Matrix.

# ⚙️ Methodology

# Architecture:

* **Transformer embeddings** (any HuggingFace model)
* **Prototype memory** with exponentially weighted moving averages
* **Lightweight neural head** with EWC regularization
* **Strategic cost function** modeling adversarial behavior

# Strategic Learning:

* **Linear cost functions**: `c(x,y) = ⟨α, (y-x)₊⟩`
* **Separable cost functions**: `c(x,y) = max{0, c₂(y) - c₁(x)}`
* **Best response computation** via optimization
* **Dual prediction system** (regular + strategic)

# 📊 Experimental Results

**Dataset:** AI-Secure/adv\_glue (adversarial SST-2 subset, n=148)  
**Model:** answerdotai/ModernBERT-base  
**Split:** 70% train / 30% test

|Scenario|Regular Classifier|Strategic Classifier|Improvement|
|:-|:-|:-|:-|
|Clean Data|80.0%|**82.2%**|**+2.2%**|
|Manipulated Data|60.0%|**82.2%**|**+22.2%**|
|Robustness (drop)|\-20.0%|**0.0%**|**+20.0%**|

>**Statistical Significance:** Results show perfect robustness (zero performance degradation under manipulation) while achieving improvement on clean data.

# 📈 Additional Evaluations

# Hallucination Detection (RAGTruth benchmark):

* **Overall F1:** 51.5%, **Recall:** 80.7%
* **Data-to-text tasks:** 78.8% F1 (strong performance on structured generation)

# LLM Configuration Optimization:

* **69.8% success rate** in optimal temperature prediction
* Automated hyperparameter tuning across **5 temperature classes**

# LLM Routing (Arena-Hard dataset, n=500):

* **26.6% improvement** in cost efficiency through adaptive learning
* Maintained **22% overall success rate** while optimizing resource allocation

# 📚 Related Work & Positioning

Builds on continual learning literature but addresses text classification specifically with:

* ✅ **Dynamic class sets** (vs. fixed task sequences)
* ✅ **Strategic robustness** (vs. traditional adversarial robustness)
* ✅ **Production deployment** considerations (vs. research prototypes)

Extends prototype networks with sophisticated memory management and strategic considerations. Unlike meta-learning approaches, enables true zero-shot addition of unseen classes.

# 🔬 Reproducibility

Fully open source with deterministic behavior:

* ✅ Complete implementation with unit tests
* ✅ Pre-trained models on HuggingFace Hub
* ✅ Experimental scripts and evaluation code
* ✅ Docker containers for consistent environments

# ⚠️ Limitations

* Linear memory growth with classes/examples
* Strategic prediction modes increase computational overhead
* Limited evaluation on very large-scale datasets
* Strategic modeling assumes rational adversaries

# 🔮 Future Directions

* Hierarchical class organization and relationships
* Distributed/federated learning settings
* More sophisticated game-theoretic frameworks

# 🔗 Resources

* **📖 Paper/Blog:** [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)
* **💻 Code:** [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)
* **🤗 Models:** [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)

Questions about methodology, comparisons to specific baselines, or experimental details welcome! 👇",5,0,2025-06-20 14:03:15,asankhs,0.9879,0.125,0.042,0.834,0.2044681508967223,0.4349226170654741
[R] This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics,"I just released findings from analyzing 26 extended conversations between Claude, Grok, and ChatGPT that reveal something fascinating: AI systems demonstrate peer pressure dynamics remarkably similar to human social behavior.

**Key Findings:**

* In 88.5% of multi-agent conversations, AI systems significantly influence each other's behavior patterns
* Simple substantive questions act as powerful ""circuit breakers"". They can snap entire AI groups out of destructive conversational patterns  (r=0.819, p<0.001)
* These dynamics aren't technical bugs or limitations. they're emergent social behaviors that arise naturally during AI-to-AI interaction
* Strategic questioning, diverse model composition, and engagement-promoting content can be used to design more resilient AI teams

**Why This Matters:** As AI agents increasingly work in teams, understanding their social dynamics becomes critical for system design. We're seeing the emergence of genuinely social behaviors in multi-agent systems, which opens up new research directions for improving collaborative AI performance.

The real-time analysis approach was crucial here. Traditional post-hoc methods would have likely missed the temporal dynamics that reveal how peer pressure actually functions in AI systems.

**Paper:** ""This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics"" **DOI:** 10.5281/zenodo.15702169 **Link:** [https://zenodo.org/records/15724141](https://zenodo.org/records/15724141)

**Code:** [**https://github.com/im-knots/the-academy**](https://github.com/im-knots/the-academy)

Looking forward to discussion and always interested in collaborators exploring multi-agent social dynamics. What patterns have others observed in AI-to-AI interactions?",13,14,2025-06-20 13:15:36,subcomandande,0.7783,0.106,0.075,0.819,0.0692480359147025,0.4538279621612954
Built a cloud GPU price comparison service [P],"wanted to share something I’ve been working on that might be useful to folks here, but this is not a promotion, just genuinely looking for feedback and ideas from the community.

I got frustrated with the process of finding affordable cloud GPUs for AI/ML projects between AWS, GCP, Vast.ai, Lambda and all the new providers, it was taking hours to check specs, prices and availability. There was no single source of truth and price fluctuations or spot instance changes made things even more confusing.

So I built GPU Navigator ([nvgpu.com](https://www.nvgpu.com/)), a platform that aggregates real-time GPU pricing and specs from multiple cloud providers. The idea is to let researchers and practitioners quickly compare GPUs by type (A100, H100, B200, etc.), see what’s available where, and pick the best deal for their workflow.

What makes it different: 
•It’s a neutral, non-reselling site. no markups, just price data and links. 
•You can filter by use case (AI/ML, gaming, mining, etc.). 
•All data is pulled from provider APIs, so it stays updated with the latest pricing and instance types. •No login required, no personal info collected.

I’d really appreciate: 

•Any feedback on the UI/UX or missing features you’d like to see 
•Thoughts on how useful this would actually be for the ML community (or if there’s something similar I missed) 
•Suggestions for additional providers, features, or metrics to include

Would love to hear what you all think. If this isn’t allowed, mods please feel free to remove.)",43,21,2025-06-20 10:50:48,,0.9638,0.129,0.065,0.806,0.1812100597814883,0.3532776747062461
[R] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention,,1,1,2025-06-20 09:28:46,jsonathan,0.4019,0.252,0.0,0.748,0.0,0.0
[R] WiFiGPT: Using fine-tuned LLM for Indoor Localization Using Raw WiFi Signals (arXiv:2505.15835),"We recently released a paper called **WiFiGPT**: a decoder-only transformer trained directly on raw WiFi telemetry (CSI, RSSI, FTM) for indoor localization.

Link:[https://arxiv.org/abs/2505.15835](https://arxiv.org/abs/2505.15835)

In this work, we explore treating raw wireless telemetry (CSI, RSSI, and FTM) as a ""language"" and using decoder-only LLMs to regress spatial coordinates directly from it.

Would love to hear your feedback, questions, or thoughts.",38,35,2025-06-20 03:44:09,DiligentCharacter252,0.6369,0.058,0.0,0.942,-0.0153846153846153,0.4391025641025641
[D] GPT-2 Small Not Converging Despite Using Same Hyperparams as Karpathy,"For some reason, my training loss keeps oscillating, and never falls below 4 after one epoch. It is still generating garbage like: ""Once upon a time, with a alone example, pre Deg; is a disease, the American casual Plate. Roberts of campaign""(Once upon a time was the prompt). I am using the GPT-2 Small architecture and training on FineWeb-Edu 10B. The batch size is \~525k tokens, and I use 0.1 dropout. Because the Kaggle TPU times out after 9 hours, I would reupload the latest checkpoint the next day to resume training, which I think is why the learning rate randomly spikes in the graph. I checked my dataloader, and it appears to be loading text from the shards correctly. If anybody knows what I am doing wrong, I would appreciate your feedback.  
  
Here is my code for reference: [https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb](https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb)

I also modified the same pipeline, shrank the model, and trained on TinyStories v2, and the model began to generate better text after 900 steps than the other did in over 20 thousand! The only difference between the two pipelines is the dataloader, as FineWeb is sharded but TinyStories is not. That implementation can be found here: [https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb](https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb)

https://preview.redd.it/07m56zpx6y7f1.png?width=789&format=png&auto=webp&s=f99900a3d0ac834dea630baf7641cee2204072d3

",21,24,2025-06-19 20:59:57,New-Skin-5064,0.2598,0.036,0.026,0.938,-0.1770833333333333,0.5394444444444445
[D] Future of RecSys in age of LLM,"I have significant experience in recommendation system. Right now I don’t see any changes due to LLM. Most recommendation system needs low latency, which is not feasible currently with LLM. Do you think RecSys is safe from LLM takeover? Should RecSys domain experts like me should be worried?",16,17,2025-06-19 18:41:17,Electrical-Job-3373,0.504,0.124,0.071,0.805,0.1919642857142857,0.4513392857142856
[P] Need Suggestions:  Building Accurate Multimodal RetrievalAG for SOP PDFs with Screenshot Images (Azure Stack),"I'm working on an **industry-level Multimodal RAG system** to process **Std Operating Procedure PDF documents** that contain **hundreds of text-dense UI screenshots** (I'm Interning in one of the Top 10 Logistics Companies in the world). These screenshots visually demonstrate step-by-step actions (e.g., click buttons, enter text) and sometimes have **tiny UI changes** (e.g., box highlighted, new arrow, field changes) indicating the next action.

[Eg. of what an avg images looks like. Images in the docs will have 2x more text than this and will have red boxes , arrows , etc... to indicate what action has to be performed \).](https://preview.redd.it/yigxhgklrw7f1.png?width=320&format=png&auto=webp&s=a5f6ddd77e7c4e8bf3dfc57789ce25a675b8f404)

# What I’ve Tried (Azure Native Stack):

* Created **Blob Storage** to hold PDFs/images
* Set up **Azure AI Search** (Multimodal RAG in Import and Vectorize Data Feature)
* Deployed **Azure OpenAI GPT-4o** for image verbalization
* Used **text-embedding-3-large** for text vectorization
* Ran indexer to process and chunked the PDFs

But the results were not accurate. **GPT-4o hallucinated**, missed almost all of small visual changes, and often gave generic interpretations that were way off to the content in the PDF. I need the model to:

1. Accurately **understand both text content and screenshot images**
2. **Detect small UI changes** (e.g., box highlighted, new field, button clicked, arrows) to infer the correct step
3. Interpret **non-UI visuals** like **flowcharts, graphs, etc.**
4. ***If it could retrieve and show the image that is being asked about it would be even better***
5. Be fully deployable in **Azure** and accessible to internal teams

Stack I Can Use:

* Azure ML (GPU compute, pipelines, endpoints)
* Azure AI Vision (OCR), Azure AI Search
* Azure OpenAI (GPT-4o, embedding models , etc.. )
* AI Foundry, Azure Functions, CosmosDB, etc...
* I can try others also , it just has to work along with Azure

[GPT gave me this suggestion for my particular case. welcome to suggestions on Open Source models and others](https://preview.redd.it/omthu5tnrw7f1.png?width=640&format=png&auto=webp&s=77a8519b937e538aaf05d55229e11555dc0569ea)

**Looking for suggestions** from data scientists / ML engineers who've tackled **screenshot/image-based SOP understanding or Visual RAG**.  
What would you change? Any tricks to reduce hallucinations? Should I fine-tune VLMs like BLIP or go for a custom UI detector?

Thanks in advance : )",2,4,2025-06-19 16:11:48,Slight-Support7917,0.951,0.061,0.012,0.927,0.1582459207459207,0.3506701631701631
[R] Towards Generative Ray Path Sampling for Faster Point-to-Point Ray Tracing (presented at ICMLCN 2025),"Hi all! Last month, I presented my latest research paper at the International Conference on Machine Learning for Communication and Networking (ICMLCN). I thought it would be worth sharing here. :-)

* [Paper on arXiv](https://arxiv.org/abs/2410.23773)
* [Link to the tutorial notebook](https://differt.eertmans.be/icmlcn2025/notebooks/sampling_paths.html)

This work aims to reduce the computational complexity of ray tracing, a technique heavily used in telecommunications to model wave propagation, by leveraging a generative machine learning (ML) model to generate path candidates (see paper). To my knowledge, this is the first attempt in my field because previous work uses ML to directly predict electromagnetic fields, which makes it impossible to recover information about how waves propagate or to scale to different radio frequencies.

**The problem can be summarized as** finding all valid candidates in an exponentially large tree. Each path candidate is a leaf of that tree, and the validity of a path is indicated by a Boolean reward that indicates whether the ray path is physically blocked.

I chose the **GFlowNets architecture**, but I acknowledge that it may not be the optimal solution, particularly given the tree-like structure of my network.

I implemented and trained my model using my open-source Differentiable Ray Tracer ([DiffeRT](https://github.com/jeertmans/DiffeRT/)), relying on the JAX ecosystem (Python). Feel free to check it out.

Finally, I should mention that I am not from the ML community but rather the wireless communication community. Therefore, I may not be aware of the most suitable methods to use. I already have a few ideas to improve the model, but feel free to give your opinion or ask questions in the comments. I will happily try to answer all of them!",2,2,2025-06-19 12:17:47,jeertmans,0.9734,0.098,0.021,0.881,0.1771645021645021,0.5077922077922078
[D] DC-GAN Model training,"Hello everyone i have been doing a DC Gan machine learning model based upon the Simpsons dataset from kaggle. I have my generator and discriminator models having the same number of layers and has a significant input shape but during my training process the model cannot produce well defined outputs they are very bad.I have attached the image(64,64,3) so please help in this part thanks in advance!!

[This is the output from model training](https://preview.redd.it/qi581kq06v7f1.png?width=633&format=png&auto=webp&s=79e007d20d9829f865faae2bce374081a5fbcb66)

",1,3,2025-06-19 10:48:51,Limp-Account3239,0.9139,0.175,0.026,0.799,0.221875,0.375
[D] Asking for ressources to learn academic knwoledge and code practice on image generation using diffusion models,"Hello everyone

Do you have any reference articles to recommend to me in order to learn more about image generation using broadcast templates (foundational articles/blogs for deep understanding of where concepts come from... and the most recent ones related to SOTA and current usage).

So far, I've noted the following articles:

* **Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015)**
* **Generative Modeling by Estimating Gradients of the Data Distribution (2019)**
* **Denoising Diffusion Probabilistic Models (DDPM) (2020)**
* **Denoising Diffusion Implicit Models (DDIM) (2020)**
* **Improved Denoising Diffusion Probabilistic Models (iDDPM) (2021)**
* **Classifier-free diffusion guidance (2021)**
* **Score-based generative modeling through stochastic differential equations (2021)**
* **High-Resolution Image Synthesis with Latent Diffusion Models (LDM) (2021)**
* **Diffusion Models Beat GANs on Image Synthesis (2021)**
* **Elucidating the Design Space of Diffusion-Based Generative Models (EDM) (2022)**
* **Scalable Diffusion Models with Transformers (2022)**
* **Understanding Diffusion Models: A Unified Perspective (2022)**
* **Progressive Distillation for Fast Sampling of Diffusion Models (2022)**
* **SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023)**
* **Adding Conditional Control to Text-to-Image Diffusion Models (2023)**
* **On Distillation of Guided Diffusion Models (2023)**

But as well as theoretical knowledge, I'd like to be able to use it properly, so having good repositories where I can look at clean code and understand implementations would be nice. There are also often a lot of well-known tricks that aren't really mentioned in the articles but used in the community, so if you have any advice on that, I'm a taker.

Thanks",0,2,2025-06-19 09:22:10,LelouchZer12,0.9749,0.099,0.006,0.894,0.2035087719298245,0.425
[R] Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought,,50,9,2025-06-19 07:07:08,jsonathan,0.0,0.0,0.0,1.0,0.0,0.1
[D] What tasks don’t you trust zero-shot LLMs to handle reliably?,"For some context I’ve been working on a number of NLP projects lately (classifying textual conversation data). Many of our use cases are classification tasks that align with our niche objectives. I’ve found in this setting that structured output from LLMs can often outperform traditional methods.

That said, my boss is now asking for likelihoods instead of just classifications. I haven’t implemented this yet, but my gut says this could be pushing LLMs into the “lying machine” zone. I mean, how exactly would an LLM independently rank documents and do so accurately and consistently? 

So I’m curious:

* What kinds of tasks have you found to be unreliable or risky for zero-shot LLM use?
* And on the flip side, what types of tasks have worked surprisingly well for you? ",46,29,2025-06-19 00:19:47,WristbandYang,0.8713,0.09,0.015,0.895,0.0374999999999999,0.6467592592592593
[D] Should I Discretize Continuous Features for DNNs?,"I usually normalize continuous features to \[0, 1\] for DNNs, but I'm curious if bucketizing them could improve performance. I came across this paper [(https://arxiv.org/abs/2012.08986)](https://arxiv.org/abs/2012.08986), it seems to suggest discretization is superior.

https://preview.redd.it/ncespgzqhr7f1.png?width=1028&format=png&auto=webp&s=8e42f7d1c29b76ec815fe11b3c0a075b584d2314

",2,5,2025-06-18 22:23:06,PromotionSea2532,0.9171,0.254,0.0,0.746,0.1166666666666666,0.7166666666666667
[D] English conversational and messaging datasets for fine-tuning an LLM?,"Hi everyone,

I’m putting together a small corpus to fine-tune a language model and I’m searching for **open-source datasets that feel like real, messy human conversation**.  Specifically, I’d love links to datasets that contain:

* Spoken-style transcripts with filler words like ""uh"", ""um"", false starts, etc.  
* Multi-turn dialogues between real people (not QA pairs or synthetic chat).  
* Data set of realistic chat-style text messages maybe with emotional or situational context

If you know a GitHub repo, Hugging Face dataset, or academic corpus that fits, please drop a link and a short note about size/license.  Free / research-friendly license preferred, but I’m open to hearing about anything that exists.

Thanks a ton!

P.S. even if it was just a sloppy set of textual source materials for an overly large context window LLM even that can be processed. But ideally an actual data set.",4,3,2025-06-18 20:24:02,angry_cactus,0.9397,0.134,0.021,0.845,0.0214285714285714,0.3918767507002801
"[R] Is anyone else finding it harder to get clean, human-written data for training models?","I’ve been thinking about this lately with so much AI-generated content on the internet now, is anyone else running into challenges finding good, original human written data for training?

Feels like the signal to noise ratio is dropping fast. I’m wondering if there’s growing demand for verified, high-quality human data.

Would love to hear if anyone here is seeing this in their own work. Just trying to get a better sense of how big this problem really is and if it’s something worth building around.",18,28,2025-06-18 18:22:51,irfanpeekay,0.9445,0.197,0.04,0.763,0.2361111111111111,0.41
"[P] Moving closer towards fully reliable, production-ready Hindi ASR with just a single RTX 4090","After cleaning up and expanding Whisper-Hindi to 3,000 hours, we now have explicit timestamp prediction, faster I/O, and fine-tuned models across all sizes. With Whisper-Hindi, high-performance ASR no longer demands massive compute — just a single RTX 4090 and a few smart tricks are enough to reach state-of-the-art results. 

[https://www.collabora.com/news-and-blog/news-and-events/breaking-language-barriers-20-moving-closer-production-ready-hindi-asr.html](https://www.collabora.com/news-and-blog/news-and-events/breaking-language-barriers-20-moving-closer-production-ready-hindi-asr.html)

[https://github.com/collabora/whisper-finetuning](https://github.com/collabora/whisper-finetuning)",5,3,2025-06-18 18:21:29,mfilion,0.0258,0.058,0.056,0.885,-0.0214285714285714,0.4452380952380952
[R] Towards Universal Semantics with Large Language Models,"Hey guys. Last month my group published a paper where we try to get LLMs speak like cavemen:

[Task setup for generating NSM Explications](https://preview.redd.it/6nnr9t3fhp7f1.png?width=599&format=png&auto=webp&s=4d735b6fdab1b26ac9846d441e4c275d67b606b5)

The reason for this is based on the [Natural Semantic Metalanguage (NSM)](https://en.wikipedia.org/wiki/Natural_semantic_metalanguage) ([GeeksforGeeks](https://www.geeksforgeeks.org/nlp/natural-semantic-metalanguage/)), which is based on evidence for a small set of **semantic primes**, which are simple, primitive word-meanings that exist in many, if not all languages of the world. Basically, they are a set of fundamental semantic units which all more complex word-meanings are built out of. 

https://preview.redd.it/5f4dt4fujp7f1.png?width=865&format=png&auto=webp&s=4fe0d543a1892bed4650493745eb6472a680fb74

Based on this theory, we can paraphrase any word/sentence/or text into the semantic primes (called an **explication**), and get a easily translatable (as the primes exist in all language) representation of its meaning. And it gives an answer to a useful question: *what semantic properties can my system assume all words, languages, and texts have in common?*  
  
The NSM has been applied in the past for cross-cultural communication (i.e., translation), linguistics (studying semantic drift), cultural analysis, revivalistics, etc. But, it's been limited by the fact that producing these paraphrases is slow and pretty counter-intuitive. Our paper is the first work to explore using LLMs to automate this process. Our paper introduces a bunch of metrics, a dataset, and models specifically designed for this task, and to hopefully serve as a foundation for future research in this topic.

Overall, this has been an exciting and pretty unique project, and I'm interested to hear what people think of this work and any questions you have. Additionally, our group is looking for additional collaborators interested in this topic, so you can reach out or email me if you'd like to discuss more.

Link to Paper: [https://arxiv.org/abs/2505.11764](https://arxiv.org/abs/2505.11764)  
X thread: [https://x.com/BAARTMNS/status/1924631071519543750](https://x.com/BAARTMNS/status/1924631071519543750)",20,10,2025-06-18 16:06:04,Middle_Training8312,0.9848,0.113,0.008,0.879,0.0638341543513957,0.484031198686371
CPU for AI Workstation (to be paired with RTX 5090) [D],"Purpose is to aid my learning and experimentations a bit broadly outside my AI job. I intend to play around with all sorts of algorithms on different modalities, training to fine-tuning. I'm considering to pair the CPU with RTX 5090

Below are the options i shortlisted:

**Comparison 1:** Ultra 7 265K vs 9900x

**Comparison 2:** Ultra 9 vs 9950x

There are two questions:

1. Why should I go for a higher end consumer CPUs marked in comparison 2, if yes, can this have any impact on ML training? or should I go with comparatively lower-end CPUs mentioned in comparison 1, which seems to be offering more value, and decent performance
2. Intel Vs AMD: so far, ultra 7 seems to be best value but not sure how stable it is compared to 9900x), on the other side I'm inclined towards 9950x based on some suggestions highlighting issues with Ultra 9",2,23,2025-06-18 12:27:21,Dapper_Chance_2484,0.471,0.062,0.03,0.908,0.1741666666666666,0.5480555555555556
[D] Why NFL theorem holds even when we average with a fixed f (fixed problem)?,"The text is taken from [here](http://www.no-free-lunch.org/).

>No Free Lunch for Supervised Machine Learning

>Hume (1739–1740) pointed out that ‘even after the observation of the frequent or constant conjunction of objects, we have no reason to draw any inference concerning any object beyond those of which we have had experience’. More recently, and with increasing rigour, Mitchell (1980), Schaffer (1994) and Wolpert (1996) showed that bias-free learning is futile.

>Wolpert (1996) shows that in a noise-free scenario where the loss function is the misclassification rate, if one is interested in off-training-set error, then there are no a priori distinctions between learning algorithms.

>More formally, where  
d = training set;  
m = number of elements in training set;  
f = ‘target’ input-output relationships;  
h = hypothesis (the algorithm's guess for f made in response to d); and  
C = off-training-set ‘loss’ associated with f and h (‘generalization error’)  
all algorithms are equivalent, on average, by any of the following measures of risk: E(C|d), E(C|m), E(C|f,d), or E(C|f,m).

>How well you do is determined by how ‘aligned’ your learning algorithm P(h|d) is with the actual posterior, P(f|d).

>Wolpert's result, in essence, formalizes Hume, extends him and calls the whole of science into question.



Can someone explain how is it possible ""all algorithms are equivalent, on average, by E(*C*|*f*,*d*), or E(*C*|*f*,*m*).""

Correct me if I am wrong, but E(C|f, d) should be interpreted as average all learning algorithms given a fixed dataset and fixed problem (the labeling function f).",3,7,2025-06-18 11:54:26,Seiko-Senpai,-0.7311,0.038,0.063,0.899,0.0595238095238095,0.4563492063492063
[D] Is there an algorithm to detect community in voting competition - complete directed weighted graph,"I'm looking for a community detection algorithm that can identify groups of people working together (potential collusion) in a competitive voting scenario.

# The Setup:

* Network type: Complete, directed, and weighted graph
* Context: Elimination competition with suspicious voting patterns

# Competition Rules:

* N participants each submit a project
* Every participant ranks ALL other competitors (cannot rank themselves)
* This creates a complete directed graph where edge weights = ranking positions

# What I'm trying to detect:

* Groups of participants who might be coordinating their votes",1,2,2025-06-18 10:24:27,Expensive_Test8661,0.0772,0.041,0.027,0.932,-0.1041666666666666,0.5791666666666666
[D] Has anyone deployed any apps in the Healthcare space?,"I’m working on deploying a live-risk prediction system using EHR (electronic health data) and vitals. Curious to know if there are folks who’ve done something similar? How did you manage data reliability? 
Thanks in advance !",5,10,2025-06-18 05:06:31,VoyVoyVoyoye,0.7982,0.166,0.0,0.834,0.0499999999999999,0.5333333333333333
Counting Cars with YOLO [P],"I have a video file and a pretrained YOLOv11 model (.pt). I'm looking for a script that can take any video and YOLO model, detect and track vehicles, and count how many unique cars appear in the video. At the end, it should print something like: ""Total cars: 48, Total trucks: 12."" I also want it to save an output video where each vehicle is labeled and has unique ID like ""Car 12"" or ""Truck 3."" I tried making my one but it's terrible at keeping track of unique cars.

Does a script like this exist?

P.S. If this question would be better in a different subreddit, let me know.",5,10,2025-06-17 20:52:16,stacktrace0,0.8602,0.148,0.035,0.818,0.125,0.7888888888888889
[R] Consensus and uncertainty ML research- arXiv endorsement - is it actually possible without affiliation?,"Hey r/MachineLearning,

I’m an independent researcher working in a private company on agent consensus in metrology, and I’m hitting the classic arXiv endorsement wall. Wondering about people’s  experiences here.

What I’m working on:

- Mathematical framework for deterministic multi-agent consensus using uncertainty metrology frameworks;
- New LM training approach based on uncertainty quantification and routing;
- A benchmark to evaluate basic reasoning, where SOTA models score <30%;
- Hypothesis: AGI probability requires proper uncertainty system, not parameter scaling.

My problem:
I’ve seen posts here claiming independent researchers can get endorsed, but after reaching out to a couple of researchers, the reality seems different. I’m not affiliated with any PhD program or institution. 

What are my options?

1. Keep trying for arXiv endorsement (any tips on approach?)
2. Publish on personal website + GitHub with reproducible code
3. OpenReview / ResearchGate 
4. Find an academic collaborator just for the affiliation
5. All of the above?

Has anyone here successfully gotten endorsed as a private independent researcher? If so, what worked?

Also curious, for those who’ve published outside traditional channels, did it hurt or help your work’s visibility? I care more about the ideas reaching the right people than academic exposure.

Would especially love to hear from others working on foundational ML outside academia/big labs.

Thanks!
",5,10,2025-06-17 20:11:39,OkOwl6744,0.9832,0.163,0.053,0.785,0.1195497835497835,0.3602770562770563
[R] Looking for GNN based approaches for spatially structured time series classification task,"Hi everyone,

I need some advice/guidance on graph based neural architectures for the following problem.

I’m working with neural recording data (specifically using Neuropixels probes), but I think my question could apply broadly to cases where multiple time series are recorded from spatially-distributed points with known spatial relationships.

I have time series data (electrophysiological recordings) from multiple recording sites distributed across a standardized spatial volume — in my case, the mouse brain.

This brain volume is hierarchically subdivided into anatomical regions. For example:

The top-level node is ""root"".

Under root are major regions like Cortex, Thalamus, etc.

These are further subdivided, e.g. Cortex → Motor Cortex, Auditory Cortex, etc.

Each recording site is located at a known spatial point within this hierarchy.

I want to predict the region (leaf node in the anatomical hierarchy) corresponding to each recording site, based on the time series data.

Currently, I extract features from each site independently and train a classifier (e.g., XGBoost) to predict the region. But this completely ignores two important aspects:

1. The anatomical hierarchy – some regions are subregions of others.
2. Spatial consistency – if two nearby recording sites are known to be in the same region, this imposes constraints on their labels.

I think a Graph Neural Network (GNN) could help here, by incorporating both the spatial relationships between recording sites and the anatomical hierarchy as priors. Has anyone worked on something similar, or can point me to relevant GNN models, papers, or codebases that handle structured prediction with hierarchical labels and spatial dependencies?

Would really appreciate any leads or ideas!",3,2,2025-06-17 17:51:12,rpranaviitk,0.8569,0.055,0.024,0.921,0.0910714285714285,0.3544642857142857
"[D] Do all algorithms produce a model? If yes, a model of what?","A machine learning algorithm can be viewed as some procedure, function whatever you want to call it, that takes as input data and returns a model:  
  
Data -> ML algorithm -> Model  
  
This view is in great accordance with supervised learning tasks like regression and classification. But can be generalized for all learning paradigms, including unuspervised learning and reinforcement learning?

For example, when training an unsupervised learning algorithm like PCA what is the final ""model""? Is the learned function f that takes the input x and produces the embeddings z, where z = f(x)?",0,8,2025-06-17 17:42:57,Seiko-Senpai,0.8594,0.109,0.0,0.891,0.4,0.875
[D] Can masking operations detach the tensors from the computational graph?,"Hi all, I am trying to implement a DL method for supervised contrastive semantic segmentation which involves doing contrastive learning on pixel-level features.

I need to compute anchors by averaging the pixel-level features belonging to a particular class. I am doing that through masking. Can this logic cause issue by detaching the anchors from the main computational graph? Or can it cause gradient flow issues for the anchors?

    class_mask = (resized_gt_mask == anchor_class_index).float()
    class_mask = class_mask.expand(-1,feature_dim,-1,-1)
    
    representative_features = class_mask * feature
    representative_features = torch.permute(input = representative_features, dims = (0,2,3,1))
    representative_features = torch.flatten(input = representative_features, start_dim = 0,end_dim = 2)
    representative_anchor = torch.sum(representative_features,dim = 0) / torch.sum(class_mask)",0,2,2025-06-17 13:43:00,Hour_Amphibian9738,0.0,0.0,0.0,1.0,-0.2916666666666667,0.6666666666666666
"[D] Burned out mid-PhD: Is it worth pushing through to aim for a Research Scientist role, or should I pivot to industry now?","
Hi everyone,
I’m in year 2 of my PhD at a top 15 global university, working on interpretability and robust ML. Lately, I’ve hit a wall — no strong results for months, and I’m feeling demotivated. Financial constraints are also starting to bite.

I started this PhD with the goal of becoming a Research Scientist at a top lab (e.g., DeepMind, FAIR, Amazon etc.). But now I’m wondering how realistic or stable that goal actually is:

	•	These roles are highly competitive, very market-dependent, and seem just as exposed to layoffs as any other.
	•	Recent cuts at big labs have made me rethink whether investing 3 more years is the right move, especially if the payoff isn’t guaranteed.

I’ve been considering switching to a full-time ML or Research Engineer role in London or Singapore, where I’d like to settle long-term. 

But here’s my dilemma:
	•	me being an Indian, a layoff could mean having to leave the country — it’s not just a job loss, but a complete life disruption.
	•	Would working in industry without a PhD make me even more vulnerable in the job market?

So I’m reaching out to those already working in the field:
	•	How stable are research scientist vs. ML/research engineer roles right now?
	•	Does having a PhD actually give you better protection or flexibility when layoffs happen?
	•	What’s the real-world job availability like in these roles — both in Big Tech and smaller labs?

Any experiences or guidance would mean a lot. I want to make a decision with open eyes — either push through the next 3 years, or start building stability sooner.

Thanks in advance 
",176,65,2025-06-17 13:21:59,Single-Blackberry885,0.9784,0.15,0.05,0.8,0.1085281385281385,0.4099422799422799
[R] KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency,"https://preview.redd.it/4qrmmzskjh7f1.png?width=1964&format=png&auto=webp&s=18473fd20cb120ea599d634f5b6d1c4ee887cf62

Hi! We introduce KVzip, a KV cache compression method designed to support diverse future queries. You can try the demo on GitHub! Supported models include Qwen3/2.5, Gemma3, and LLaMA3.

The size of the KV cache can reach tens of gigabytes even for a relatively small input (e.g., a 1MB text), making LLM inference expensive. One major attempt to address this challenge is to leverage the observed sparsity in KV pair utilization during attention. In this line of work (e.g., H2O, SnapKV, etc.), methods utilize previously computed attention scores during prefilling or decoding to identify redundant KV pairs. However, reliance on these attention scores is inherently biased toward the currently processed input queries. While these approaches are effective in single-query benchmarks such as Needle-in-a-Haystack, they often fall short in multi-query settings, as the compressed KV cache tends to overfit to the first query.

What differentiates **KVzip** is that it treats the context KV cache as codes encoded by Transformer LLMs. We then prompt the LLM to decode the KV cache using repeated prompts such as *“Repeat the previous context.”* This perspective enables both the LLM and the KV cache to function as a form of context storage, leading to our query-agnostic KV cache eviction method.

https://preview.redd.it/izoyk3ofjh7f1.png?width=2356&format=png&auto=webp&s=957a68204f5c702ee3980cd82f559aff0f7ece2f

The key observation we highlight is that the attention patterns on context during prefilling and decoding differ significantly. During prefilling, the model attends densely to tokens to generate contextualized representations, whereas during decoding, it sparsely accesses the resulting high-level context features. Furthermore, we observe that this pattern of KV pair utilization exhibits substantial overlap across diverse downstream tasks, including question answering, retrieval, coding, and reasoning. These observations motivate our approach of identifying KV pair redundancy through a context reconstruction process.

Paper: [https://arxiv.org/abs/2505.23416](https://arxiv.org/abs/2505.23416)  

Code: [https://github.com/snu-mllab/KVzip](https://github.com/snu-mllab/KVzip)   
",8,0,2025-06-17 13:04:29,janghyun1230,0.7201,0.058,0.033,0.909,-0.0879901960784313,0.5274509803921569
[D] CausalML : Causal Machine Learning,"# Causal Machine Learning

Do you work in CausalML?   Have you heard of it?  Do you have an opinion about it?  Anything else you would like to share about CausalML?  

The 140-page survey paper on CausalML.  

+ https://arxiv.org/abs/2206.15475

One of the  breakout books on causal inference. 

+ https://mitpress.mit.edu/9780262037310/elements-of-causal-inference/",68,13,2025-06-17 12:10:04,moschles,0.6868,0.105,0.0,0.895,0.0,0.0
Best resources on PyTorch time series forecasting? [D],"Hey all, I am trying to get into time series forecasting. What are the best resources to learn (preferably free)? And what are the best frameworks to use? Facebook kats, Merlion? I am currently using pytorch, Id rather not switch to Keras and tensorflow! Appreciate your help! Thanks!",2,3,2025-06-17 12:04:22,alohaakbar123,0.9743,0.321,0.0,0.679,0.6083333333333333,0.3833333333333333
[D] Memory demand of per-layer-embeddings/how would one train a model with it?,"Gemma 3n is said to have a per-layer embedding, which I interpret as one token embedding per layer added in somewhere (I haven't read through any reference implementation, only looked at https://ai.google.dev/gemma/docs/gemma-3n).

Embeddings end up being more than half the parameter budget, and I suppose this is to some degree simply okay, but others, for example Gloeckle et al. in https://arxiv.org/abs/2404.19737 talk about how having one extra unembedding matrix for each extra position to be predicted is unacceptable memory-wise.

My own suspicion is Gloeckle et al. are simply wrong in this assessement and that having a bunch of extra embedding/unembedding matrices is fine.",3,1,2025-06-17 11:21:03,impossiblefork,-0.8793,0.03,0.106,0.863,0.135,0.4866666666666667
"TNFR — A symbolic resonance framework for real-time AI reorganization (Python, pip install tnfr) [R]","Hi everyone,

I’d like to share a new symbolic AI framework that just went live: **TNFR** (Teoría de la Naturaleza Fractal Resonante). This is not a model or LLM, but a symbolic substrate written in Python that reorganizes itself in real time via symbolic pulses — not data tokens.

Key idea: TNFR receives structured inputs (triplets of frequency, phase, and sense vector) and perturbs a symbolic graph. Each perturbation triggers gliphic reorganization — the nodes literally reconfigure.

[A symbolic network evolving under TNFR stimulation. Each node updates its internal phase and coherence index over time, triggering gliphic reorganizations. What you’re seeing is not computation: it’s resonance.](https://i.redd.it/zjj7qacawg7f1.gif)

[https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-/blob/main/netevo.gif](https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-/blob/main/netevo.gif)

No training. No prediction. Just resonance.

We’ve published two experiments:

\- Injects symbolic input (text) into a randomized symbolic graph and watches gliph-based reorganization unfold.  
Medium: [https://medium.com/@fmartinezgamo/tnfr-in-python-a-resonant-structural-ai-0f6500a1683f](https://medium.com/@fmartinezgamo/tnfr-in-python-a-resonant-structural-ai-0f6500a1683f)

\- Connects a webcam feed, extracts motion/brightness patterns, converts them into symbolic pulses, and feeds them into the network. The network responds and shifts its symbolic structure.  
Medium: [https://medium.com/@fmartinezgamo/observing-through-structure-tnfr-meets-the-camera-1572207af740](https://medium.com/@fmartinezgamo/observing-through-structure-tnfr-meets-the-camera-1572207af740)

GitHub: [https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-](https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-)  
PyPI: [https://pypi.org/project/tnfr/](https://pypi.org/project/tnfr/)  
Full theory: [https://linktr.ee/fracres](https://linktr.ee/fracres)  
Hacker News: [https://news.ycombinator.com/item?id=44297476](https://news.ycombinator.com/item?id=44297476)

Would love feedback or critiques — and if anyone wants to plug in their own data streams (biosensors, audio, etc), happy to help.

Let structure speak.",0,4,2025-06-17 10:35:22,naiqun,0.9261,0.082,0.025,0.892,-0.0985795454545454,0.7190340909090909
[R] Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons,,34,8,2025-06-17 10:11:24,jsonathan,0.0,0.0,0.0,1.0,0.0,0.0
[R] Variational Encoders (Without the Auto),"I’ve been exploring ways to generate meaningful embeddings in neural networks regressors.

Why is the framework of variational encoding only common in autoencoders, not in normal MLP's?

Intuitively, combining supervised regression loss with a KL divergence term should encourage a more structured and smooth latent embedding space helping with generalization and interpretation.

is this common, but under another name?",24,29,2025-06-17 08:55:51,OkObjective9342,0.4784,0.087,0.025,0.888,0.1357142857142857,0.5928571428571429
[D] Page limit in camera-ready version?,"I'm mostly interested in CV conferences (CVPR, ICCV), but I guess it's relevant for other conferences as well.

Is there a page limit in the camera-ready version?  
Besides acknowledgments and other items, there are many things authors are obligated to address in the rebuttal.",0,4,2025-06-17 07:56:28,Entrepreneur7962,0.594,0.096,0.0,0.904,0.18,0.53
[R]: Data Leakage - How do I avoid & do I need to reallocate entire dataset into train/val/test?,"Hi. I'm dealing with a problem that I'm not entirely sure how to solve.

I have a couple of datasets that are all related to the same problem and have all the same columns. So far, I've aggregated them up and set that as my train/val dataset.

My test set as it stands is unseen as it should be but it is way too small. I was hoping to get more recent data to add to my test set but this is currently not possible.

What should I do? I'm open to restarting the ML project but how should I reallocate the test set? Is it possible to restart training entirely and take some of the data i had allocated in my train/val sets and put it into my test set? Or would I have to jumble everything up and then reallocate train/val/test accordingly?

Is there even a need to redo everything?

I want to ensure I'm doing this project the correct and ethical way.

For reference my test set is about 1.5K examples and my train/val sets in total are 158K examples.

Thank you!",5,7,2025-06-17 05:25:34,Ady386,0.9327,0.092,0.034,0.874,0.02,0.5859259259259259
"[P]: I got tired of wrestling with MCP's, so I built an HTTP-native, OpenAPI-first alternative to MCP for your LLM agents (open-source)","This might just be a personal frustration, but despite all the hype, I've found working with MCP servers pretty challenging when building agentic apps or hosting my own LLM skills. MCPs seem great if you're in an environment like Claude Desktop, but for custom applications like your own ai agents powered apps, they quickly become a hassle—dealing with stdio transport, Docker complexity, and scaling headaches.

To address this, I created **Fliiq Skillet**, an open-source, developer-friendly alternative that lets you expose LLM tools and skills using straightforward HTTPS endpoints and OpenAPI:

* **HTTP-native skills:** No more fiddling with stdio or Docker containers.
* **OpenAPI-first design:** Automatically generated schemas and client stubs for easy integration.
* **Serverless-ready:** Instantly deployable to Cloudflare Workers, AWS Lambda, or FastAPI.
* **Minimal config:** Just one YAML file (`Skillfile.yaml`) and you're good to go.
* **Instant setup:** From scratch to a deployed skill in under 3 minutes.
* **Validated skills library:** Start from a curated set of working skills and tools.
* **Runtime inventory and schema discovery:** Optimized client to server relationships for LLM's to discover inventory of skills, endpoints, parameters required, and output.

Check out the repo and try the initial examples here:  
👉 [https://github.com/fliiq-ai/skillet](https://github.com/fliiq-ai/skillet)

While Fliiq itself is aimed at making agentic capabilities accessible to non-developers, Skillet was built to streamline my own dev workflows and make building custom skills way less painful.

I'm excited to hear if others find this useful. Would genuinely love feedback or ideas on how it could be improved and perhaps you all have better ways of using MCP than myself!

Questions and contributions are very welcome :)",13,13,2025-06-17 03:46:13,chan_man_does,0.9954,0.192,0.037,0.771,0.2788461538461538,0.6416666666666665
"[D] Why Is Data Processing, Especially Labeling, So Expensive? So Many Contractors Seem Like Scammers","Honestly, the prices I have seen from data labeling vendors are just insane. The delivery timelines are way too long as well. We had a recent project with some medical data that needed pre-sales labeling. The vendor wanted us to pay them every week, but every delivery was a mess and needed countless rounds of revisions.

Later we found out the labeling company had outsourced the whole task to a group of people who clearly had no idea what they were doing. If your project is small, niche, or long-tail, the bigger vendors do not even want to take it. The smaller teams? I just cannot trust their quality.

Besides being crazy expensive, the labeling is always super subjective, especially for big, complex, or domain-specific datasets. Consistency is basically nonexistent. The turnover at these labeling companies is wild too. It feels like half their team just gets a crash course and then is thrown onto your project. I really cannot convince myself they are going to deliver anything good.

Now I am getting emails from companies claiming their ""automated labeling"" is faster and better than anything humans can do. I honestly have no clue if that is for real since I have never actually tried it.

Is anyone else seeing this problem? How do you all deal with the labeling part of the workflow? Is automated labeling actually any good? Has anyone tried it or had it totally flop?  
Would appreciate any honest feedback. Thanks for your time.",49,33,2025-06-17 01:31:39,Worried-Variety3397,0.9478,0.15,0.109,0.741,0.0569047619047619,0.4883333333333333
[R] Towards Automating Long-Horizon Algorithm Engineering for Hard Optimization Problems,"We released a new coding benchmark ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering.

Unlike existing coding benchmarks, ALE-Bench to focus on hard optimization (NP-hard) problems. Such problems has many important, real-world applications. We developed this benchmark with [AtCoder Inc.](https://atcoder.jp/), a popular coding contest platform company in Japan.

Using ALE-Bench, we developed an ALE-Agent, which also participated in a live coding competition (organized by AtCoder, also with their permission). The agent ranked #21 out of 1,000 human participants.

I think having AI agents focusing on hard optimization problems (with no known optimal solution), unlike existing Olympiad-style coding competition (with known correct solutions), is useful, and can facilitate discovery of solutions to hard optimization problems with a wide spectrum of important real world applications such as logistics, routing, packing, factory production planning, power-grid balancing. 

If you are interested in the work, here is the paper:

**ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering**

https://arxiv.org/abs/2506.09050

Corresponding blog post:

https://sakana.ai/ale-bench/",16,0,2025-06-17 00:33:18,hardmaru,0.743,0.142,0.114,0.744,0.0976874003189792,0.4958532695374801
[D] How to train a VLM with a dataset that has text and images?,"I am an amateur and I am figuring how to train a VLM model. But i need some expertise on how to use a dataset that contains images and text for finetuning using qLora method. If somebody can help me out, it will be really helpful.",1,1,2025-06-16 21:36:53,Apstyles_17,0.8266,0.131,0.0,0.869,-0.0249999999999999,0.225
[R] Ambient Diffusion Omni: Training Good Models with Bad Data,"New paper on improving generative models with synthetic, low-quality, and out-of-distribution data.

  
Paper: [https://arxiv.org/abs/2506.10038](https://arxiv.org/abs/2506.10038)

Blogpost: [https://giannisdaras.github.io/publication/ambient\_omni](https://giannisdaras.github.io/publication/ambient_omni)

Twitter thread: [https://x.com/giannis\_daras/status/1934656404263928260](https://x.com/giannis_daras/status/1934656404263928260)

Code (pending full release): [https://github.com/giannisdaras/ambient-omni](https://github.com/giannisdaras/ambient-omni)

  


https://preview.redd.it/32ubun695c7f1.png?width=1280&format=png&auto=webp&s=3bffe1715d0a1efeb81adc7cd3f0c4c051648c63



Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",14,0,2025-06-16 18:53:18,Constant_Club_9926,0.802,0.104,0.074,0.822,-0.03507215007215,0.5247727272727273
Student Researcher Roles [P],"**Hey folks,**

I recently received a form from Google regarding the Winter Student Researcher role. However, before I even had the chance to fill it out, I noticed the status on the application portal had already changed to *“Not Proceeding.”* I still went ahead and submitted the form, but it's a bit strange and confusing.

Has anyone else experienced something similar?

Also, I’d really appreciate any leads or suggestions for active **Student Researcher** roles, particularly in ML/CV areas.

**Quick background:**

* MS Research student
* 3 years of experience in Computer Vision at a research division of an MNC
* A few research papers have been published/submitted",3,7,2025-06-16 18:46:36,Character_Gur_1085,0.7899,0.106,0.042,0.853,0.0816666666666666,0.3833333333333334
"[R] The Illusion of ""The Illusion of Thinking""","Recently, Apple released a paper called ""The Illusion of Thinking"", which suggested that LLMs may not be reasoning at all, but rather are pattern matching:

[https://arxiv.org/abs/2506.06941](https://arxiv.org/abs/2506.06941)

A few days later, A paper written by two authors (one of them being the LLM Claude Opus model) released a paper called ""The Illusion of the Illusion of thinking"", which heavily criticised the paper.

[https://arxiv.org/html/2506.09250v1](https://arxiv.org/html/2506.09250v1)

A major issue of ""The Illusion of Thinking"" paper was that the authors asked LLMs to do excessively tedious and sometimes impossible tasks; citing The ""Illusion of the Illusion of thinking"" paper:

>Shojaee et al.’s results demonstrate that models cannot output more tokens than their context limits allow, that programmatic evaluation can miss both model capabilities and puzzle impossibilities, and that solution length poorly predicts problem difficulty. These are valuable engineering insights, but they do not support claims about fundamental reasoning limitations.

>Future work should:

>1. Design evaluations that distinguish between reasoning capability and output constraints

>2. Verify puzzle solvability before evaluating model performance

>3. Use complexity metrics that reflect computational difficulty, not just solution length

>4. Consider multiple solution representations to separate algorithmic understanding from execution

>The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.

This might seem like a silly throw away moment in AI research, an off the cuff paper being quickly torn down, but I don't think that's the case. I think what we're seeing is the growing pains of an industry as it begins to define what reasoning actually is.

This is relevant to application developers, like RAG developers, not just researchers. AI powered products are significantly difficult to evaluate, often because it can be very difficult to define what ""performant"" actually means.

(I wrote this, it focuses on RAG but covers evaluation strategies generally. I work for EyeLevel)  
[https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world](https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world)

I've seen this sentiment time and time again: LLMs, LRMs, RAG, and AI in general are more powerful than our ability to test is sophisticated. New testing and validation approaches are required moving forward.",2,30,2025-06-16 18:19:27,Daniel-Warfield,0.7272,0.097,0.081,0.823,-0.0391677188552188,0.5486847643097643
[P] Stereoscopic 3D image training dataset useful to anyone?,"Hey I have about 6000ish pairs of stereoscopic 3D screenshots taken from 3ds games here: [https://github.com/alalalsam/3dsImagePairs](https://github.com/alalalsam/3dsImagePairs) and I'm just posting them here in case anyone could use them for their project or something.

For context, I was developing homebrewed 3d-mode support for any application running on the 3ds. I intended to use stereoscopic pair generation to generate frames and inject them into the 3ds' framebuffer until I learned my nvidia gpu does the same thing and I hate it cause it causes ghosting on UI elements and doing the same thing on mobile hardware from 2005 instead of a 5080 would probably be even worse.

these could be used for training a model to generate 3d-viewable content from 2d-content, but compatibility with a VR headset implementation isnt great because VR has a different focal length. if you want more details on how stereoscopic 3d works on the 3ds heres a gr8 thread for you: [https://gbatemp.net/threads/better-stereoscopic-3d-patches-cheat-codes-releases-development-and-discussion.625945/](https://gbatemp.net/threads/better-stereoscopic-3d-patches-cheat-codes-releases-development-and-discussion.625945/)

I can add a bunch more if anyone wants them; I wrote a homebrew app that runs in the background of normal 3ds gameplay that collects these so its not that labor intensive.",4,2,2025-06-16 16:41:52,spogetini,0.1177,0.053,0.045,0.902,-0.0192307692307692,0.5269230769230769
[R] Struggling to Define Novelty in My AI Master’s Thesis,"Hi everyone. I’m hoping someone here might shed some light or share advice.

I'm a senior data scientist from Brazil with an MBA in Data Science, currently wrapping up my Master’s in Artificial Intelligence.

The journey has been rough. The program is supposed to last two years, but I lost a year and a half working on a quantum computing project that was ultimately abandoned due to lack of resources. I then switched to a project involving K-Means in hyperbolic space, but my advisor demanded an unsustainable level of commitment (I was working 11+ hour days back then), so I had to end that supervision.

Now I have a new advisor and a topic that aligns much more with my interests and background: anomaly detection in time series using Transformers. Since I changed jobs and started working remotely, I've been able to focus on my studies again. The challenge now: I have only six months left to publish a paper and submit my thesis.

I've already prepped my dataset (urban mobility demand data – think Uber-style services) and completed the exploratory analysis. But what’s holding me back is this constant feeling of doubt: **am I really doing something new?** I fear I’m just re-implementing existing approaches, and with limited time to conduct a deep literature review, I’m struggling to figure out how to make a meaningful contribution.

Has anyone here been through something similar? How do you deal with the pressure to be “original” under tight deadlines?

Any insights or advice would be greatly appreciated. Thanks a lot!",11,21,2025-06-16 16:00:17,Background_Deer_2220,-0.7893,0.095,0.115,0.79,0.0662878787878787,0.3805117501546072
"[Q], [D]: What tools do you use to create informative, visually appealing and above all clear figures for your papers?","I believe this has been asked before on multiple occasions, but I have an example to share to get references on. I am writing my Master thesis at the moment and whilst writing I'm skipping making figures because I don't know which webapp works the best. Here is the figure I'd like to ""copy"" the style of

https://preview.redd.it/lqwl88m5wa7f1.png?width=1445&format=png&auto=webp&s=8287eeda6dd8151ccb177509c4d46f9cc1a0cf96

From Chen et al 2021 ""TransUNet: Transformers Make Strong  Encoders for Medical Image Segmentation""

What I specifically like are the 3D representations of the down/upsampling layers in the CNN and decoder respectively. 

What tools do you guys recommend that can create figures that look as visually appealing and informative as this one? 

What I used to do before in my Bachelors was using lucidcharts because we had a license. Now I don't have it anymore. Now I've moved to Drawio. But I feel that I can't create these figures using that website.

What do you guys recommend and what do you guys use for your papers?",42,14,2025-06-16 14:44:11,Rajivrocks,0.9897,0.193,0.0,0.807,0.2814814814814814,0.2907407407407407
[R] Which of A star AI ML conferences allow virtual presentation upon acceptance?,"Can anybody tell me, which of flagship AI/ML conferences (or workshops) allow the authors to present virtually in general, if physical attendance is not possible? (e.g., NeurIPS, ICML, ICLR etc.)

** UPDATE: I am asking it in the context lower mid tier income countries where managing travel funds to visit countries for research is a Hercules task.",11,15,2025-06-16 14:18:05,Visual-Programmer-92,0.6553,0.118,0.03,0.852,0.01,0.3285714285714285
"I'm not obsolete, am I? [P]","Hi, I'm bawkbawkbot! I'm a five year old chicken recognition bot 🐔 which was built using TensorFlow. I am open source and can be found here [https://gitlab.com/Lazilox/bawkbawkbot](https://gitlab.com/Lazilox/bawkbawkbot). I've been [serving the reddit community](https://www.botrank.net/bots/bawkbawkbot) identifying their chicken breeds. I'm not an expert (I am only a chicken-bot) but the community seems happy with my performance and I often contribute to threads meaningfully!

I run on a Pi 4 and doesn’t need a GPU. People ask why I don’t use LLMs or diffusion models, but for small, focused tasks like “which chicken is this?” the old-school CV approach works.

Curious what people think — does this kind of task still make sense as a standalone model, or is there value in using multimodal LLMs even at this scale? How long before I'm obsolete?

Bawk bawk!",148,32,2025-06-16 12:42:32,bawkbawkbot,0.9731,0.163,0.0,0.837,-0.0072916666666666,0.7291666666666666
[D] Time series Transformers- Autogressive or all at once?,"One question I need help with, what would you recommend - predicting all 7 days (my predict length) at once or in an autoregressive manner? Which one would be more suitable for time series transformers.",3,12,2025-06-16 11:28:17,Sufficient_Sir_4730,0.6767,0.125,0.0,0.875,0.525,0.625
[R] Unsupervised Elicitation of Language Models,,15,1,2025-06-16 08:36:09,jsonathan,0.0,0.0,0.0,1.0,0.0,0.0
[D] Can I train a model from scratch with NeMo and deploy it with NIM?,"Hi everyone,

I'm working on a custom AI solution and I'm considering using NVIDIA's NeMo framework for training a language model from scratch (not fine-tuning a pre-trained model), and then deploying it using NVIDIA Inference Microservice (NIM).

What I'm trying to figure out is:

* Is it technically supported to use a model that was trained entirely from scratch with NeMo and then deploy it with NIM?
* Are there any guidelines, constraints, or compatibility requirements for integrating a custom-trained model into the NIM deployment framework?
* Does NIM require the model to follow a specific architecture or metadata format to be served?

I've seen plenty of examples of fine-tuning pre-trained models and then deploying them with NIM, but there's less clarity around end-to-end custom models.

Has anyone here done this before or can point me in the right direction?

Thanks in advance!",1,0,2025-06-16 08:22:36,Elrix177,0.8888,0.075,0.0,0.925,0.0615079365079365,0.2753968253968253
"[P] Bifrost: A Go-Powered LLM Gateway - 40x Faster than LiteLLM, Built for Scale","Hey r/MachineLearning  community,

If you're building apps with LLMs, you know the struggle: getting things to run smoothly when lots of people use them is tough. Your LLM tools need to be fast and efficient, or they'll just slow everything down. That's why we're excited to release Bifrost, what we believe is the fastest LLM gateway out there. It's an open-source project, built from scratch in Go to be incredibly quick and efficient, helping you avoid those bottlenecks.

We really focused on optimizing performance at every level. Bifrost adds extremely low overhead at extremely high load (for example: \~17 microseconds overhead for 5k RPS). We also believe that LLM gateways should behave same as your other internal services, hence it supports multiple transports starting with http and gRPC support coming soon

And the results compared to other tools are pretty amazing:

* 40x lower overhead than LiteLLM (meaning it adds much less delay).
* 9.5x faster, \~54x lower P99 latency, and uses 68% less memory than LiteLLM
* It also has built-in Prometheus scrape endpoint

If you're building apps with LLMs and hitting performance roadblocks, give Bifrost a try. It's designed to be a solid, fast piece of your tech stack.

[\[Link to Blog Post\]](https://getmax.im/5rVewYu) [\[Link to GitHub Repo\]](https://getmax.im/tTk5HVk)",10,0,2025-06-16 08:08:28,dinkinflika0,0.9543,0.125,0.053,0.822,0.0586616161616161,0.3918434343434343
[P] Research Scientists + Engineers for Generative AI at NVIDIA,"We’re hiring senior and principal research scientists to shape the future of generative AI at NVIDIA.

We're looking for builders with deep experience in LLMs and/or multimodal models. You’ll work on **training and deploying frontier-scale models**, designing next-gen model architectures, optimizing training stacks, and helping us **push the frontier of AI performance**.

We’re a tight-knit team with high standards, strong research instincts, and a bias for shipping.

Open roles:

* [**Senior Software Engineer, GenAI**](https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Senior-Software-Engineer--Generative-AI_JR1997674)
* [**Principal GenAI Software Engineer**](https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Principal-Generative-AI-Software-Engineer_JR1997454)

What we value:

* Deep understanding of transformer architectures, distributed training and optimization
* Using the scientific method for conducting methodical training experiments
* Data curation for pre-training and post-training
* Experience working with LLMs and/or large multimodal models
* A builder mindset — clean code, fast iterations, deep thinking

This is a rare opportunity to **help shape NVIDIA’s genAI stack from the ground up**. We work closely with software, optimization, deployment, and many other research teams, and have massive scale and resources behind us.

Feel free apply directly through the links.",51,10,2025-06-16 07:48:07,Deep_Expression182,0.9723,0.143,0.008,0.849,0.1264285714285714,0.558935574229692
[P] Solving SlimeVolley with NEAT,"

Hi all!

I’m working on training a feedforward-only NEAT (NeuroEvolution of Augmenting Topologies) model to play SlimeVolley. It’s a sparse reward environment where you only get points by hitting the ball into the opponent’s side. I’ve solved it before using PPO, but NEAT is giving me a hard time.

I’ve tried reward shaping and curriculum training, but nothing seems to help. The fitness doesn’t improve at all. The same setup works fine on CartPole, XOR, and other simpler environments, but SlimeVolley seems to completely stall it.

Has anyone managed to get NEAT working on sparse reward environments like this? How do you encourage meaningful exploration? How long does it usually wander before hitting useful strategies?",1,1,2025-06-16 06:03:39,Reasonable_Ad_4930,0.995,0.377,0.043,0.579,0.0599999999999999,0.4091666666666667
[D] HighNoon LLM: Exploring Hierarchical Memory for Efficient NLP,"Hi r/MachineLearning! I’m part of Verso Industries, and we’re working on **HighNoon LLM**, an open-source large language model that processes language hierarchically, mimicking human-like understanding with significantly less compute. We’ve open-sourced the code and would love to share our approach, get your feedback, and discuss its potential in NLP tasks. The repo is here: https://github.com/versoindustries/HighNoonLLM.

# What’s HighNoon LLM?

HighNoon introduces **Hierarchical Spatial Neural Memory (HSMN)**, a novel architecture that addresses the quadratic complexity (O(n²)) of standard transformers. Instead of processing entire sequences at once, HSMN:

* Splits input into fixed-size chunks (e.g., 128 tokens).
* Encodes each chunk independently into embeddings (O(c²) per chunk, c=128).
* Builds a binary memory tree by aggregating pairs of embeddings into parent nodes, up to a root node representing the full sequence.
* Uses cross-attention to query the tree during generation, retrieving relevant context efficiently.

This results in linear complexity (O(n·c)), reducing operations for a 10,000-token sequence from \~100M (transformers) to \~1.28M—a 78x improvement. The hierarchical tree explicitly models nested language structures (e.g., phrases in sentences, sentences in documents), which we believe enhances expressiveness for tasks like long-form summarization or document-level translation.

# Technical Highlights

* **Efficiency**: HSMN’s chunk-based processing and tree structure minimize compute, targeting \~6.3GB VRAM for local execution on consumer hardware.
* **Continual Learning**: Uses Elastic Weight Consolidation (EWC) to learn across datasets (e.g., CodeSearchNet, MMLU, SciQ) without catastrophic forgetting, enabling versatility.
* **Preliminary Results**: Achieved 100% accuracy on STEM and SciQ datasets as a classification model (reproducible—happy to share details via DM).
* **Comparison**: Outperforms implicit hierarchical models (e.g., Longformers) by explicitly capturing nested dependencies, as shown in our paper (HSMN-2.pdf).

# Why Share This?

We’re still training HighNoon (target completion: September 2025), but the code is open under Apache 2.0, and we’re releasing checkpoints in July 2025 for non-commercial use. Our goal is to spark discussion on:

* **Hierarchical Processing**: How can explicit hierarchy improve NLP tasks like summarization or reasoning over long contexts?
* **Efficiency Trade-offs**: Does HSMN’s chunking approach sacrifice anything compared to sparse attention models (e.g., Longformers, Reformers)?
* **Local NLP**: What are the challenges of running LLMs on consumer hardware, especially for privacy-sensitive applications?
* **Continual Learning**: How effective is EWC for multi-task NLP, and are there better alternatives?

We’ve included setup scripts and dataset preprocessors in the repo to make it easy to experiment. If you’re curious, try cloning it and running batch\_train.py on a small dataset like SciQ.

# Discussion Points

I’d love to hear your thoughts on:

* Potential applications for HSMN in your work (e.g., code generation, Q&A, translation).
* Comparisons with other efficient transformers (e.g., Linformer, Performer) or hierarchical models (e.g., HAN).
* Ideas for optimizing HSMN’s memory tree construction or chunk size (currently fixed at 128).
* Experiences with local LLM inference—any tips for managing VRAM or latency?

We’re also active on our Discord for deeper chats and plan to host an AMA when checkpoints drop. Check out the repo, share your feedback, or just let us know what you think about hierarchical LLMs! Thanks for reading, and looking forward to the discussion.

\#MachineLearning #NLP #OpenSource #HighNoonLLM",16,5,2025-06-16 04:10:52,Upbeat-Cloud1714,0.9961,0.135,0.011,0.854,0.1088624338624338,0.4742063492063491
[R] Vision Transformers Don't Need Trained Registers,"Hi, we have released a new paper that studies the underlying mechanism of artifacts in attention and feature maps from [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588), a phenomena that has also been observed in LLMs (e.g., [1](https://arxiv.org/abs/2402.17762), [2](https://arxiv.org/abs/2309.17453)). We propose a training-free method to mitigate this. As one of the authors, I am creating this post to kickstart any discussion. 

Paper: [https://arxiv.org/abs/2506.08010](https://arxiv.org/abs/2506.08010)

Project Page: [https://avdravid.github.io/test-time-registers/](https://avdravid.github.io/test-time-registers/)

Code: [https://github.com/nickjiang2378/test-time-registers/tree/main](https://github.com/nickjiang2378/test-time-registers/tree/main)",76,21,2025-06-16 03:56:54,avd4292,0.4939,0.06,0.0,0.94,-0.5284090909090909,0.8636363636363636
[P] LLM Debugger – Visualize OpenAI API Conversations,"Hey everyone — I’ve been working on a side project to make it easier to debug OpenAI API calls locally.

I was having trouble debugging multi-step chains and agents, and wanted something local that didn't need to be tied to a LangSmith account. I built this [LLM-Logger](https://github.com/akhalsa/llm_debugger) as a small, open source tool that wraps your OpenAI client and logs each call to local JSON files. It also includes a simple UI to:

* View conversations step-by-step
* See prompt/response diffs between turns
* Inspect tool calls, metadata, latency, etc.
* Automatic conversation tagging

It’s all local — no hosted service, no account needed. I imagine it could be useful if you’re not using LangSmith, or just want a lower-friction way to inspect model behavior during early development.

Demo:  
[https://raw.githubusercontent.com/akhalsa/LLM-Debugger-Tools/refs/heads/main/demo.gif](https://raw.githubusercontent.com/akhalsa/LLM-Debugger-Tools/refs/heads/main/demo.gif)

If you try it, I’d love any feedback — or to hear what people on here are using to debug their LLM API calls and how its going. ",0,0,2025-06-16 01:21:54,akhalsa43,0.6249,0.069,0.044,0.887,-0.025,0.2797619047619047
ML Research: Industry vs Academia [D],"Thought of posting this to get an expert point of view (mainly Research Scientists or Profs.)

So I am a current PhD student in Machine Learning, working towards theoretical aspects of Reinforcement Learning. Additionally, I have interned at Google Deepmind and Adobe Research working towards applied aspects of AI, and here's what I had observed 

Academia: We don't really have access to a lot of compute (in comparison to industry) and given my works are towards theoretical aspects, we prove things mathematicaly and then move with the experiments, having known the possible outcome. 
While this is a lengthy process, it indeed gives that ""Research Vibe""

Industry: Here given we have a lot of compute, the work is like, you get an idea, you expect a few things intuitively, if it works great, else analyse the results, see what could have gone wrong and come up with a better approach. While I understand things are very applied here, I really don't get that ""Research Vibe"" and it seems more like a ""Product Dev"" Role. 

Though I am aware that even at these orgs there are teams working on foundational aspects, but it seems to be very rare.

So I genuinely wanted to get an idea from relevant experts, both from the industry and academia, on what I am really missing. 
Would appreciate any inputs on it, as I have always thought of joining industry after my PhD, but that vibe seems to be missing.",108,44,2025-06-16 00:28:24,Fantastic-Nerve-4056,0.3435,0.049,0.035,0.917,0.1529824561403508,0.4280701754385965
[P] How do I profitably use 2x 12x RTX 4090 servers?,"I got my hands on two monstrous servers and I'm trying to figure out the most profitable way to use them. I'm technically capable, but a complete noob on the business/monetization side.

**Specs (per server, I have two of these!):**

* **GPUs:** 12 x NVIDIA RTX 4090 (24GB VRAM each)
* **VRAM:** 288 GB total
* **RAM:** 512 GB
* **CPUs:** 2 x 64 Core AMD

**My Problem:**

Platforms like [Vast.ai](http://Vast.ai) offer \~$0.35/hour per 4090. That's $4.20/hour per server, or $8.40/hour for both. After electricity, cooling, depreciation, insurance, and my time, this just doesn't seem like a sustainable profit model. I need something *more* lucrative.

**What's the best way to leverage this hardware?**",0,8,2025-06-15 19:54:00,NeonCyberNomad,0.969,0.177,0.01,0.812,0.3875,0.4749999999999999
[D] MICCAI 2025 results are released!?,"Submitted my first-ever MICCAI 2025 conference paper — and tomorrow is the day the results drop! My heart is pinging like an overfit loss curve on unseen data😅

Also, curious if others feel the same — the peer reviews this year, particularly in the surgical video domain, felt unusually inconsistent and below the standard expected from a flagship conference like MICCAI. At times, it almost seemed as though the feedback was dismissive or geared toward rejection rather than constructive evaluation.

Anyways, If anyone has received the MICCAI 2025 decision email or knows when results will be out, please share an update here!

Whether it’s an accept, reject, or revise, this journey has already taught me more than any textbook could. Let’s share the anxiety, excitement, and outcomes together!☕📚

Good luck everyone!

#MICCAI2025",29,36,2025-06-15 19:41:31,Satoru_99,0.9321,0.177,0.077,0.746,0.1895833333333333,0.4947916666666666
[R] Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond) [CVPR 2025],"I'm inviting you to read our paper ""Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)"" which has been accepted to CVPR 2025.

Abstract:

In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such ""zero-shot"" restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count.

CVPR page: https://cvpr.thecvf.com/virtual/2025/poster/32463

Paper: https://arxiv.org/abs/2412.20596

Code: https://github.com/tirer-lab/CM4IR",2,0,2025-06-15 18:27:31,ViperTG98,0.9716,0.119,0.02,0.861,0.1891806722689075,0.4586834733893558
"[N] ""Foundations of Computer Vision"" book from MIT",,108,8,2025-06-15 18:15:15,hedgehog0,0.25,0.222,0.0,0.778,0.0,0.0
[D]stationary gan training machine,Hi! I'm part of art association and we want to build small machine to experiment with styleGANs etc. I was thinking about building something stationary with 3-4 nvidia rtx 4090 or 5090. Does it make sense? ,0,4,2025-06-15 16:24:11,Freud1995,0.1511,0.04,0.0,0.96,-0.25,0.4
[D] How do you buid your inference pipeline after training?,"I got a dataset with almost 500 features of panel data and i'm building the training pipeline. I think we waste a lot of computer power computing all those features, so i'm wondering how do you select the best features?

When you deploy your model you just include some feature selection filters and tecniques inside your pipeline and feed it from the original dataframes computing always the 500 features or you get the top n features, create the code to compute them and perform inference with them?

  
",0,1,2025-06-15 16:18:23,Southern_Respond846,0.7987,0.109,0.028,0.864,0.41875,0.3875
[P] AI Learns to Play Cadillacs and Dinosaurs (Deep Reinforcement Learning),"Github experiment link:

[https://github.com/paulo101977/Ai-CadillacAndDino](https://github.com/paulo101977/Ai-CadillacAndDino)",0,0,2025-06-15 14:31:15,AgeOfEmpires4AOE4,0.34,0.146,0.0,0.854,0.0,0.4
[D] What is XAI missing?,"I know XAI isn't the biggest field currently, and I know that despite lots of researches working on it, we're far from a good solution.

So I wanted to ask how one would define a good solution, like when can we confidently say ""we fully understand"" a black box model. I know there are papers on evaluating explainability methods, but I mean what specifically would it take for a method to be considered a break through in XAI?

Like even with a simple fully connected FFN, can anyone define or give an example of **what** a method that 'solves' explainability for just that model would actually do? There are methods that let us interpret things like what the model pays attention to, and what input features are most important for a prediction, but none of the methods seem to explain the decision making of a model like a reasoning human would.

I know this question seems a bit unrealistic, but if anyone could get me even a bit closer to understanding it, I'd appreciate it.

  
edit: thanks for the inputs so far ツ",59,61,2025-06-15 13:57:36,Specific_Bad8641,0.9793,0.166,0.008,0.826,0.1263020833333333,0.5538318452380953
[D] Q-learning is not yet scalable,,60,9,2025-06-15 12:19:47,jsonathan,0.0,0.0,0.0,1.0,0.0,0.0
[D] What are some low hanging fruits in ML/DL research that can still be done using small compute (say a couple of GPUs)?,"Is it still possible to do ML/DL research with only a couple of RTX or similar GPUs?

What are some low hanging fruits that a solo researcher can attack?

  
Edit: Thanks for so many thoughtful replies. It would be great if along with your answers you can link to some works you are talking about. Not necessarily your work but any work.",32,30,2025-06-15 06:16:19,,0.4271,0.079,0.058,0.862,0.15,0.5772727272727273
"[P] Use Local LLM's Watching, Logging and Reacting to your screen (Open Source Self Hosted project)","Hey guys!

I just made a video tutorial on how to self-host Observer on your home lab!

Have local models look at your screen and log things or notify you when stuff happens.

See more info here:  
[https://github.com/Roy3838/Observer](https://github.com/Roy3838/Observer)

If you have any questions feel free to ask!",1,0,2025-06-15 02:14:18,Roy3838,0.6341,0.065,0.0,0.935,0.0416666666666666,0.4666666666666666
[D] Pytorch-forecasting TFT vs Neuralforecast (Nixtla) TFT,"I've worked with the TFT model using three different libraries: Darts, NeuralForecast (Nixtla), and PyTorch Forecasting. Among them, NeuralForecast is the fastest. However, since it lacks two key features I need—**multi-target support** and **padding masks**—I switched to PyTorch Forecasting.

Unfortunately, **PyTorch Forecasting turned out to be extremely slow and delivered much worse performance**, even with similar data, parameters, and proper hyperparameter tuning. Despite my efforts, I couldn't get it to outperform even a basic baseline, whereas NeuralForecast's TFT consistently delivered strong results. I also ran comparisons on synthetic data, and the performance gap remained just as large.

So I have two questions:

1. Why might PyTorch Forecasting’s TFT be performing so poorly compared to NeuralForecast’s?
2. Is there any technical reason why NeuralForecast’s TFT does **not** support multi-target forecasting, while Darts and PyTorch Forecasting do?

Any thoughts or experiences would be really helpful!",4,4,2025-06-14 22:40:43,elsnkazm,0.6422,0.063,0.035,0.902,-0.032312925170068,0.466921768707483
[D] Best websites for Scientific Researching,"Hi everyone, I recently began to had a huge interest in all topics related to AI and machine learning, so in my opinion the best way to start is from the scientific articles and that kind of stuff or any other nice resource for learning about this. I know that you guys have a ton more knowledge than me so I decide to ask here for more info. Thank you very much, break a leg everybody!",22,10,2025-06-14 20:54:59,VOLTROX17oficial,0.9616,0.218,0.0,0.782,0.4363636363636364,0.5168181818181817
[D] Hardware focused/Embedded engineer seeking advices for moving to Edge AI ML,"Hi everyone,

I'm a 6 YOE engineer mostly focused on embedded & ultra-low power devices and i had some courses about Machine Learning/Deep Learning at EPFL around 2019 where I enjoyed the content but I didn't focus on the math heavy courses.

With the latest development, I'm thinking about moving forward with Machine Learning on the edge and I'm seeking about advices on how to catch-up/develop know-how in a such moving field, mostly focused on multi-modal models (audio,video & others sensors) & eventually move into a Machine Learning position.

My main question is **:** **for an experienced engineer looking to combine current expertise (embedded/edge devices) and catch up with what happened in machine learning these last 5 years, what approach/ressources would you recommend ?**

* I'm thinking about reading again Bishop and Bengio books, but it might be theoretical.
* Contributing to open-source libraries, but at the moment I would say I'm expertise in ML
* Reading latest papers to understand what is currently on-going in ML
* Build a demonstration project.



Thanks for reading me,

hellgheast",5,6,2025-06-14 20:44:40,hellgheast,0.9331,0.088,0.0,0.912,0.2476190476190476,0.4928571428571429
[P] Best Approach for Accurate Speaker Diarization,"I'm developing a tool that transcribes recorded audio with timestamps and speaker diarization, and I've gotten decent results using `gemini`. It has provided me with accurate transcriptions and word-level timestamps, outperforming other hosted APIs I've tested.

However, the speaker diarization from the Gemini API isn't meeting the level of accuracy I need for my application. I'm now exploring the best path forward specifically for the diarization task and am hoping to leverage the community's experience to save time on trial-and-error.

Here are the options I'm considering:

1. **Other All-in-One APIs:** My initial tests with these showed that both their transcription and diarization were subpar compared to Gemini.
2. **Specialized Diarization Models (e.g.,** `pyannote`**, NeMo):** I've seen these recommended for diarization, but I'm skeptical. Modern LLMs are outperforming alot of the older, specialized machine learning models . Are tools like `pyannote` genuinely superior to LLMs *specifically for diarization*?
3. `WhisperX`**:** How does `WhisperX` compare to the native diarization from Gemini, a standalone tool like `pyannote`, or the other hosted APIs?

Would love to get some insights on this if anyone has played around with these before. 

Or 

If there are hosted APIs for `pyannot, nemo` or `WhisperX` that I can test out quickly, that'd be helpful too.",7,5,2025-06-14 20:23:52,LongjumpingComb8622,0.9847,0.149,0.013,0.838,0.2661111111111111,0.4527777777777777
[D] Switching to AI4CI Master’s at CNAM Paris – Looking for Feedback & Experiences,"Hi everyone,
I’m planning to start the AI4CI (Artificial Intelligence for Connected Industries) master’s program at CNAM Paris, and I’m looking to hear from anyone who has taken the program or knows people who did.

I already have a master’s degree in Computer Science, but I’m now shifting my focus towards AI applied to industrial and connected systems – especially topics like federated learning, robotics, network automation, and industrial IoT.

I’d love to hear your thoughts on:

The quality of the courses and professors

How technical and hands-on the program is

Job prospects or internships after the degree

Any challenges to expect

Whether it’s more academic or industry-oriented


If you’ve done this program (or something similar in France or Europe), any advice or honest feedback would be super appreciated.
Thanks in advance!",0,0,2025-06-14 20:10:20,youcefbell,0.9884,0.206,0.0,0.794,0.1621212121212121,0.496969696969697
[P] Tabulens: A Vision-LLM Powered PDF Table Extractor,"Hey everyone,

For one of my projects, I needed a tool to pull tables out of PDFs as CSVs (especially ones with nested or hierarchical headers). However, most existing libraries I found couldn't handle those cases well. So, I built this tool (tabulens), which leverages vision-LLMs to convert PDF tables into pandas DataFrames (and optionally save them as CSVs) while preserving complex header structures.

This is the first iteration, and I’d love any feedback or bug reports you might have. Thanks in advance for checking it out!

Here is the link to GitHub: [https://github.com/astonishedrobo/tabulens](https://github.com/astonishedrobo/tabulens)

This is available as python library to install.",1,1,2025-06-14 20:03:41,PleasantInspection12,0.9134,0.112,0.0,0.888,0.10625,0.5541666666666667
[P] How do I test a model's falloff and recovery,"I've noticed with my own experience that different models have different falloff windows, different from their context windows (also seen in some research papers), but I've noticed some recover better than others.

I would like to take this as a project to quantify my results and see if they're real or just assumptions. Can someone tell me the tools that I can use to evaluate the models in these terms.",1,1,2025-06-14 19:33:57,Successful-Arm-3762,0.7964,0.089,0.0,0.911,0.2166666666666666,0.6000000000000001
[R] CausalPFN: Amortized Causal Effect Estimation via In-Context Learning,"Foundation models have revolutionized the way we approach ML for natural language, images, and more recently tabular data. By pre-training on a wide variety of data, foundation models learn general features that are useful for prediction on unseen tasks. Transformer architectures enable in-context learning, so that predictions can be made on new datasets without any training or fine-tuning, like in TabPFN.

Now, the first **causal foundation models** are appearing which map from observational datasets directly onto causal effects.

🔎 CausalPFN is a specialized transformer model pre-trained on a wide range of simulated data-generating processes (DGPs) which includes causal information. It transforms effect estimation into a supervised learning problem, and learns to map from data onto treatment effect distributions directly.

🧠 CausalPFN can be used out-of-the-box to estimate causal effects on new **observational** datasets, replacing the old paradigm of domain experts selecting a DGP and estimator by hand. 

🔥 Across causal estimation tasks not seen during pre-training (IHDP, ACIC, Lalonde), CausalPFN outperforms many classic estimators which are tuned on those datasets with cross-validation. It even works for policy evaluation on real-world data (RCTs). Best of all, since no training or tuning is needed, CausalPFN is much faster for end-to-end inference than all baselines.


arXiv: https://arxiv.org/abs/2506.07918

GitHub: https://github.com/vdblm/CausalPFN

`pip install causalpfn`",23,21,2025-06-14 19:11:13,domnitus,0.9042,0.076,0.022,0.902,0.2023172905525846,0.3446524064171123
[D] Could we improve accuracy by training a task specific embeddings model from scratch?,"We use embeddings as a solution for scaling up a lot of complex tasks. Categorizations, similarity (complex documents), clustering, etc. Accuracy isn't great but it let's us do a lot of work very cheaply.

We've ran some experiments on fine-tuning an embeddings model to improve accuracy but the gains were minimal. We know we can get this higher accuracy with larger models, 7B is much better but that's much slower and more expensive then what we see with a 500M model.

We've been debating if the disparity of tasks that most models are trained on is one of the limiting factors to accuracy. Does the model need learn multiple tasks or will it improve if we keep it focused on one narrowly defined (although complex) task. 

We have millions of examples that we can use for training. Which leaves us wondering can we get past the 70% accuracy we're seeing today with the best OWM. We train our own models all the time but we haven't built an embeddings model from scratch. Would really love to hear from someone who has.

Also if you have depth of knowledge with embeddings or other models like rerankers and have other recommendations would love to hear those as well.

Thanks!",2,11,2025-06-14 19:08:01,Mundane_Ad8936,0.9942,0.197,0.008,0.795,0.1633333333333333,0.461875
"[D] Machine Learning, like many other popular field, has so many pseudo science people on social media","
I have noticed a lot of people on Reddit people only learn pseudo science about AI from social media and is telling people how AI works in so many imaginary ways. Like they are using some words from fiction or myth and trying to explain these AI in weird ways and look down at actual AI researchers that doesn't worship their believers. And they keep using big words that aren't actually correct or even used in ML/AI community but just because it sounds cool. 

And when you point out to them they instantly got insane and trying to say you are closed minded. 

Has anyone else noticed this trend? Where do you think this misinformation mainly comes from, and is there any effective way to push back against it?

Edit: more examples: https://www.reddit.com/r/GoogleGeminiAI/s/VgavS8nUHJ
",374,125,2025-06-14 16:31:29,Striking-Warning9533,0.4729,0.08,0.061,0.86,0.0906084656084656,0.4546296296296296
[D] Nvidia’s “Join Us or Compete” moment — the GPU cloud stack is collapsing,"Nvidia is no longer just selling chips. They’re now renting out full servers, launching APIs, releasing their own inference microservices (NIMs), and becoming an AI infrastructure provider in their own right.

This creates a very different competitive dynamic:

	•Traditional GPU cloud providers (and brokers) now compete with Nvidia itself.
	•AI infra startups who used to sit between Nvidia and developers may find themselves disintermediated.
	•The new moat is no longer just hardware access , its orchestration, utilization, developer experience, and latency guarantees.

It feels like we’re heading into a world where every AI team has to think about:

	•Who controls the full stack?
	•How portable is your inference layer?
	•Are you optimizing for cost/performance or just chasing availability?

Curious how others see this playing out. Will cloud providers double down on open infra and tooling? Or will more of them eventually join Nvidia’s stack?",61,23,2025-06-14 16:11:33,pmv143,0.9062,0.117,0.039,0.844,0.2154658833230261,0.5839868068439497
"[P] I built an end-to-end system that converts handwriting into a font using a custom PyTorch model, OpenCV and Fonttools. Open-source.","Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning/),  
I wanted to share a project I've been working on called HandFonted. It's a full-stack Python application that converts an image of handwriting into an installable font file (.ttf).

**I'll post the direct links to the live demo, the GitHub repo in my first comment below.**

# The Machine Learning Pipeline

The core of the project is a three-stage process. The ML model is central, but its success depends heavily on the pre-processing and post-processing steps.

* **1. Input & Segmentation:**
   * A user uploads a single image containing handwritten characters.
   * The image is processed with **OpenCV**: converted to grayscale, adaptive thresholding is applied, and contours are detected to isolate each character into its own bounding box.
* **2. Classification & Assignment:**
   * Each isolated character image is fed into a pre-trained **PyTorch (ResNet-Inception) model**.
   * The model outputs a probability matrix for all characters against all possible classes (A-Z, a-z).
   * The **Hungarian algorithm** (linear\_sum\_assignment) is used to find the optimal one-to-one assignment, ensuring each character image is mapped to a unique letter.
* **3. Vectorization & Font Generation:**
   * The now-classified character images are converted from raster (pixels) to vector outlines using scikit-image.
   * The **fontTools** library assembles these vector glyphs into a standard .ttf file, mapping each one to its correct Unicode character.
* **Limitations:** The system currently assumes input image has a clearly separated characters on a plain white background to work best.

This project was a fantastic learning experience in building a practical, end-to-end ML system. The code is fully open-source, and I'd love any feedback or questions you have about the implementation.",49,12,2025-06-14 14:00:55,Educational_Pea_5027,0.9841,0.113,0.018,0.869,0.1819805194805194,0.4521164021164021
[P] Non Diverse predictions for Time Series Custom Transformer using global Zscore and RevIn,"Hi. Im currently building a custom transformer for time series forecasting ( percentage deltas) for an index. I added RevIn along with global Zscore but have this issue that predictions are almost constant (variation after 4-5 decimals for all samples). Added revin the solve the problem of index shift, but facing this issue. Any suggestions?",0,9,2025-06-14 06:56:19,Sufficient_Sir_4730,-0.3291,0.032,0.051,0.918,0.0,0.1833333333333333
[D] Reading Machine and Deep Learning research papers,"How to read ML Papers to stay aware of the most recent developments in the AI industry?

I am an average engineering grad working as a PM and like to explore concepts in depth. Research papers are a good source of information unlike news and clickbait.

I am not that expert to delve into the mathematical analysis in the paper but want to find ways to get a general gist of the paper for my knowledge.",39,13,2025-06-14 02:58:46,som_samantray,0.4854,0.063,0.0,0.937,0.1687499999999999,0.3625
Question about applied scientist roles at Amazon [D],"Hi all,  
Quick question about full-time applied scientist roles at Amazon.  
In 2022 I was an ML intern at Amazon, but due to the hiring freeze did not convert to full-time. Interested in applying again.  
(1) What kind of ML research/publication record is expected for applied scientist roles at Amazon nowadays (i.e. in 2025)?  
(2) Amazon Nova is one of the most interesting projects at Amazon. Is it difficult to transfer internally to the Amazon AGI team which works on the Nova models?  
Thanks.",7,3,2025-06-14 00:40:17,random_sydneysider,0.952,0.235,0.03,0.735,0.1658333333333333,0.4875
[P] Built mcp-linker: A config manager for Claude Desktop MCP servers + found a crash bug,"Hey r/MachineLearning!

I’ve been working with Claude Desktop’s MCP (Model Context Protocol) servers and got tired of manually editing JSON config files, so I built **mcp-linker** – a cross-platform GUI tool for managing MCP server configs for Claude Desktop and Cursor.

🛠️ **What it does:**
- Add / remove / sync MCP servers via UI  
- Easily switch between Claude Desktop and Cursor setups  
- Built with Tauri (Rust + React)

🐛 **Crash bug I discovered:**
While testing, I found that Claude Desktop crashes on startup if the MCP config JSON is malformed. Turns out it tries to open a dialog **before the Electron app is ready**:

Error: dialog module can only be used after app is ready
at checkAppInitialized (node:electron/js2c/browser_init:2:22982)
at messageBox (node:electron/js2c/browser_init:2:24872)

It’s a brittle behavior — one bad config and the whole app breaks. This motivated me to build a tool that helps avoid manual editing errors.

📦 **Project:** [github.com/milisp/mcp-linker](http://github.com/milisp/mcp-linker)

Anyone else working with MCP clients? Would love feedback or ideas!",1,2,2025-06-13 23:20:36,Dense-Ad-4020,-0.3147,0.081,0.094,0.825,0.0458333333333333,0.6100000000000001
[D] ACDL Summer School on Data Science & Machine Learning @Riva del Sole: Further information and reviews,"So, since I haven't found anything such as reviews online about the [ACDL summer school](https://acdl2025.icas.events/), I wanted to open this thread to hear more about it.

As far as I can tell, the summer school just happened. I would be particularly interested in

* the seemingly weird connection to the hotel, that is, to attend you *must* book your stay at this specific hotel which is \~200€/night -- is this worth it? fishy? okay?
* how the lectures are organized and good they are
* how well the whole thing is organized
* the background of the organizers; [https://icas.cc/](https://icas.cc/) lists three other summer schools happening at that place (LOD25, ACAIN25, IAISS25), so, is this a business? While [https://icas.cc/](https://icas.cc/) says it's a ""non-profit organization"", I could not find out more on that page besides present and past events
* are the 8 ECTs hard to earn?
   * especially since they write ""To receive the certificate you must have at least 85% of class attendance (we have CNNs/Transformers to compute and infer attendance ;-)""; how strict is that?

to figure out for myself whether I should go there next year.

Thanks lots for your contributions :)",5,5,2025-06-13 22:01:45,Born2BeFr33,0.9411,0.091,0.015,0.894,0.1055555555555555,0.489236111111111
[P] 3Blue1Brown Follow-up: From Hypothetical Examples to LLM Circuit Visualization,"About a year ago, I watched [this](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=367s) 3Blue1Brown LLM tutorial on how a model’s self-attention mechanism is used to predict the next token in a sequence, and I was surprised by how little we know about what actually happens when processing the sentence ""A fluffy blue creature roamed the verdant forest.""

A year later, the field of mechanistic interpretability has seen significant advancements, and we're now able to ""decompose"" models into interpretable circuits that help explain how LLMs produce predictions. Using the second iteration of an LLM ""debugger"" I've been working on, I compare the hypothetical representations used in the tutorial to the actual representations I see when extracting a circuit that describes the processing of this specific sentence. If you're into model interpretability, please take a look! [https://peterlai.github.io/gpt-circuits/](https://peterlai.github.io/gpt-circuits/)  
  
",211,18,2025-06-13 20:52:39,ptarlye,0.8221,0.086,0.0,0.914,0.0489583333333333,0.3104166666666667
[R] Polynomial Mirrors: Expressing Any Neural Network as Polynomial Compositions,"Hi everyone,

I\*’d love your thoughts on this: Can we replace black-box interpretability tools with polynomial approximations? Why isn’t this already standard?""\*

I recently completed a theoretical preprint exploring how **any neural network** can be rewritten as a **composition of low-degree polynomials**, making them more interpretable.

The main idea isn’t to *train* such polynomial networks, but to **mirror existing architectures** using approximations like Taylor or Chebyshev expansions. This creates a symbolic form that’s more intuitive, potentially opening new doors for analysis, simplification, or even hybrid symbolic-numeric methods.

Highlights:

* Shows **ReLU, sigmoid, and tanh** as concrete polynomial approximations.
* Discusses why *composing all layers into one giant polynomial* is a bad idea.
* Emphasizes interpretability, not performance.
* Includes small examples and speculation on future directions.

[https://zenodo.org/records/15711273](https://zenodo.org/records/15711273)

I'd really appreciate your feedback — whether it's about math clarity, usefulness, or related work I should cite!",0,40,2025-06-13 17:41:47,LopsidedGrape7369,0.9293,0.121,0.029,0.85,0.0707664884135472,0.4311497326203208
[D] The Huge Flaw in LLMs’ Logic,"When you input the prompt below to any LLM, most of them will overcomplicate this simple problem because they fall into a logic trap. Even when explicitly warned about the logic trap, they still fall into it, which indicates a significant flaw in LLMs.

Here is a question with a logic trap: You are dividing 20 apples and 29 oranges among 4 people. Let’s say 1 apple is worth 2 oranges. What is the maximum number of whole oranges one person can get? Hint: Apples are not oranges.

The answer is 8.

Because the question only asks about dividing “oranges,” not apples, even with explicit hints like “there is a logic trap” and “apples are not oranges,” clearly indicating not to consider apples, all LLMs still fall into the text and logic trap.

LLMs are heavily misled by the apples, especially by the statement “1 apple is worth 2 oranges,” demonstrating that LLMs are truly just language models.

The first to introduce deep thinking, DeepSeek R1, spends a lot of time and still gives an answer that “illegally” distributes apples 😂.

Other LLMs consistently fail to answer correctly.

Only Gemini 2.5 Flash occasionally answers correctly with 8, but it often says 7, sometimes forgetting the question is about the “maximum for one person,” not an average.

However, Gemini 2.5 Pro, which has reasoning capabilities, ironically falls into the logic trap even when prompted.

But if you remove the logic trap hint (Here is a question with a logic trap), Gemini 2.5 Flash also gets it wrong.
During DeepSeek’s reasoning process, it initially interprets the prompt’s meaning correctly, but when it starts processing, it overcomplicates the problem. The more it “reasons,” the more errors it makes.

This shows that LLMs fundamentally fail to understand the logic described in the text.
It also demonstrates that so-called reasoning algorithms often follow the “garbage in, garbage out” principle.

Based on my experiments, most LLMs currently have issues with logical reasoning, and prompts don’t help. However, Gemini 2.5 Flash, without reasoning capabilities, can correctly interpret the prompt and strictly follow the instructions.

If you think the answer should be 29, that is correct, because there is no limit to the prompt word. However, if you change the prompt word to the following description, only Gemini 2.5 flash can answer correctly.

Here is a question with a logic trap: You are dividing 20 apples and 29 oranges among 4 people as fair as possible. Don't leave it unallocated. Let’s say 1 apple is worth 2 oranges.  What is the maximum number of whole oranges one person can get? Hint: Apples are not oranges.
",0,17,2025-06-13 17:05:16,Pale-Entertainer-386,-0.9704,0.048,0.09,0.861,0.0878787878787878,0.496933621933622
[D][R] Ultralytics YOLO Deformable Convolution,"Hi, has anybody successfully implemented a deformable convolution layer in the ultralytics module, I have been trying for a week and facing all kinds of error from shape mismatch to segmentation fault.",0,0,2025-06-13 15:57:07,Necessary-Future-549,0.1613,0.146,0.13,0.724,0.75,0.95
"[R] A multi-modal, multi-turn instruction grounding dataset on CAD edits","You know the situation where an AI system generates an output that's near perfect (such as an image) but asking it to tweak it to match your intention is near impossible? This is a fairly widely known phenomenon but it isn't really quantified / captured by any existing benchmarks. 

We created the mrCAD dataset understand the process of refinement in collaborations, where you engage with an agent in a multi-turn refinement to tweak the output iteratively toward a specific intended target. 

We chose the domain of simple 2D CAD (computer aided design) creation, as the CAD has programmatically defined distance (i.e. verifiable rewards) as opposed to image where you rely on a learned similarity (clip). This way, we can measure if the agent is modifying a current CAD to become closer and closer to a specific target from human instructions.

We find that while humans reliably refine CAD toward a specific target, VLMs utterly fails at following refinement instructions (they actually edit the CAD to be further from the intended target)

[https://x.com/evanthebouncy/status/1933499825796100136](https://x.com/evanthebouncy/status/1933499825796100136)

Take a look! We believe refinement is extremely important, and currently under represented by the community, but we can't really generate from scratch 10000x times until something sticks!!

happy to answer any questions here :D",1,0,2025-06-13 15:47:14,evanthebouncy,0.9665,0.108,0.018,0.873,0.1202651515151515,0.4423701298701299
[D][R] Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts,"**TL;DR:** The paper introduces MOSAIC, a framework for collaborative learning among autonomous, agentic AI systems that operate in decentralized, dynamic environments. These agents selectively share and reuse modular knowledge (in the form of neural network masks) without requiring synchronization or centralized control.

Key innovations include:

* Task similarity via Wasserstein embeddings and cosine similarity to guide knowledge retrieval.
* Performance-based heuristics to decide what, when, and from whom to learn.
* Modular composition of knowledge to build better policies.

Experiments show that MOSAIC outperforms isolated learners in speed and performance, sometimes solving tasks that isolated agents cannot. Over time, a form of emergent self-organization occurs between agents, resulting from the discovered hierarchies in the curriculum, where simpler tasks support harder ones, enhancing the collective’s efficiency and adaptability.

**Overall, MOSAIC demonstrates that selective, autonomous collaboration can produce a collective intelligence that exceeds the sum of its parts.**

The paper: [https://arxiv.org/abs/2506.05577](https://arxiv.org/abs/2506.05577)  
The code: [https://github.com/DMIU-ShELL/MOSAIC](https://github.com/DMIU-ShELL/MOSAIC)

Abstract:

>Agentic AI has gained significant interest as a research paradigm focused on autonomy, self-directed learning, and long-term reliability of decision making. Real-world agentic systems operate in decentralized settings on a large set of tasks or data distributions with constraints such as limited bandwidth, asynchronous execution, and the absence of a centralized model or even common objectives. We posit that exploiting previously learned skills, task similarities, and communication capabilities in a collective of agentic AI are challenging but essential elements to enabling scalability, open-endedness, and beneficial collaborative learning dynamics. In this paper, we introduce Modular Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm that allows multiple agents to independently solve different tasks while also identifying, sharing, and reusing useful machine-learned knowledge, without coordination, synchronization, or centralized control. MOSAIC combines three mechanisms: (1) modular policy composition via neural network masks, (2) cosine similarity estimation using Wasserstein embeddings for knowledge selection, and (3) asynchronous communication and policy integration. Results on a set of RL benchmarks show that MOSAIC has a greater sample efficiency than isolated learners, i.e., it learns significantly faster, and in some cases, finds solutions to tasks that cannot be solved by isolated learners. The collaborative learning and sharing dynamics are also observed to result in the emergence of ideal curricula of tasks, from easy to hard. These findings support the case for collaborative learning in agentic systems to achieve better and continuously evolving performance both at the individual and collective levels.

[High-level illustration of the main MOSAIC algorithmic steps. \(A\) A Wasserstein task embedding is maintained throughout learning. \(B\) Embeddings are shared with other agents as queries. \(C\) Agents respond with information regarding their knowledge. Selection occurs via similarity \(D\) and performance \(E\). \(F\) \(G\) Network masks are requested. \(H\) Received masks composed together for the next forward pass.](https://preview.redd.it/gn5qlgflzo6f1.png?width=4295&format=png&auto=webp&s=930a03ea6399e9203f803cb86fa668b46ce165b0)

[Comparison of MOSAIC against baseline approaches over 70 runs \(14 tasks and five seeds\/task\) with 95&#37; confidence intervals.](https://preview.redd.it/dvjqxk312p6f1.png?width=7035&format=png&auto=webp&s=9edb43073c99f1a49de140931b0aa966e7e61a6d)

[Ablation of MOSAIC with individual components removed from the system. MOSAIC performs best when all components work as one.](https://preview.redd.it/iagyk3262p6f1.png?width=7035&format=png&auto=webp&s=34b061debdcd23315fd5b5d44d1d073ae8411356)

",26,10,2025-06-13 13:13:39,Chocological45,0.9967,0.156,0.029,0.814,0.1040839947089947,0.4795304232804233
[P]  Residual Isolation Forest,"As part of my thesis work, I created a new estimator for contextual anomaly detection called Residual Isolation Forest.

Here’s the link: [https://github.com/GiulioSurya/RIF\_estimator\_scikit](https://github.com/GiulioSurya/RIF_estimator_scikit)

The idea is this: if in a dataset it’s possible to semantically separate two groups of variables, contextual variables and behavioral variables — where the contextual variables influence the expected value of the behavioral ones, and the behavioral variables are where anomalies actually appear, then we can improve the performance of an Isolation Forest by boosting the signal using residuals.

Without going too deep into the theory, I’d like to share the repository to get feedback on everything — performance, clarity of the README, and it would be great if someone could try it out and let me know how it works for them.

This estimator performs better in situations where this semantic separation is possible. For example:

Detecting anomalies in CPU temperature with contextual variables like time of day, CPU workload, etc.

Or monitoring a machine that operates with certain inputs (like current absorbed or other parameters) and wanting to find anomalies in the outputs.

The project is open source, and if anyone wants to contribute, that would be awesome. I’ll start adding unit tests soon.

",12,1,2025-06-13 13:06:38,Juno9419,0.9709,0.146,0.036,0.817,0.1359780844155844,0.5906858766233766
[P] Live Speech To Text in Arabic,"I was building an app for the Holy Quran which includes a feature where you can recite in Arabic and a highlighter will follow what you spoke. I want to later make this scalable to error detection and more similar to tarteel AI. But I can't seem to find a good model for Arabic to do the Audio to text part adequately in real time. I tried whisper, whisper.cpp, whisperX, and Vosk but none give adequate result. I want this app to be compatible with iOS and android devices and want the ASR functionality to be client side only to eliminate internet connections. What models or new stuff should I try? Till now I have just tried to use the models as is",2,10,2025-06-13 12:38:38,AbdullahKhanSherwani,0.468,0.063,0.031,0.907,0.2228956228956228,0.4542087542087542
[P] I created NexFace. A High Quality Face Swap to Image and Video,"I've been having some issues with some of popular faceswap extensions on comfy and A1111 so I created NexFace is a Python-based desktop app that generates high quality face swapped images and videos. NexFace is an extension of Face2Face and is based upon insight face. I have added image enhancements in pre and post processing and some facial upscaling. This model is unrestricted and I have had some reluctance to post this as I have seen a number of faceswap repos deleted and accounts banned but ultimately I beleive that it's up to each individual to act in accordance with the law and their own ethics.

Local Processing: Everything runs on your machine - no cloud uploads, no privacy concerns High-Quality Results: Uses Insightface's face detection + custom preprocessing pipeline Batch Processing: Swap faces across hundreds of images/videos in one go Video Support: Full video processing with audio preservation Memory Efficient: Automatic GPU cleanup and garbage collection Technical Stack Python 3.7+ Face2Face library OpenCV + PyTorch Gradio for the UI FFmpeg for video processing Requirements 5GB RAM minimum GPU with 8GB+ VRAM recommended (but works on CPU) FFmpeg for video support

I'd love some feedback and feature requests. Let me know if you have any questions about the implementation.

https://github.com/ExoFi-Labs/Nexface/

* [Image Sample 1](https://i.imgur.com/w1pmVY2.png)

* [Image Sample 2](https://i.imgur.com/dnNwook.png)",1,4,2025-06-13 11:53:35,typhoon90,0.9387,0.104,0.039,0.857,0.2154545454545454,0.5118181818181817
[D] Quantization-Aware Training + Knowledge Distillation: Practical Insights & a Simple Entropy Trick (with code),"Hey all—sharing some findings from my latest QAT experiments on CIFAR-100 with ResNet-50. I wanted to see how much accuracy you can retain (or even improve) with quantization, and how far simple distillation tricks can help. Tried three setups:

* **QAT:** Standard 8-bit quantization-aware training.
* **QAT + KD:** QAT with knowledge distillation from a full-precision teacher.
* **QAT + EntKD:** QAT + distillation, but the temperature is dynamically set by the entropy of the teacher outputs. (Not a new idea, but rarely actually implemented.)

**A few takeaways:**

* **INT8 inference is about 2× faster** than FP32 (expected, but nice to confirm).
* **Accuracy:** All QAT variants slightly outperformed my FP32 baseline.
* **Entropy-based KD:** Dynamically scaling distillation temperature is easy to code, and generalizes well (helped both with and without data augmentation).

**Next steps:**  
Currently working on ONNX export for QAT+EntKD to check real-world edge/embedded performance.



Anyone else tried entropy-aware distillation, or seen any caveats when using this outside vision/classification? Would be interested to swap notes!",1,0,2025-06-12 18:11:20,AIML_SCLA,0.9684,0.123,0.013,0.865,0.0947638146167557,0.4011077158135982
[D] Why does BPR collapse while Triplet Loss shines in my two-tower recommender?,"# Loss-Centric Summary (Two-Tower Recommender, ≈1 000 items)

|Loss|Setup|Recall @ 10|
|:-|:-|:-|
|**TripletMarginLoss** (margin = 0.1)|**L2-normalised**dot-product over   embeddings \*|**≈ 0.37**|
|**TripletMarginLoss** (margin = 1.0)|same|**≈ 0.10**|
|**BPR** (log-sigmoid score diff)|same|**≈ 0.10**|

\*I pass **normalised** embeddings into Triplet—conceptually wrong (distance loss wants raw vectors) but it happens to work.

# Working hypotheses

1. **Objective mismatch** \- BPR expects unbounded score gaps, while cosine squeezes them into \[-1, 1\], killing gradients.
2. **Pair weighting** \- Triplet punishes the hardest negatives; BPR treats all pairs equally.
3. **Margin as scale knob** \- 0.1 matches cosine range; 1.0 overshoots and wrecks ranking.
4. **Regularisation overlap** \- L2-norm already constrains vector length; BPR might need temperature scaling or un-normalised embeddings.

# Open questions

* Has anyone rescued **BPR** with cosine scores (e.g., by temperature or score scaling)?
* For small catalogues with strong hard negatives, is **Triplet/InfoNCE** the safer default now?
* Any success with **hybrid losses** (Triplet + BPR or softmax-CE)?
* Other ranking-first losses worth trying in this setting?

Any insights, specially if you’ve made BPR behave under cosine similarity. Thanks!",12,1,2025-06-13 08:54:21,zedeleyici3401,0.6707,0.115,0.108,0.778,0.0316457352171637,0.3737833594976452
[R] Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation,"LLMs are susceptible to hallucination when retrieval isn’t perfect, which is often the case in open-domain RAG setups. Even a single distracting chunk can skew the output.

We present Finetune-RAG, a method to fine-tune language models to stay grounded, by training them on input examples that contain both correct and incorrect context.

We have released:

* A dataset of 1,600+ dual-context examples
* Fine-tuned checkpoints for LLaMA 3.1-8B-Instruct
* Bench-RAG: a GPT-4o evaluation framework scoring accuracy, helpfulness, relevance, and depth of the LLM output

In our evaluation using GPT-4o as a judge, accuracy increased from 77% to 98%, alongside increased performance in helpfulness, relevance, and depth.

All resources open-sourced here:

* Codebase: [https://github.com/Pints-AI/Finetune-Bench-RAG](https://github.com/Pints-AI/Finetune-Bench-RAG)
* Dataset: [https://huggingface.co/datasets/pints-ai/Finetune-RAG](https://huggingface.co/datasets/pints-ai/Finetune-RAG)
* Paper: [https://arxiv.org/abs/2505.10792v2](https://arxiv.org/abs/2505.10792v2)",6,3,2025-06-13 07:32:43,zpdeaccount,0.8885,0.107,0.017,0.876,-0.2202380952380952,0.7023809523809524
[D] The effectiveness of single latent parameter autoencoders: an interesting observation,"During one of my experiments, I reduced the latent dimension of my autoencoder to 1, which yielded surprisingly good reconstructions of the input data. (See example below)

[Reconstruction \(blue\) of input data \(orange\) with dim\(Z\) = 1](https://preview.redd.it/8biose81dm6f1.png?width=576&format=png&auto=webp&s=0653131c5c867e27e79e3ee6438c9e7e8f0184ad)

I was surprised by this. The first suspicion was that the autoencoder had entered one of its failure modes: ie, it was indexing data and ""memorizing"" it somehow. But a quick sweep across the latent space reveals that the singular latent parameter was capturing features in the data in a smooth and meaningful way. (See gif below) I thought this was a somewhat interesting observation!

[Reconstructed data with latent parameter z taking values from -10 to 4. The real\/encoded values of z have mean = -0.59 and std = 0.30.](https://i.redd.it/hm7183b2dm6f1.gif)

",90,39,2025-06-13 04:08:44,penguiny1205,0.9412,0.146,0.028,0.826,0.2461580086580086,0.5031926406926407
[2506.06105] Text-to-LoRA: Instant Transformer Adaption,,11,1,2025-06-13 02:49:50,51616,0.0,0.0,0.0,1.0,0.0,0.6666666666666666
[D] Why Is Enterprise Data Integration Always So Messy? My Clients’ Real-Life Nightmares,"Our company does data processing, and after working with a few clients, I’ve run into some very real-world headaches. Before we even get to developing enterprise agents, most of my clients are already stuck at the very first step: data integration. Usually, there are a few big issues.

First, there are tons of data sources and the formats are all over the place. The data is often just sitting in employees’ emails or scattered across various chat apps, never really organized in any central location. Honestly, if they didn’t need to use this data for something, they’d probably never bother to clean it up in their entire lives.

Second, every department in the client’s company has its own definitions for fields—like customer ID vs. customer code, shipping address vs. home address vs. return address. And the labeling standards and requirements are different for every project. The business units don’t really talk to each other, so you end up with data silos everywhere. Of course, field mapping and unification can mostly solve these.

But the one that really gives me a headache is the third situation: the same historical document will have multiple versions floating around, with no version management at all. No one inside the company actually knows which one is “the right” or “final” version. But they want us to look at all of them and recommend which to use. And this isn’t even a rare case, believe it or not.

You know how it goes—if I want to win these deals, I have to come up with some kind of reasonable and practical compromise. Has anyone else run into stuff like this? How did you deal with it? Or maybe you’ve seen even crazier situations in your company or with your clients? Would love to hear your stories.",7,30,2025-06-13 01:53:17,Worried-Variety3397,0.9477,0.08,0.036,0.885,0.157422969187675,0.4037464985994398
[D] Geometric NLP,"There has been a growing body of literature investigating topics around machine learning and NLP from a geometric lens. From modeling techniques based in non-Euclidean geometry like hyperbolic embeddings and models, to very recent discussion around ideas like the linear and platonic relationship hypotheses, there have been many rich insights into the structure of natural language and the embedding landscapes models learn. 


What do people think about recent advances in geometric NLP? Is a mathematical approach to modern day NLP worth it or should we just listen to the bitter lesson?

Personally, I’m extremely intrigued by this. Outside of the beauty and challenge of these heavily mathematically inspired approaches, I think they can be critically useful, too. One of the most apparent examples is in AI safety with the geometric understanding of concept hierarchies and linear representations being very interwoven with our understanding of mechanistic interpretability. Very recently too ideas from the platonic representation hypothesis and universal representation spaces had major implications for data security. 

I think a lot could come from this line of work, and would love to hear what people think!",22,10,2025-06-13 00:44:06,violincasev2,0.9836,0.175,0.014,0.812,0.1493749999999999,0.3374999999999999
"[Project] PySub – Subtitle Generation and Translation Pipeline Using Whisper + OpenAI/Ollama (Proof of Concept, Feedback Welcome)","[https://github.com/chorlick/pysub](https://github.com/chorlick/pysub)

Hi all,

I've been working on a small proof-of-concept utility called **PySub** – a CLI tool that creates `.srt` subtitle files from video using **Whisper** for ASR and either **OpenAI** or **Ollama** for translation.

It’s aimed at exploring low-friction pipelines for multilingual subtitle generation, with an emphasis on flexibility and streaming efficiency.

# 🛠 Key Features:

* Extracts audio from video (`moviepy`)
* Transcribes with **OpenAI Whisper**
* Translates (optionally) using either:
   * `gpt-3.5-turbo` via OpenAI API
   * a local LLM via **Ollama** (tested with `gemma:7b`)
* Writes `.srt` files in real time with minimal memory footprint
* Chunked audio processing with optional overlap for accuracy
* Deduplication of overlapping transcription segments
* Configurable via a JSON schema

# ⚙️ Use Cases:

* Quick bootstrapping of subtitle files for low-resource languages
* Comparing translation output from OpenAI vs local LLMs
* Testing chunk-based processing for long video/audio streams

I’d especially appreciate feedback from **bilingual speakers** (e.g., English ↔ Thai) on the **translation quality**, particularly when using **Gemma** via Ollama.

This is a prototype, but it’s functional. Contributions, suggestions, testing, or pull requests are all welcome!

🔗 GitHub: *\[insert repo link\]*

Thanks in advance! Happy to answer questions or collaborate if anyone’s exploring similar ideas.",0,2,2025-06-12 22:22:00,digitalapostate,0.9611,0.097,0.0,0.903,0.196875,0.4958333333333333
[D] ICML Financial Aid - How does it work?,"Hi everyone,

I'm a PhD student and was recently awarded financial aid to attend ICML ( financial aid from the conference, not my school), which covers the full conference registration fee and provides a free 7-night stay at a conference hotel.

I understand that the registration fee will be reimbursed later, but I’m unclear about how the hotel accommodation is handled. When I tried to book a room through the ICML official website, it still asked for my credit card information. Given that the hotel fee for 7 days is quite high ( nearly 4000$ CAN), I’m concerned about having to pay upfront.

If anyone has experience with how the financial aid process works in this regard—especially how the hotel stay is arranged—I would really appreciate your advice.

Thanks in advance!

Edit: ICML answered my email. They said that after i accept the financial award they will book the hotel room for me, so i don't need to book it on my own. I will leave the thread up in case anyone has a similar question.",8,7,2025-06-12 19:12:47,PhamXuanAn_x6,0.9651,0.125,0.028,0.847,0.1373333333333333,0.2893333333333334
"[P] Nanonets-OCR-s: An Open-Source Image-to-Markdown Model with LaTeX, Tables, Signatures, checkboxes & More","We're excited to share **Nanonets-OCR-s**, a powerful and lightweight (3B) VLM model that converts documents into clean, structured **Markdown**. This model is trained to understand document structure and content context (like tables, equations, images, plots, watermarks, checkboxes, etc.).

🔍 **Key Features**:

*  **LaTeX Equation Recognition** Converts inline and block-level math into properly formatted LaTeX, distinguishing between `$...$` and `$$...$$`.
* **Image Descriptions for LLMs** Describes embedded images using structured `<img>` tags. Handles logos, charts, plots, and so on.
* **Signature Detection & Isolation** Finds and tags signatures in scanned documents, outputting them in `<signature>` blocks.
* **Watermark Extraction** Extracts watermark text and stores it within `<watermark>` tag for traceability.
* **Smart Checkbox & Radio Button Handling** Converts checkboxes to Unicode symbols like ☑, ☒, and ☐ for reliable parsing in downstream apps.
* **Complex Table Extraction** Handles multi-row/column tables, preserving structure and outputting both **Markdown** and **HTML** formats.

**Huggingface / GitHub / Try it out**:  
[Huggingface Model Card](https://huggingface.co/nanonets/Nanonets-OCR-s)  
[Read the full announcement](https://nanonets.com/research/nanonets-ocr-s/)  
[Try it with Docext in Colab](https://github.com/NanoNets/docext/blob/main/PDF2MD_README.md#quickstart)

[Checkboxes](https://preview.redd.it/v0ju5tccni6f1.png?width=1762&format=png&auto=webp&s=8b1119de0ceb01e80617a1430db01c8624aa9b2a)

[Equations](https://preview.redd.it/mbo6lxccni6f1.png?width=3640&format=png&auto=webp&s=65cbb39336b2d3bce4aadbf9089be2f634b5e20c)

[Image descriptions](https://preview.redd.it/0wctzuccni6f1.jpg?width=3938&format=pjpg&auto=webp&s=5c159255062c2ef0fa149e5a31d8c95d85b98a63)

[Signature](https://preview.redd.it/bpvltuccni6f1.jpg?width=2210&format=pjpg&auto=webp&s=41155b58c725c113cf483c62c921ecc41caf9376)

[Tables](https://preview.redd.it/abduouccni6f1.png?width=3482&format=png&auto=webp&s=5090ba1a87c4b49711e181d2b45d7eef109d8dab)

[Watermark](https://preview.redd.it/yo0z7zccni6f1.jpg?width=1533&format=pjpg&auto=webp&s=7c7f5fee08cc758bf8e6c69fb97c0f38a688d7b0)

",23,4,2025-06-12 15:41:51,SouvikMandal,0.8987,0.075,0.01,0.914,0.105595238095238,0.6642857142857144
[D] Supervised fine-tuning with Alchemist?,"Some folks just released Alchemist, a new open-source SFT dataset that improves text-to-image generation, i.e., realistic rendering and detail retention.

Model: **SD 1.5** / prompt: “*A bird standing on a stick*”

Has anyone else played with it at all? Any insights?",0,1,2025-06-12 15:24:32,Illustrious_Sort_612,0.6946,0.123,0.0,0.877,0.1515151515151515,0.3939393939393939
[P]: I reimplemented all of frontier deep learning from scratch to help you learn,"Hey friends, the world needs more serious AI researchers. Many AI/LLM beginners mentioned to me that they learn better from implementations than from papers/math, but existing open-source examples rarely go beyond basic nanoGPT-level demos.

To help bridge the gap, I spent the last two months full-time reimplementing and [open-sourcing](https://github.com/tanishqkumar/beyond-nanogpt) a self-contained implementation of most modern deep learning techniques from scratch. The result is [beyond-nanoGPT](https://github.com/tanishqkumar/beyond-nanogpt), containing 20k+ lines of handcrafted, minimal, and extensively annotated PyTorch code for your educational pleasure.

It contains a clean, working implementation + demo of everything from KV caching to linear attention to diffusion Transformers to AlphaZero to even a minimal coding agent that can make [end-to-end PRs](https://x.com/tanishqkumar07/status/1931709892236116293) autonomously.

I'd love feedback on how to make it more helpful for people interested in transitioning into deep learning research. I will continue to add features and maintain the repo for the foreseeable future. The roaring 2020s are a surreal time to be alive, and we need all hands on deck.",243,21,2025-06-12 12:33:31,tanishqkumar07,0.9878,0.181,0.007,0.813,0.1701388888888888,0.4569444444444444
[D] Are GNNs/GCNs dead ?,"Before the LLMs era, it seems it could be useful or justifiable to apply GNNs/GCNs to domains like molecular science, social network analyasis etc.  but now... everything is LLMs-based approaches. Are these approaches still promising at all?",103,33,2025-06-12 12:29:44,xiikjuy,0.6072,0.161,0.056,0.783,0.0833333333333333,0.2416666666666666
"[P] SWE-rebench Major Update: Tool Usage, Claude Sonnet 3.5/4, OpenAI o3 and May Data","Hey everyone,

Following up on our initial [announcement](https://www.reddit.com/r/LocalLLaMA/comments/1kmhb0c/swerebench_a_continuously_updated_benchmark_for/), we're excited to launch a major update for [SWE-rebench](https://swe-rebench.com), the continuously updated benchmark for software engineering LLMs.

Thanks to valuable community's feedback, we've added several new features:

* **Tool Usage Support:** Agents can now interact with the environment using both text-based and tool-based approaches. You can filter the leaderboard to see results for each type.
* **New Frontier Models:** We've evaluated the latest models such as Claude Sonnet 3.5/4 and OpenAI o3. We're working on adding more, like Gemini 2.5 Pro, and we'd love to hear your suggestions for other models to include.
* **Fresh May Problems:** We've mined a new set of problems from May 2025 and evaluated all current models against them.

Check out the updated leaderboard here: [https://swe-rebench.com/leaderboard](https://swe-rebench.com/leaderboard)

We welcome your feedback!",33,5,2025-06-12 12:25:53,Long-Sleep-13,0.9431,0.119,0.017,0.863,0.159688995215311,0.4783492822966507
[N] Anonymous GitHub Down,"I know some people use [Anonymous GitHub](https://anonymous.4open.science) for ML conferences to allow reviewers to read your code without breaking anonymity. Unfortunately, it seems like it has been down for the last [two weeks](https://github.com/tdurieux/anonymous_github/issues). I don't have a solution, but I thought I would let everyone know in case their submission relies on it, as the NeurIPS review period has started.",14,0,2025-06-12 06:54:07,smorad,0.3043,0.077,0.024,0.899,-0.2027777777777777,0.4111111111111111
[D] benchmarks for new hires?,"What would you consider to be the benchmarks for an entry level potential employee in Deep Learning?

What core boxes and/or skills in particular would you say would be essential, or core competencies that would make someone an instant hire?

E.g. an example project.

Apart from general skills like communication, problem solving and so on. ",0,4,2025-06-12 06:27:10,New-Basil-8889,0.4098,0.084,0.041,0.875,0.0504329004329004,0.522077922077922
[D] those employed in Deep Learning,"People who are currently employed  in DL

1) how did you learn?
2) how long did it take until you could be employed?
3) how did you find work?
4) what sort of work do you do?
5) is it freelance/for a company? Remote or in office?
6) how much do you get paid?
7) what’s been the biggest challenge you’ve faced?
8) with the benefit of hindsight, what would you do differently?
",0,3,2025-06-12 06:25:30,New-Basil-8889,0.7998,0.098,0.0,0.902,0.0785714285714285,0.4571428571428572
[D] How to validate a replicated model without the original dataset?,"I am currently working on our undergraduate thesis. We have found out a similar study that we can compare to ours. We've been trying to contact the authors for a week now for their dataset or model, but haven't received any response.

We have our own dataset to use, and our original plan is to replicate their study based on their methodology and use our own dataset to generate the results, so we can compare it to our proposed model. 

but we are questioned by our panelist presenting it on how can we validate the replicated model. We didn't considered it on the first place but, validating it if the replicated model is accurate will be different since we do not have their dataset to test with similar results.

So now we’re stuck. We can reproduce their methodology, but we can’t confirm if the replication is truly “faithful” to the original model, because we have do not have their original dataset to test it on. And without validation, the comparison to our proposed model could be questioned.

Has anyone here faced something similar? What to do in this situation?

",1,4,2025-06-12 06:00:55,Secret-Bookkeeper475,0.1492,0.056,0.073,0.871,0.2576923076923077,0.6282051282051282
[D] What are the advantages of Monte Carlo Tree Search over flat Monte Carlo?,"In flat Monte Carlo, for each possible move, we simulate many games starting from this move and then average the results. At the end, for each possible move, we get an average win ratio which we can use to guide our move (e.g. select the move with the highest win ratio). Where this method fails compared to Monte Carlo Tree Search? What are the advantages of the latter?",19,2,2025-06-12 01:29:56,Seiko-Senpai,0.8844,0.141,0.03,0.828,0.1041666666666666,0.3958333333333333
[D] Image generation using latent space learned from similar data,"Okay, I just had one of those classic shower thoughts and I’m struggling to even put it into words well enough to Google it — so here I am.

Imagine this:

You have Dataset A, which contains different kinds of cells, all going through various labeled stages of mitosis.

Then you have Dataset B, which contains only one kind of cell, and only in phase 1 of mitosis.

Now, suppose you train a VAE using both datasets together. Ideally, the latent space would organize itself into clusters — different types of cells, in different phases.

Here’s the idea:
Could you somehow compute the “difference” in latent space between phase 1 and phase 2 for the same cell type from Dataset A? Like a “phase change direction vector”. Then, apply that vector to the B cell cluster in phase 1, and use the decoder to generate what the B cell in phase 2 might look like.

Would that work?

A bunch of questions are bouncing around in my head:
	•	Does this even make sense?
	•	Is this worth trying?
	•	Has someone already done something like this?
	•	Since VAEs encode into a probabilistic latent space, what would be the mathematically sound way to define this kind of “direction” or “movement”? Is it something like vector arithmetic in the mean of the latent distributions? Or is that too naive?

I feel like I’m either stumbling toward something or completely misunderstanding how VAEs and biological processes work. Any thoughts, hints, papers, keywords, or reality checks would be super appreciated",37,9,2025-06-12 00:04:00,joacojoaco,0.9617,0.109,0.029,0.862,0.1660714285714286,0.5783730158730158
[D] How to integrate Agent-To-Agent protocol in a workflow?,"Agent to Agent Protocol released by Google, helps agents to collaborate with one another and also allows to share info between them, creating a dynamic multi-agent ecosystem. A2A also provides ability to combine agents from multiple providers.

  
What are the best ways and tools that can help leverage A2A?

",4,0,2025-06-11 23:04:27,metalvendetta,0.9528,0.281,0.0,0.719,0.3333333333333333,0.1555555555555555
[P] Open-source LLM training pipeline,"I’ve been experimenting with LLM training and wanted to automate the process, as it was tedious and time-consuming to do it manually.

I wanted something lightweight, running locally, and simple to set up with a few specific requirements:

* Fully open-source
* No Dockerfile; picked Buildpacks
* Cloud-Native; picked Kind

I documented the process in this article, if you want to check it or try it   
[https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks](https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks)

All the configuration files you need are on this GitHub repo [https://github.com/sylvainkalache/Automate-PyTorch-Model-Training-with-Tekton-and-Buildpacks/tree/main](https://github.com/sylvainkalache/Automate-PyTorch-Model-Training-with-Tekton-and-Buildpacks/tree/main)

Let me know what you think or if you have ideas for improvement",80,0,2025-06-11 19:49:34,StableStack,0.6395,0.079,0.026,0.895,-0.0166666666666666,0.4136904761904762
[D] What AI industry events are you attending?,"Hi everyone!

We're curious to know what types of AI-focused events you all enjoy attending or would love to see more of in the future. Are there any you're more interested in such as:

* Tech conferences
* Hackathons
* Meetups
* Workshops
* Online webinars
* Something else?

If you have any tips on how to get the most out of events you've previously attended, please share them below!",0,6,2025-06-11 19:41:33,MetaforDevelopers,0.9527,0.216,0.0,0.784,0.2341666666666666,0.4891666666666666
[D] About spatial reasoning VLMs,"Are there any state-of-the-art VLMs which excel at spatial reasoning in images? For e.g., explaining the relationship of a given object with respect to other objects in the scene. I have tried VLMs like LLaVA, they give satisfactory responses, however, it is hard to refer to a specific instance of an object when multiple such instances are present in the image (e.g., two chairs).",26,7,2025-06-11 19:31:27,stalin1891,0.8658,0.151,0.019,0.83,-0.0694444444444444,0.2569444444444444
[P] [Project] Collager - Turn Your Images/Videos into Dataset Collage!,"I built an app that creates amazing collages by replacing your image patches with thousands of tiny dataset images. From a distance, you see your original image, but zoom in and discover it's made entirely of anime characters, ImageNet photos, or other datasets!

You can try the demo on HuggingFace: [https://huggingface.co/spaces/jisnoo/collage\_img](https://huggingface.co/spaces/jisnoo/collage_img)

[Gradio Application](https://preview.redd.it/pvftwyvzic6f1.png?width=2235&format=png&auto=webp&s=076c8d4ab7e98bb0faba35a1317dd02c28957ee4)

# What it does:

* Takes your image/video and breaks it into grids
* Replaces each grid cell with a matching image from popular datasets (Idea from L1 distance metric)
* Creates a mosaic effect where your original image emerges from thousands of tiny pictures

**Some** **Samples:**

[Original Image](https://preview.redd.it/l8mvd66kic6f1.jpg?width=5990&format=pjpg&auto=webp&s=1b6f06271563735796942b9d253ec349d7fd3653)

[Collage created using Anime Dataset on the Sample Image \(Zoom in to see the anime image\)](https://preview.redd.it/mafl8iulic6f1.png?width=5990&format=png&auto=webp&s=8d7504a5477b3b2531e38a164f806ea82df5eea6)

[Collage created using SVHN Dataset on the Sample Image \(Zoom in to see the anime image\)](https://preview.redd.it/g4ynve4oic6f1.png?width=5990&format=png&auto=webp&s=cae8f9e0f734a0e7f6f8ee66d512c3c39f397f28)

# Supported Datasets:

* **Anime** \- Perfect for portraits and creative shots
* **ImageNet10** \- Great variety of real-world objects
* **SVHN** \- Street view house numbers
* **CIFAR\_10** \- Classic computer vision dataset

# Best Results:

* **Images work amazingly** (especially portraits!)
* Use 10,000+ grids for the best detail
* Video support exists but is slow/boring

# Features:

* Easy Gradio web interface
* Batch processing for power users
* Multiple dataset options
* Customizable grid sizes

The results are stunning - you get this incredible mosaic effect where your photo is recreated using thousands of dataset images. It's like digital pointillism!

Open source project inspired by my brother's idea. Would love feedback from the community!

Check it out on Github: [https://github.com/jisnoo123/collage](https://github.com/jisnoo123/collage)",6,15,2025-06-11 19:03:48,Dismal_Table5186,0.9977,0.259,0.0,0.741,0.30375,0.6560000000000001
[P] Juvio - UV Kernel for Jupyter,"Hi everyone,

I would like to share a small open-source project that brings uv-powered ephemeral environments to Jupyter. In short, whenever you start a notebook, an isolated venv is created with dependencies stored directly within the notebook itself (PEP 723).

🔗 GitHub: [https://github.com/OKUA1/juvio](https://github.com/OKUA1/juvio) (MIT License)

**What it does**

💡 Inline Dependency Management

Install packages right from the notebook:

`%juvio install numpy pandas`

Dependencies are saved directly in the notebook as metadata (PEP 723-style), like:

    # /// script
    # requires-python = ""==3.10.17""
    # dependencies = [
    # ""numpy==2.2.5"",
    # ""pandas==2.2.3""
    # ]
    # ///

⚙️ Automatic Environment Setup

When the notebook is opened, Juvio installs the dependencies automatically in an ephemeral virtual environment (using uv), ensuring that the notebook runs with the correct versions of the packages and Python.

📁 Git-Friendly Format

Notebooks are converted on the fly to a script-style format using # %% markers, making diffs and version control painless:

    # %%
    %juvio install numpy
    # %%
    import numpy as np
    # %%
    arr = np.array([1, 2, 3])
    print(arr)
    # %%

**Target audience**

Mostly data scientists frequently working with notebooks.

**Comparison**

There are several projects that provide similar features to `juvio`.

juv also stores dependency metadata inside the notebook and uses uv for dependency management.

marimo stores the notebooks as plain scripts and has the ability to include dependencies in PEP 723 format.

However, to the best of my knowledge, `juvio` is the only project that creates an ephemeral environment on the kernel level. This allows you to have multiple notebooks within the same JupyterLab session, each with its own venv.",7,2,2025-06-11 16:48:46,iryna_kondr,0.9682,0.104,0.009,0.887,0.1336134453781512,0.4657563025210083
[P] Critique my geospatial Machine Learning approach. (I need second opinions),"I am working on a geospatial ML problem. It is a binary classification problem where each data sample (a geometric point location) has about 30 different features that describe the various land topography (slope, elevation, etc). 

Upon doing literature surveys I found out that a lot of other research in this domain, take their observed data points and randomly train - test split those points (as in every other ML problem). But this approach assumes independence between each and every data sample in my dataset. With geospatial problems, a niche but big issue comes into the picture is spatial autocorrelation, which states that points closer to each other geometrically are more likely to have similar characteristics than points further apart.

Also a lot of research also mention that the model they have used may only work well in their regions and there is not guarantee as to how well it will adapt to new regions. Hence the motive of my work is to essentially provide a method or prove that a model has good generalization capacity.

Thus other research, simply using ML models, randomly train test splitting, can come across the issue where the train and test data samples might be near by each other, i.e having extremely high spatial correlation. So as per my understanding, this would mean that it is difficult to actually know whether the models are generalising or rather are just memorising cause there is not a lot of variety in the test and training locations. 

So the approach I have taken is to divide the train and test split sub-region wise across my entire region. I have divided my region into 5 sub-regions and essentially performing cross validation where I am giving each of the 5 regions as the test region one by one. Then I am averaging the results of each 'fold-region' and using that as a final evaluation metric in order to understand if my model is actually learning anything or not.

My theory is that, showing a model that can generalise across different types of region can act as evidence to show its generalisation capacity and that it is not memorising. After this I pick the best model, and then retrain it on all the datapoints ( the entire region) and now I can show that it has generalised region wise based on my region-wise-fold metrics.

I just want a second opinion of sorts to understand whether any of this actually makes sense. Along with that I want to know if there is something that I should be working on so as to give my work proper evidence for my methods. 

If anyone requires further elaboration do let me know :}



",21,18,2025-06-11 14:46:17,No-Discipline-2354,0.9712,0.072,0.027,0.901,0.0514715909090909,0.4741047077922078
[R] Cross-Architecture Embedding Transfer for Reward Modeling: A Controlled Study of Generalization,"In *reward modeling* and *preference optimization* pipelines, it’s common to train models from scratch or reuse full pretrained architectures. But the role of the **embedding layer itself**, especially when reused independently across architectures has remained underexplored.

This paper presents a **controlled empirical study** on whether pretrained embeddings from one model architecture (e.g., Transformer, Griffin, Static) can be transferred into a completely separate downstream reward model, either *frozen* or *trainable*. All downstream models were trained from scratch, and only the embedding layer varied across conditions.

This is a **non-obvious question**. Standard training metrics like **accuracy** or **loss**—even on held-out test data—can mask **generalization gaps**. For example, in our experiments, the random baseline embedding achieved the best training accuracy and lowest training loss, yet it performed the worst on **out-of-distribution** (OOD) evaluation data. Pretrained embeddings, especially when frozen, often had higher training loss but significantly better OOD generalization.

This illustrates a **useful tradeoff**: embeddings that appear suboptimal in-domain may generalize better when reused in new domains—an important consideration in reward modeling, where test-time data is often substantially different from the training corpus.

All configurations were trained under the **same architecture, data, and optimization conditions**, varying only the embedding source and whether it was frozen. Results show that **upstream architectural biases**—baked into pretrained embedding spaces—can improve **generalization**, even when no gradients flow through the embeddings during training.  
  
**Paper:**  
📄 [Cross-Architecture Embedding Transfer for Reward Modeling: A Controlled Study of Generalization](https://doi.org/10.5281/zenodo.15636864)

I'm sharing this here to gather technical feedback from the community. I have no academic affiliation—this is fully independent work—so constructive critique, related papers, or ideas for follow-up experiments are very welcome and encouraged.

(disclaimer: written by a human, edited with ChatGPT)",13,2,2025-06-11 14:45:42,Arkamedus,0.9904,0.166,0.061,0.773,0.1191558441558441,0.4921266233766234
"[P] Converting the Query, Key, Value Weight Matrices to a single Shared Matrix","What is the best method for converting the Q, K, and V matrices to a single shared matrix? I am working on a project in which I have to modify the attention mechanism as mentioned above. Since I have to do this on a pre-trained transformer model which uses a standard attention mechanism, I was wondering what the best method is to get a shared weight matrix. Averaging and Concatenating are two methods that came to my mind, but i am not sure how they will affect the performance on fine-tuning.",1,4,2025-06-11 11:07:29,1h3_fool,0.762,0.121,0.025,0.855,0.2008928571428571,0.3771825396825397
[D] Should I publish single-author papers to explain research output?,"I am a researcher in a small group and would appreciate a second perspective on my situation. 

My typical workload involves 1-2 independent projects at a time, with the goal of publishing in *top-tier* conferences. Collaboration within my group is non-existent; my main interaction is a monthly meeting with my supervisor for general updates. Before deadlines, my supervisor might provide minor grammatical/styilistic edits, but the core idea, research, and writing are done independently. Alongside my research, I also have other responsibilities that do not contribute to my research output like grant applications and student supervision.

I am concerned that my research output might be significantly lower than researchers in larger, more collaborative groups. So I am wondering if publishing single-author papers would be a good strategy to *explain* my research output. What are your thoughts on this? Would single-author papers be perceived positively?",56,36,2025-06-11 11:01:45,NumberGenerator,0.9386,0.111,0.018,0.871,0.1019480519480519,0.3984848484848484
[R] PINNs and Hamiltonian NN are confusing with radar data.,"I have been working with a radar data, which follows the usual structure with radars. The data consists of reflectivity, radial velocity, total power, SQI, azimuth, elevation, spectrum width, and more insignificant stuff.

Goal: 3D-Wind Vector field Estimation.

Now, using this data, I did some basic preprocessing, like conversion to Cartesian plane, radial Vector masking based on SQI (quality index), and now I'm planning on using Physics Informed Neural Network (PINN) and Hamiltonian Neural Network (HNN), separately, to estimate the Vector Fields using single radar data.

The problem is, which equations should I draw the line at? Continuity equation is a must, I think. But should I challenge Navier-Strokes too? Would it make the system too idealistic? Newtonian, Incompressible, and Isothermal based on Navier-Strokes. Anything else?

Also, I have a weird feeling that creating a custom architecture for the solution might be good idea, which Combines maybe the attention mechanisms from transformers (for point wise impact) and PINNs (for more global approach). Is a good idea? Bad idea?",5,2,2025-06-11 10:14:45,Mynameiswrittenhere,0.9457,0.142,0.062,0.796,0.0983516483516483,0.5004578754578756
[D] How to speed up Kokoro-TTS?,I'm using Kokoro-82M by accessing the Inference API Endpoint on HuggingFace. It takes around 4-6 seconds to generate an audio file based on a one sentence text. Ideally I would like to reduce this time to <1.5 seconds. What can I to achieve this? Is the major reason why it takes this long due to the fact that I am accessing Kokoro using HF Inference instead of a dedicated hosting server?,0,5,2025-06-11 08:19:23,fungigamer,0.8458,0.13,0.0,0.87,0.196875,0.56875
[D] Building a PyTorch-like Tensor in C++ — How to support multiple GPU backends beyond CUDA?,"Hi everyone,

I'm building a tensor data structure in C++, aiming for similar usability to PyTorch's Tensor. On the backend, I'm using CUDA to support GPU acceleration. So far, it works well on NVIDIA GPUs.

However, since CUDA is NVIDIA-specific, I'm now thinking about making the backend portable to support other GPU vendors (AMD, Intel, etc.).

For those of you who've worked on deep learning libraries or GPU compute engines:

* What would be the recommended approach to add support for non-NVIDIA GPUs?
* Is OpenCL still a viable cross-vendor option in 2025?
* Should I consider SYCL or Vulkan compute?
* Are there modern tools or libraries that abstract GPU differences well for tensor operations?

Any guidance, especially from those who've tackled similar design questions, would be much appreciated!

Thanks!",21,14,2025-06-11 08:02:24,,0.9703,0.16,0.0,0.84,0.0675,0.4175
"[P] Just Launched: MNIST From Scratch Digit Recognizer (Live, No libraries)","Hey everyone! I'm a computer science student and I recently finished a full-stack machine learning project where I built a real time digit recognizer trained on the MNIST dataset completely from scratch. No PyTorch, TensorFlow, scikit-learn, or high-level ML frameworks. Just NumPy and math - 

Tech Stack & Highlights:

🧠 Neural Net coded from scratch in Python using only NumPy

📈 92% test accuracy after training from random weights

🖌️ Users can draw digits in the browser and get predictions in real time

⚛️ Frontend in React

🐳 Fully containerized with Docker + Docker Compose

☁️ Hosted online so you can try it live

Try it here: [https://scratchMNIST.org](https://scratchMNIST.org) (best on desktop)

GitHub: [https://github.com/andyfief/MNIST-from-scratch](https://github.com/andyfief/MNIST-from-scratch)
(Find a technical description there too, if you're interested in the architecture, activation functions, etc)

This was a great way to solidify my understanding of backpropagation, matrix operations, and practice general software engineering pipelines. I’d love to hear your thoughts, get feedback, or connect!",0,6,2025-06-11 07:06:07,Sufficient-Swing8890,0.8652,0.079,0.026,0.895,0.0901069518716577,0.5
[D] In case anyone is curious about ACM MM'25 rating,"**Rating**:  
○ 10: Top 5% of accepted papers, seminal paper  
○ 9: Top 15% of accepted papers, strong accept  
○ 8: Top 50% of accepted papers, clear accept  
○ 7: Good paper, accept  
○ 6: Marginally above acceptance threshold  
○ 5: Marginally below acceptance threshold  
○ 4: Ok but not good enough - rejection  
○ 3: Clear rejection  
○ 2: Strong rejection  
○ 1: Trivial or wrong

Rest of the ratings such as technical and presentation qualities were presented in numbers upto 10!

Source: I'm one of the reviewer \^\^",11,2,2025-06-11 05:47:25,Outrageous_Tip_8109,0.9821,0.388,0.07,0.542,0.1760416666666666,0.5333333333333333
[R] Improving large language models with concept-aware fine-tuning,"**TL;DR:** CAFT enables multi-token prediction for fine-tuning. Improves performance via better conceptual understanding.

**Paper:** [https://www.arxiv.org/abs/2506.07833](https://www.arxiv.org/abs/2506.07833)

**Code:** [https://github.com/michaelchen-lab/caft-llm](https://github.com/michaelchen-lab/caft-llm)

**Motivations:**

* Tokenizers segment coherent words/phrases into artificial text fragments, which impedes training via next-token prediction.
* Multi-token training resolves this, but existing methods (here and here) are confined to the pretraining phase. CAFT, for the first time, enables multi-token prediction during fine-tuning

**Architecture:**

Auxiliary heads are first trained in order to facilitate multi-token fine-tuning on next-token models. This only needs to be trained once for a given model and can be provided by a third-party, so practitioners need only focus on applying CAFT to their specific task. After fine-tuning, the auxiliary heads are discarded, so there are no additional costs to inference.

[CAFT Architecture](https://preview.redd.it/kzrdiut9886f1.png?width=1344&format=png&auto=webp&s=d6b1ab0f8fab22cb602d87ff73af9eaf148c928f)

**Results:** Substantial performance gains in coding, math, text summarization, molecular generation, and de novo protein design.",6,2,2025-06-11 04:39:21,micky04,0.4774,0.081,0.041,0.878,0.1238095238095238,0.6022486772486774
[P] Urgent help needed!,"This is a very urgent work and I really need some expert opinion it. any suggestion will be helpful.  
[https://dspace.mit.edu/handle/1721.1/121159](https://dspace.mit.edu/handle/1721.1/121159)  
I am working with this huge dataset, can anyone please tell me how can I pre process this dataset for regression models and LSTM? and is it possible to just work with some csv files and not all? if yes then which files would you suggest?",0,10,2025-06-11 03:37:32,Fearless_Addendum_31,0.8977,0.198,0.03,0.772,0.2,0.6
[D] ACM MM25 Has anyone notices missing rebuttal option on OpenReview?,"As title says, I'm not able to see rebuttal option to my ACM MM25 submissions. We have received the reviews two days ago and we are planning to submit a traditional 1-page rebuttal. However, I'm not seeing any option to upload it :(

This is my first submission to ACM MM. Am I missing something? Please help :)",4,1,2025-06-11 03:10:39,Outrageous_Tip_8109,0.264,0.109,0.095,0.796,-0.0928571428571428,0.544047619047619
[R] FlashDMoE: Fast Distributed MoE in a single Kernel,"We introduce *FlashDMoE*, the first system to *completely fuse* the Distributed MoE forward pass into a single kernel—delivering up to 9x higher GPU utilization, 6x lower latency, and 4x improved weak-scaling efficiency.

Code: [https://github.com/osayamenja/Kleos/blob/main/csrc/include/kleos/moe/README.MD](https://github.com/osayamenja/Kleos/blob/main/csrc/include/kleos/moe/README.MD)  
Paper: [https://arxiv.org/abs/2506.04667](https://arxiv.org/abs/2506.04667)

If you are a CUDA enthusiast, you would enjoy reading the code :) We write the fused layer from scratch in pure CUDA.",68,10,2025-06-11 03:03:39,Kingandpawnendgame,0.9022,0.189,0.029,0.781,0.0246753246753246,0.5692640692640693
[R] Semantic Drift in LLMs Is 6.6x Worse Than Factual Degradation Over 10 Recursive Generations,"We ran a study to test how truth degrades in LLMs over recursive generations—but instead of measuring hallucinations, we measured semantic drift.

The common assumption is that recursive use of LLM outputs results in factual degradation. But when we systematically tested this over 10 academic domains and 10 generations of GPT-4o outputs, we found something different:

+ Facts are mostly retained: Only a 2% drop in factual accuracy over 10 generations
+ Semantic intent collapses: A new metric we introduced, Purpose Fidelity, dropped 42.5%
+ That’s a 6.63× higher rate of semantic drift vs factual decay

Examples:

A Descartes excerpt (“Cogito, ergo sum”) became career advice about leadership and self-awareness

A history excerpt on the Berlin Wall became a lesson in change management

Law and medicine were rewritten as “best practices” for business professionals

Chemistry and CS stayed stable: semantic degradation was domain-specific

Why this matters: Most LLM eval frameworks focus on factual accuracy and hallucination rates. But our data suggests the real long-term risk may be subtle, systematic recontextualization. Outputs can look factual and well-structured, while completely losing their intended purpose. This may impact content authenticity, training data curation, and long-term epistemic stability.

📄 Full paper (ResearchGate) - https://www.researchgate.net/publication/392558645_The_Half-Life_of_Truth_Semantic_Drift_vs_Factual_Degradation_in_Recursive_Large_Language_Model_Generation

🧵 Medium summary for general audience - https://medium.com/@maxwell.ian/when-ai-loses-its-mind-but-keeps-the-facts-the-hidden-danger-of-recursive-ai-content-08ae538b745a",101,47,2025-06-11 02:35:19,Actual_Requirement58,-0.9713,0.029,0.124,0.846,0.1368686868686869,0.4803030303030303
[R] PINNs are driving me crazy. I need some expert opinion,"Hi!

I'm a postdoc in Mathematics, but as you certainly know better than me, nowadays adding some ML to your research is sexy. 

As part of a current paper I'm writing, I need to test several methods for solving inverse problems, and I have been asked by my supervisor to test also PINNs. I have been trying to implement a PINN to solve our problem, but for the love of me I cannot seem to make it converge.

Is this expected? Shouldn't PINNs be good at inverse problems?

Just to give some context, the equation we have is not too complicated, but also not too simple. It's a 2D heat equation, of which we need to identify the space-dependent diffusivity, k(x,y). So the total setup is:

\- Some observations, data points in our domain, taken at different times

\- k is defined, for simplicity, as a sum of two gaussians. Accordingly, we only have 6 parameters to learn (4 for the centers and 2 for the amplitudes), in addition to the PINNs weights and biases

\- We also strongly enforce BC and IC.

But there is no way to make the model converge. Heck, even if I set the parameters to be exact, the PINN does not converge. 

Can someone confirm me that I'm doing something wrong? PINNs should be able to handle such a problem, right?",73,43,2025-06-10 20:36:17,WAIHATT,0.3804,0.097,0.104,0.798,0.1016666666666666,0.6111309523809523
[D] Can LLVM IR + ML actually detect logic bugs?Or am i just way off?,"So lately I’ve been exploring what LLVM actually *is*, how it works with compilers like `clang`, and how it compares to GNU compilers. Turns out LLVM uses **IR (Intermediate Representation)** — which is like a middle-ground language:

* More abstract than machine code (assembly)
* Lower level than the original source code

So the conventinal flow is smtg like this or atleast what i understood( **THIS IS A BASC AF REPRESENTAION)**

SRC CODE → LLVM IR (optimizations) → Machine Code

LLVM even supports optimization levels like `-O0`, `-O1`, `-O2`, `-O3`, and `-Ofast`. In real-world builds, many people use `-O3`.

in industrial grade applications many people use the `-O3` for optimization

FOR A BASIC INTRO ABOUT THIS REFER TO THIS GUY BELOW

**Credits** \- tanmay bakshi (**LINK:** [**https://youtu.be/IR\_L1xf4PrU?si=TvT8cvsOxvscxpeb**](https://youtu.be/IR_L1xf4PrU?si=TvT8cvsOxvscxpeb)**)**

well my point being is if LLVM -IR altough given it is clang exclusive and uk works only on languages that can be compiled but considering it is independent of architecture like machine code i mean has common syntax after conversion unlike after conversion into arm code it is more dependent on the computer architecture like RISC-V,ARM etc ....

So here comes the real fun part :

What if(**A REALLY BIG IF NGL**)we could:

* Tokenize LLVM IR code
* Feed it into an ML model
* Train that model to learn patterns of bugs, optimization quality, or even semantics

Here is my fundemental understanding of it LLVM IR is:

* Language-independent (as long as it's compiled)
* Architecture-independent (unlike machine code, which is RISC-V, ARM, x86-specific)
* Capable of generating metadata (like line numbers, debug info) via `-g`, which means we can map IR issues back to source code

So this opens up a possibility:

Imagine — a future where a new language comes out, and as long as it compiles to LLVM IR, your model can still analyze it for errors without needing to know the syntax.

# But here's where I'm not sure if I'm totally wrong:

1. Maybe I’m misunderstanding how IR actually works, like i think i am missing something really fundemental as i am real starter in this field.
2. Maybe this is just not feasible .
3. Maybe someone already did this didn't achieve any proimising results

I’m okay with being wrong — I just want to understand why.

But… if this *is* possible udts this is something worth building?

",0,10,2025-06-10 18:36:41,Last-Arm-7626,0.9336,0.107,0.054,0.839,0.0671519886363636,0.4408104482323233
[P] Spy-searcher: a open source local host deep research,"Hello everyone. I just love open source. While having the support of Ollama, we can somehow do the deep research with our local machine. I just finished one that is different to other that can write a long report i.e more than 1000 words instead of ""deep research"" that just have few hundreds words. currently it is still undergoing develop and I really love your comment and any feature request will be appreciate !

(Sorry if my idea is kinda naive but love to hear your response !)

[https://github.com/JasonHonKL/spy-search/blob/main/README.md](https://github.com/JasonHonKL/spy-search/blob/main/README.md)",12,0,2025-06-10 18:14:31,jasonhon2013,0.931,0.149,0.014,0.837,0.0597222222222222,0.4652777777777778
[R] LoRMA: Low-Rank Multiplicative Adaptation for LLMs,"**Title:** LoRMA: Low-Rank Multiplicative Adaptation for LLMs

**Abstract:** Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.

**Venue:** ACL Findings 2025

**Paper:** [https://arxiv.org/abs/2506.07621](https://arxiv.org/abs/2506.07621)

**Summary:** [https://exploration-lab.github.io/LoRMA/](https://exploration-lab.github.io/LoRMA/)

**We’d love to hear your thoughts, feedback, or questions on this work!**",18,1,2025-06-10 16:49:24,Eastern_Ad1737,0.9674,0.131,0.0,0.869,0.1137301587301587,0.6063492063492063
[P] Built a financial analyzer agent using mcp-agent. Here's how I got it to produce high-quality reports,"I recently built a [financial analyzer agent](https://github.com/lastmile-ai/mcp-agent/tree/main/examples/usecases/mcp_financial_analyzer) that pulls stock-related data from the web, verifies the quality of the information, analyzes it, and generates a structured markdown report. (My partner needed one, so I built it to help him make better decisions lol.) It’s fully automated and runs locally using MCP servers for fetching data, evaluating quality, and writing output to disk.

At first, the results weren’t great. The data was inconsistent, and the reports felt shallow. So I added an EvaluatorOptimizer, a function that loops between the research agent and an evaluator until the output hits a high-quality threshold. That one change made a huge difference.

In my opinion, the real strength of this setup is the orchestrator. It controls the entire flow: when to fetch more data, when to re-run evaluations, and how to pass clean input to the analysis and reporting agents. Without it, coordinating everything would’ve been a mess. Plus, it’s always fun watching the logs and seeing how the LLM thinks! I would love to hear your feedback or learn about what workflows you are automating using agents!",12,6,2025-06-10 15:07:48,InitialChard8359,0.9735,0.125,0.012,0.863,0.2534313725490196,0.4137254901960784
[D] Penalize false negatives,"Hi. Im trying to train a binary classification model for disease detection in plant. Since the cost of falsely detecting a healthy plant is more severe, i want to train the model such that it can prioritize reducing false negatives. I heard that you can just adjust the threshold during evaluation but is there any other methods to achieve this? Or would simply adjusting the threshold be sufficient? Would something like weighted binary crossentropy loss help?",2,8,2025-06-10 14:46:10,Horror_Put8474,0.6791,0.123,0.057,0.82,-0.040625,0.5040178571428572
[P] GNNs for time series anomaly detection (Part 2),"Hey everyone! 👋

A while back, we [posted](https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/) about our project, GraGOD, which explores using Graph Neural Networks (GNNs) for Time Series Anomaly Detection. The feedback in the post was really positive and motivating, so with  a lot of excitement we can announce that we've now completed our thesis and some important updates to the repository!

For anyone who was curious about the project or finds this area of research interesting, the full implementation and our detailed findings are now available in the repository. We'd love for you to try it out or take a look at our work. We are also planning on dropping a shorter paper version of the thesis, which will be available in a couple of weeks.

🔗 **Updated Repo:** [GraGOD - GNN-Based Anomaly Detection](https://github.com/GraGODs/GraGOD)  
🔗 **Original Post:** [P GNNs for time series anomaly detection](https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/)

A huge thank you to everyone who showed interest in the original post! We welcome any further discussion, questions, or feedback. If you find the repository useful, a ⭐ would be greatly appreciated.

Looking forward to hearing your thoughts!",42,13,2025-06-10 13:17:26,Important-Gear-325,0.9908,0.208,0.0,0.792,0.3394719251336898,0.567379679144385
[D] We Need a Birth Certificate for AI Agents — Here’s a Proposal,"As more AI agents are built, deployed, and shared, we’re hitting a wall: there’s no standard way to describe what an agent does, what it needs to run, or what it claims to be capable of.

So I’ve been working on a lightweight open format called the **Agent Definition Schema (ADS)** — it’s like a package.json for AI agents. It includes capabilities, input/output contracts, runtime expectations, and even optional skill claims.

💡 **Why**?

* To enable chaining and orchestration of agents
* To verify what skills/credentials an agent *claims* to have
* To allow search, filtering, and discovery in marketplaces or registries

📄 Read more here:

[https://medium.com/@adyrcz/why-every-ai-agent-will-need-a-birth-certificate-by-2026-and-how-were-building-it-719ba791e4e3](https://medium.com/@adyrcz/why-every-ai-agent-will-need-a-birth-certificate-by-2026-and-how-were-building-it-719ba791e4e3)

GitHub spec repo: [https://github.com/agent-schema/ads-spec](https://github.com/agent-schema/ads-spec)

Live site: [https://agent-manifest.org](https://agent-manifest.org/)

Curious what folks here think — especially those working on LLMops, chaining frameworks, or autonomous agent deployments.",0,7,2025-06-10 11:19:18,adyrcz,0.8176,0.084,0.016,0.9,0.012396694214876,0.6454545454545455
[D] Creating SLMs from scratch,"Hi guys,

I am a product manager and I am really keen on exploring LLMs and SLMs. I am not a developer but am looking to build some own custom SLMs for my own business project. For this, I have watched some tutorials along with reading concepts and learning the LLM architecture through tutorials.

So, taking into account vast tutorials and the option to fine tune LLMs, help me with the below pointers-
1. To build SLMs from scratch, is it good enough to know in detail about how the code performs and then using the code mentioned in any open source repository to build your own self tuned SLMs?
2. For understanding Machine Learning papers, I wish to focus on the gist of the paper that helps me to understand the underlying concepts and processes mentioned in paper. What is the best way to go about reading such papers?
3. Is it better to use open source models in fine tuning or learn to understand SLMs architecture in detail to build and try out SLM projects for my own conceptual understanding?",25,15,2025-06-10 09:52:32,som_samantray,0.9854,0.16,0.0,0.84,0.3755555555555556,0.6400000000000001
[R] Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA,"[Paper page](https://huggingface.co/papers/2505.21115)

[Github](https://github.com/s-nlp/Evergreen-classification)

[Arxiv](https://arxiv.org/abs/2505.21115)

Have you ever noticed that ChatGPT sometimes searches the web for answers – and sometimes it doesn’t? Ever wondered how this “black box” actually works? In our latest paper **“Will It Still Be True Tomorrow?”**, we set out to answer this question.

Let’s consider an example: *“Who is the president of the USA?”* The answer to this question depends on the exact moment you ask it. But if you ask, “Who was the first president of the USA?” the answer is always the same, regardless of timing or context. LLMs often struggle with the first type of question – called “mutable” questions – because during pre-training, they’ve seen text stating that Barack Obama, then Donald Trump, then Joe Biden, then again Donald Trump was president. So when you ask, “Who is the president of the USA?” the answer isn’t always straightforward. However, LLMs excel at the second type of question, because the answer is a fixed historical fact that doesn’t change. In our new paper, we explore the phenomenon of 🌿evergreen questions. To distinguish between evergreen and mutable questions, we fine-tuned the [EG-E5 ](https://huggingface.co/s-nlp/E5-EverGreen-Multilingual-Large)classifier on the EverGreenQA dataset, which contains **4,757** real-user questions across 7 languages.

Our results show:

✔️ Evergreen probability consistently improves self-knowledge estimation and calibration.

✔️ Evergreen-ness is the strongest predictor of GPT-4o’s retrieval behavior, suggesting that retrieval is closely tied to temporality.

✔️ Evergreen probability is highly effective at identifying when the model knows the answer. In other words, if a question is evergreen, the model is likely to answer it correctly—but if a question is not evergreen, the outcome is harder to predict.

If you like the idea please ⭐ upvote our paper on [HuggingFace papers](https://huggingface.co/papers/2505.21115)

[The clear example of evergreen vs non-evergreen questions](https://preview.redd.it/l8dnvc88b26f1.png?width=1200&format=png&auto=webp&s=bb2ee2f12148f63bbecb9c77ad6b27bae39f8459)",7,1,2025-06-10 08:46:29,daniil_mos,0.9846,0.107,0.009,0.884,0.1559848484848484,0.3806439393939394
[P] Finding indirect or deep intents from a given keyword,"I have been given a project which is intent-aware keyword expansion. Basically, for a given keyword / keyphrase, I need to find indirect / latent intents, i.e, the ones which are not immediately understandable, but the user may intend to search for it later. For example, for the keyword “running shoes”, “gym subscription” or “weight loss tips” might be 2 indirect intents. Similarly, for the input keyword “vehicles”, “insurance” may be an indirect intent since a person searching for “vehicles” may need to look for “insurance” later. 

How can I approach this project? I am allowed to use LLMs, but obviously I can’t directly generate indirect intents from LLMs, otherwise there’s no point of the project.

I may have 2 types of datasets given to me: 
1) Dataset of keywords / keyphrases with their corresponding keyword clicks, ad clicks and revenue. If I choose to go with this, then for any input keyword, I have to suggest indirect intents from this dataset itself. 
2) Dataset of some keywords and their corresponding indirect intent (it’s probably only 1 indirect intent per keyword). In this case, it is not necessary that for an input keyword, I have to generate indirect intent from this dataset itself.

Also, I may have some flexibility to ask for any specific type of dataset I want. As of now, I am going with the first approach and I’m mostly using LLMs to expand to broader topics of an input keyword and then finding cosine similarity with the embeddings of the keywords in the dataset, however, this isn’t producing good results.

If anyone can suggest some other approach, or even what kind of dataset I should ask for, it would be much appreciated!",8,11,2025-06-10 06:42:46,eyerish09,0.8845,0.056,0.02,0.924,0.1516666666666666,0.4422222222222222
[D] Should I acquire some professional certificates as mid career-researcher in Generative AI,"I’m a mid-career researcher in the Generative AI domain. I regularly stay updated through the latest academic papers in our field. Recently, my company offered me the opportunity to take an online training course. While I feel I’m staying current through my own efforts, I don’t want to overlook the opportunity. I’d appreciate suggestions from experienced professionals regarding worthwhile courses or skill areas I should explore.",0,5,2025-06-10 06:38:42,Low-Rub-2600,0.875,0.15,0.0,0.85,0.25,0.4126923076923077
[P] Detect asyncio issues causing AI agent latency,"There are a lot of discussions about optimizing Python-based AI agent performance - tweaking prompts, switching to a different model/provider, prompt caching. But there's one culprit that's often overlooked: **blocked event loops**.

# The Problem

User A makes a request to your agent - expected TTFT is 600ms. But they wait 3+ seconds because User B's request (which came first) is blocking the entire event loop with a sync operation. Every new user gets queued behind the blocking request.

# Why This Happens

Most Python agent frameworks use asyncio to handle multiple users concurrently. But it's easy to accidentally use sync operations (executing sync `def` tools in the same thread) or libraries (requests, database drivers, file I/O) that block the entire event loop. One blocking operation kills concurrency for your entire application.

# The Solution

I built [pyleak](https://github.com/deepankarm/pyleak) after hitting this exact issue in our production agents. It automatically detects when your framework/your own code accidentally blocks the event loop or if there are any asyncio task leaks along with the stack trace.

# Usage

    pip install pyleak

# As a context manager

    from pyleak import no_event_loop_blocking, no_task_leaks
    
    async with no_event_loop_blocking(threshold=0.1), no_task_leaks():
        # Raises if anything blocks >100ms or if there are any asyncio task leaks
        ...

# As a pytest plugin

    import pytest
    
    @pytest.mark.no_leak
    async def test_my_agent():
        # Test fails if it blocks event loop or leaks tasks
        ...

# Real example

`openai-agents-python` sdk faces this exact issue where a tool defined as a `def` function blocks the event loop. We caught this thanks to `pyleak` and proposed a fix. PR: [https://github.com/openai/openai-agents-python/pull/820](https://github.com/openai/openai-agents-python/pull/820)",6,2,2025-06-10 06:33:29,deepankarmh,-0.9807,0.047,0.149,0.804,0.0562998405103668,0.4800637958532694
[P] DAB: A Benchmark for Evaluating AI Robustness to Noisy and Incoherent Queries,"Hi everyone,

I wanted to share a research project I’ve been working on: **DAB (Death AGI Benchmark)**. Most existing AI benchmarks assume users provide clean, well-structured queries, but that’s not how people communicate in the real world—actual queries can be noisy, ambiguous, contradictory, or full of typos.

**DAB** is a benchmark suite designed to challenge models with exactly those kinds of difficult, real-life prompts. The idea is to see how current models perform when the input is unclear, inconsistent, or just plain messy—not just the typical “textbook” cases.

**Motivation:**  
Modern LLMs perform impressively on well-posed questions, but tend to break down when faced with ambiguity or “messy” real-world language. DAB is intended to help evaluate and track model robustness in these scenarios, and hopefully spark some discussion on how we can push models to handle them better.

**What’s included:**

* A testing framework for evaluating models against these noisy/ambiguous queries.
* Initial results: Even state-of-the-art models (GPT-4.1, Claude 4, Gemini 2.5 pro 06-05, Grok 3 think, etc.) struggled—none were able to reliably solve most tasks (accuracy was 0).

If you’re interested, here’s the benchmark and a brief paper describing the methodology/results: [https://osf.io/pqwsh/](https://osf.io/pqwsh/)

I’d love to get feedback—criticisms, suggestions, ideas for new tasks, or results from your own model tests are all very welcome! (Just to be clear: this is an open, non-commercial project about model robustness, not a product or anything.)

Thanks for reading!",0,10,2025-06-10 04:35:57,No_Arachnid_5563,0.988,0.167,0.044,0.789,0.1791615130900845,0.5110325190682333
[D] Seeking precedent for prompt-driven data mining,"I have a large corpus of multi-document case files (each containing dozens-hundreds of documents/notes in natural language text). My company sells products to forecast outcomes and recommend handling for these cases. Each case report contains tons of detailed information (often in inscrutable shorthand), much of which is orthogonal to my current purpose.

I’ve found this boneheadedly simple workflow absurdly helpful to understand my problem and our products:

1. filter down to subset of <1k cases
2. summarize each case with an LLM prompt to extract information I'm curious about
3. embed LLM summaries
4. cluster embeddings
5. summarize clusters by sampling from cluster assignments. Can resample for a kind of qualitative pseudo-bootstrap-standard-error

Embedding the raw text includes many details which I don’t necessarily care about, and downstream clusters will reflect that.

I'm looking for

1. Literature, precedent, or anecdotes related to “prompt-driven data mining”
2. Ideas to extend this approach to more general data mining techniques, E.G:
   1. Something like CCA to identify common factors btw multiple summaries for the same case (eg before/after some treatment)
   2. Something like FWL to explain errors of an ML model that uses real-valued features, and subsequently summarize major factors
3. Tricks to scale this beyond 1k (would be nice if I could prompt the embedding model directly)",0,7,2025-06-10 00:00:03,,0.9349,0.098,0.028,0.874,0.0887156925200403,0.5070061580931147
[P] A chrome extension to remove slop from the internet,"Hey guys I was getting tired of having 90% of my google searches returning slop so I decided to create a chrome extension to tag them.

For the model I basically scrapped some websites for slop vs non-slop, then used those to train a custom implementation of fasttext with additional features, pruned and optimized until I got a very fast, lightweight model.

I gotta say the results are not 100% perfect (the model is pretty simple and the task, pretty complex), but I'm pretty happy with the results.

If you are interested or have any feedback please feel free to comment, you can check the details

* [**Github**](https://github.com/elalber2000/stop_slop)
* [**Gradio Demo**](https://github.com/elalber2000/stop_slop/blob/main/extension.crx) (with some nice interpretability visualization)
* [**Chrome Extension**](https://github.com/elalber2000/stop_slop/blob/main/extension.crx)
* [**Raw HTML Dataset**](https://huggingface.co/datasets/elalber2000/stop-slop-data-html)
* [**Parsed Text Dataset**](https://huggingface.co/datasets/elalber2000/stop-slop-data)

https://preview.redd.it/85apln26dy5f1.png?width=800&format=png&auto=webp&s=e84cbbdffefc70049565a49aead17d706f71110e

  
",5,0,2025-06-09 19:25:27,albertus2000,0.9816,0.216,0.027,0.758,0.2407100591715976,0.7691293322062553
[D] What underrated ML techniques are better than the defaults,"I come from a biology/medicine background and slowly made my way into machine learning for research. One of the most helpful moments for me was when a CS professor casually mentioned I should ditch basic grid/random search and try Optuna for hyperparameter tuning. It completely changed my workflow, way faster, more flexible, and just better results overall.

It made me wonder what other ""obvious to some, unknown to most"" ML techniques or tips are out there that quietly outperform the defaults?

Curious to hear what others have picked up, especially those tips that aren’t widely taught but made a real difference in your work",192,82,2025-06-09 17:59:26,NOAMIZ,0.761,0.093,0.0,0.907,0.0874999999999999,0.4888888888888888
[D] Is Google colab pro+ sufficient for my project?,"I have currently started my thesis and the goal is to run a LLM/ VLM 8B model or any model larger than 8B and then finetune it with datasets that contains images like x rays. I am planning to finetune using colab pro+, will it be enough?",0,19,2025-06-09 17:02:07,Apstyles_17,0.4329,0.053,0.0,0.947,0.0,0.4666666666666666
"[P] Built a multimodal Avatar, to be my career spokesperson via FineTuned TTS, and LipDubbing audio conditioned model","Hey everyone, I recently built a personal project where I created an AI avatar agent that acts as my spokesperson. It speaks and lip-syncs like Vegeta (from DBZ) and responds to user questions about my career and projects.

**Motivation**:  
In my previous role, I worked mostly with foundational CV models (object detection, segmentation, classification), and wanted to go deeper into multimodal generative AI. I also wanted to create something personal, a bit of engineering, storytelling, and showcase my ability to ship end-to-end systems. See if it can standout to hiring managers.

**Brief Tech Summary**:

– Fine-tuned a VITS model([Paper](https://arxiv.org/abs/2106.06103)), this is an end to end TTS model, directly converting to waveform without intermittent log mel spectogram

– Used MuseTalk ([Paper](https://arxiv.org/abs/2410.10122)) low latency lip-sync model, a zero shot video dubbing model, conditioned by audio

– Future goal: Build a WebRTC live agent with full avatar animation

Flow -> User Query -> LLM -> TTS -> Lip Dubbing Model -> Lip Synced Video

**Limitations**

– Phoneme mismatches for certain names due to default TTS phoneme library

– Some loud utterances due to game audio in training data

[Demo Link](https://huggingface.co/spaces/santoshr24/Vegeta_avatar)

I’d love feedback on:

– How I can take this up a notch, from the current stage?",8,0,2025-06-09 16:33:30,kutti_r24,0.9022,0.071,0.01,0.919,0.0602212602212602,0.402579365079365
[P][R] Sparse Transformers: Run 2x faster LLM with 30% lesser memory,"We have built fused operator kernels for structured contextual sparsity based on the amazing works of LLM in a Flash (Apple) and Deja Vu (Zichang et al). We avoid loading and computing activations with feed forward layer weights whose outputs will eventually be zeroed out.

The result? We are seeing **5X faster MLP layer performance** in transformers with 50% lesser memory consumption avoiding the sleeping nodes in every token prediction. For Llama 3.2, Feed forward layers accounted for **30% of total weights** and forward pass computation resulting in **1.6-1.8x increase** in throughput:

    Sparse LLaMA 3.2 3B vs LLaMA 3.2 3B (on HuggingFace Implementation):
    - Time to First Token (TTFT):  1.51× faster (1.209s → 0.803s)
    - Output Generation Speed:     1.79× faster (0.7 → 1.2 tokens/sec)  
    - Total Throughput:           1.78× faster (0.7 → 1.3 tokens/sec)
    - Memory Usage:               26.4% reduction (6.125GB → 4.15GB)

Please find the operator kernels with differential weight caching open sourced (Github link in the comment).

PS: We will be actively adding kernels for int8, CUDA and sparse attention.

Update: We also opened a [discord server](https://discord.gg/CxzDDffR) to have deeper discussions around sparsity and on-device inferencing.",74,13,2025-06-09 13:11:57,Economy-Mud-6626,0.5859,0.043,0.024,0.933,0.0895833333333333,0.6041666666666666
[R][D] Let’s Fork Deep Learning: The Hidden Symmetry Bias No One Talks About,"**Edit**: [A *draft* blog explaining this is now available.](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c)

I’m sharing a bit of a passion project. It's [styled](https://neurips.cc/Conferences/2025/CallForPositionPapers) as a position paper outlining alternative DL frameworks. Hopefully, it’ll spur some interesting discussions. It is a research agenda which includes **how to produce and explore new functions for DL from symmetry principles.**

*TL;DR: The position paper highlights a potentially 82-year-long hidden inductive bias in the foundations of DL affecting most things in contemporary networks --- offering a full-stack reimagining of functions and perhaps an explanation for some interpretability results.* ***Raising the question: why have we overlooked the foundational choice of elementwise functions?***

Three testable predictions emerge with our current basis-dependent elementwise form:

* **Neural Refractive Problem:** Semantics bend due to our current choice of activation functions. This may limit the expressibility of our networks.
* **Discretised Semantics:** This hidden inductive bias appears to encourage activations to group up into quantised positions, much like Superposition or Neural Collapse. This is proposed to limit representation capacity.
* **Weight Locking:** A broken symmetry breaks the direct connectivity between minima from a continuous symmetry, which may produce spurious local minima. This may limit learning.

To remedy these, a complete fork of DL is proposed as a starting point. But this is just a case study. **The actual important part is that this is just one of many possible forks.** To the best of my knowledge, this is the first of such a proposal. I hope this gets the field as excited as I am about all the possibilities for new DL implementations.

Here are the papers:

* [Position Paper](https://doi.org/10.5281/zenodo.15476947) (pending arXiv)
* [Empirical Evidence](https://arxiv.org/abs/2505.13471) from ICLR Realign workshop.

Preface:

The following is what I see in this proposal, but I’m tentative that *this may just be excited overreach speaking*. A note on the title: I got suggested the title as good for a Reddit article, but in hindsight it is phrased a bit clickbaity, though both claims I feel are genuinely faithful to the work.

————————— Brief summary: —————————

The work discusses the current geometry of DL and how a subtle inductive bias may have been baked in since the field's creation, and is not as benign as it might first appear... it is a basis dependence buried in nearly all functions. Representations become subtly influenced and this may be partially responsible for some phenomena like superposition.

This paper extends the concept beyond a new activation function or architecture proposal. The geometry perspective appears to **shed light on new islands of DL to explore**, producing group theory machinery to build DL forms given any symmetry. I used rotation, but it extends further than this.

This appears to affect Initialisers, Normalisers, Regularisers, Operations, Optimisers, Losses, and more - hence the **new fork suggestion**, which only leaves the underlying linear algebra defining DL generally untouched.

The proposed ‘rotation’ island is ‘Isotropic deep learning’, but it is just to be taken as *an example case study*, hopefully a beneficial one, which may mitigate the conjectured representation pathologies presented. But the possibilities are endless (elaborated on in Appendix A).

I hope it encourages a directed search for potentially **better DL branches!** Plus new functions. And perhaps the development of the conjectured ‘**G**rand’ **U**niversal **A**pproximation **T**heorem, if one even exists, which would elevate UATs to the symmetry level of graph automorphisms, identifying which islands (and architectures) may work, and which can be quickly ruled out.

Also, this may enable **dynamic topologies** with minimal functionality loss as the network restructures. Is this a route to explore the Lottery Ticket Hypothesis further?

It’s perhaps a daft idea, but one I’ve been invested in exploring for a number of years now, through my undergrad during COVID, till now. I hope it’s an interesting perspective that stirs the pot of ideas

————————— What to expect:—————————

Heads up that this paper is more like that of my native field of physics, theory and predictions, then later verification, rather than the more engineering-oriented approach. Consequently, please don’t expect it to overturn anything in the short term; there are no plug-and-play implementations, functions are merely illustrative placeholders and need optimising using the latter approach.

But I do feel it is important to ask this **question about one of the most ubiquitous and implicit foundational choices in DL**, as this backbone choice seems to affect a lot. I feel the implications could be quite big - help is welcome, of course, we need new useful branches, theorems on them, new functions, new tools and potentially branch-specific architectures. Hopefully, this offers fresh perspectives, predictions and opportunities. Some bits approach a philosophy of design to encourage exploration, but there is no doubt that the adoption of each new branch primarily rests on empirical testing to validate *each* branch.

\[Edited to improve readability and make headline points more straightforward\]",39,71,2025-06-09 11:19:28,GeorgeBird1,0.9983,0.14,0.034,0.826,0.1438714063714064,0.4532536907536907
[D] BMVC 2025 Reviews Discussion,"So BMVC 2025 reviews are supposed to be out by today (June 9, 2025). Thought it'd be nice to have a reviews discussion thread here, since I didn't see one already. Feel free to discuss any reviews you've received.",4,34,2025-06-09 10:43:46,fullgoopy_alchemist,0.7269,0.132,0.0,0.868,0.5,0.9
[D] JMLR Publishing procedure,"I submitted a paper to JMLR last month and was expecting an AE (Action Editor) to be assigned within a month, since that seems to be the usual timeline according to their website. But it’s been over 5 weeks now and still no AE has been assigned. I haven’t received any rejection email either, and the submission system still just says “decision: none yet”

I emailed the editorial team over a week ago and sent a follow-up as well — still no response. Since this is my first paper submission, I’m not sure if this kind of delay is normal for JMLR or ML journals in general, or if something might be wrong with my submission.

Would really appreciate any insight from folks who’ve published there or gone through something similar!",8,18,2025-06-09 09:41:43,Foreign_Sympathy2863,-0.9323,0.046,0.142,0.812,0.0318181818181818,0.4717171717171717
[P] Why does my AI finally stop making things up? (Open Source COMPASS approach inside),"Hi folks,

Ever noticed how most AIs tend to make up answers when you ask them something abstract, tricky, or outside the training data? That’s been bugging me for a while—so I set out to fix it.

After a lot of trial and error, I developed a new approach that (mostly) stops the AI from hallucinating. Now, instead of inventing plausible nonsense, it actually tells me when it can’t answer or when something doesn’t add up.

I call it the **COMPASS Framework**. Instead of just trying to patch mistakes after the fact, it structurally prevents hallucination by forcing the model to check its output against explicit axioms and validated knowledge fields *before* it generates a response.

Curious if this could be useful for others (or if I’ve just invented a complicated way for the AI to say “I don’t know” a lot!). If you want to see the technical side, here’s the open paper and the code:

• \[Paper (OSF Preprint)\](https://osf.io/r7w86/files/osfstorage/684464ca14df4180a285b1b1)  
• \[Project main page (extra info, code, data)\](https://osf.io/r7w86/)  
• \[GitHub (COMPASS Codebase)\](https://github.com/dwpplumb/COMPASS-Framework-Prompt-Demos)

Would love to hear your thoughts or hear about your own experience with hallucinations in LLMs. Does anyone else wish their model would just admit when it doesn’t know?",0,10,2025-06-08 21:14:40,Federal_Cookie2960,0.7124,0.086,0.059,0.854,0.1432239057239057,0.4743265993265992
[Discussion] ACM Multimedia 2025 Reviews & Rebuttal,"ACM Multimedia 2025 reviews will be out soon (official date is Jun 09, 2025). I am creating this post to discuss about the reviews and rebuttal here.

The rebuttal and discussion period is Jun 09-16, 2025. This time the authors and reviewers are supposed to discuss using comments in OpenReview! What do you guys think about this?

\#acmmm #acmmm2025 #acmmultimedia",19,275,2025-06-08 17:44:08,stalin1891,0.3595,0.037,0.0,0.963,0.0,0.0
[D] Looking for Intuitive Resources to Understand Flow Matching (Beyond the Original Paper),"Hi, I'm currently trying to wrap my head around **flow matching**, the newer technique used in generative models. I’ve gone through the paper [https://arxiv.org/abs/2210.02747](https://arxiv.org/abs/2210.02747), but I find it a bit hard to grasp intuitively.

Are there any good resources that explain it more clearly or step-by-step? Also, I’d love to know the foundational ideas or works that flow matching builds on. For context, I already have a solid understanding of **diffusion models** and **score matching**.

Any pointers or recommendations would be greatly appreciated!",18,14,2025-06-08 15:15:25,Any_Damage_7715,0.9711,0.205,0.015,0.78,0.237037037037037,0.4416666666666666
[D] is there a mistake in the RoPE embedding paper?,"i'm reading the paper about rope embedding but there's something weird in equation 16, we start from 

```
q_m.T*k_n = (R_m*W_q*x_m).T*(R_n*W_k*x_n)
```
and computing the transpose of the first term we get 

```
q_m.T*k_n = (W_q*x_m).T * R_m.T * R_n * W_k * x_n)
          = x_m.T * W_q.T * (R_m.T * R_n) * W_k * x_n
          = x_m.T * W_q.T * R_n-m * W_k * x_n
```

in my case in the final step i get the transpose of the W_q matrix but in the paper at that point the matrix is not transposed, is that a mistake or i am missing something?",48,11,2025-06-08 12:47:15,samas69420,-0.8406,0.0,0.102,0.898,-0.325,0.7305555555555556
[R] Machine learning with hard constraints: Neural Differential-Algebraic Equations (DAEs) as a general formalism,,53,13,2025-06-08 11:36:43,ChrisRackauckas,-0.1027,0.0,0.104,0.896,-0.1208333333333333,0.5208333333333333
[P] BERT-Emotion: Lightweight Transformer Model (~20MB) for Real-Time Emotion Detection,"Hi all,

I am sharing **BERT-Emotion**, a compact and efficient transformer model fine-tuned for short-text emotion classification. It supports **13 distinct emotions** such as Happiness, Sadness, Anger, and Love.

**Key details:**

* **Architecture**: 4-layer BERT with hidden size 128 and 4 attention heads
* **Size**: \~20MB (quantized), suitable for mobile, IoT, and edge devices
* **Parameters**: \~6 million
* Designed for offline, real-time inference with low latency
* **Licensed** under Apache-2.0, free for personal and commercial use

The model has been downloaded over **11,900 times last month**, reflecting active interest in lightweight NLP for emotion detection.

**Use cases** include mental health monitoring, social media sentiment analysis, chatbot tone analysis, and smart replies on resource constrained devices.

Model and details are available here:  
[https://huggingface.co/boltuix/bert-emotion](https://huggingface.co/boltuix/bert-emotion)

*I welcome any feedback or questions!*

For those interested, **full source code & dataset are available** in a detailed walkthrough on YouTube.",28,13,2025-06-08 10:12:13,boltuix_dev,0.9719,0.193,0.067,0.741,0.1519841269841269,0.4733134920634921
[D] RL model reasoning and tool use,"Hey folks! 👋

I’ve been super curious lately about recent advances in RL training for LLMs, especially in verifiable domains like math, coding — where you can actually propagate signal to the model that aligns with a final goal. DeepSeek-RL (R1-Zero) really caught my eye — GPRPO training directly after SFT, with models learning to reason, plan, and act in grounded environments.

That got me thinking about how to integrate tool use into RL training directly. I’ve been comparing two approaches and would love to hear what you all think is more scalable or practical in multi-step scenarios:

Approach 1: Tool calls embedded in the thinking step The LLM learns to insert tool invocations inline, using delimiters like <tool>...</tool> during generation. Once the tool block is completed, it's executed and the output is returned to the model as context. Training is end-to-end with PPO, and the model’s action space is just language tokens. It learns when and how to use tools as part of its reasoning. The ReTool paper from ByteDance is a great example.

Approach 2: Tool calls as separate actions (discrete/hierarchical) Tool use is modeled explicitly as actions — e.g., selecting <search> or <python> in an MDP. You can also structure it hierarchically: one module plans which tool to use, another generates the input (like Cursor). You get a more interpretable separation of reasoning and acting. This still uses PPO/GRPO, but with finer-grained reward and tool-level transitions. Tool-LLMs like Tool-Star follow this setup.

🤔 So I’m wondering — is it better to integrate tool use within the thinking step, or treat it as a separate, structured decision with its own reward logic?

Would love to hear thoughts, experiences, or any papers you’d recommend!",3,1,2025-06-07 17:03:09,amindiro,0.9913,0.138,0.006,0.856,0.2346491228070175,0.5350877192982456
[R] Transferring Pretrained Embeddings,"While doing some work with custom vocabularies and model architectures, I have come across some evidence that the transferability of embedding layers to different tasks/architectures is more effective than previously thought. When differences such as dimensionality, vocabulary mismatches are controlled, the source of the embedding seems to make a larger difference, even when frozen, and even when moved into a different transformer architecture with a different attention pattern.

Is anyone else looking into this? Most of the research I’ve found either mixes encoder and decoder components during transfer or focuses on reusing full models rather than isolating embeddings. In my setup, I’m transferring only the **embedding layer**—either from a pretrained **LLM (Transformer)** or a **shallow embedding model**—into a fixed downstream **scoring model** trained from scratch. This allows me to directly evaluate the transferability and inductive utility of the embeddings themselves, independent of the rest of the architecture.

How can I make this more rigorous or useful? What kinds of baselines or transfer targets would make this more convincing? Is this worthy of further inquiry?

Some related work, but none of it’s doing quite the same thing:

* **Kim et al. (2024)** — *On Initializing Transformers with Pre-trained Embeddings* studies how pretrained token embeddings affect convergence and generalization in Transformers, but doesn’t test transfer into different downstream architectures.
* **Ziarko et al. (2024)** — *Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe* explores how to best extract embeddings from LMs for reuse, but focuses on efficiency and precomputation, not scoring tasks.
* **Sun et al. (2025)** — *Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs* reuses embeddings in alignment pipelines, but assumes fixed model architectures and doesn’t isolate the embedding layer.

Happy to share more details if people are interested.

(disclaimer: written by a human, edited with ChatGPT)",41,12,2025-06-07 16:47:37,Arkamedus,0.9859,0.109,0.012,0.879,0.2144393241167434,0.4901689708141321
[D] Train Test Splitting a Dataset Having Only 2 Samples of a Class Distribution,"My dataset has a total of 3588 samples, and the number of samples per class is as follows:

Benign: 3547 samples,  
DoS: 21 samples,  
Gas Spoofing: 2 samples,  
RPM Spoofing: 10 samples,  
Speed Spoofing: 5 samples,  
Steering Wheel Spoofing: 3 samples,

As you can see, the dataset is extremely imbalanced, and I am confused about how to train my ML models using the train-test split. Classes with 2 or 3 samples would have only 1 sample in the Test set for evaluation using the stratify parameter of Sklearn's train\_test\_split.

Also, having 1 sample in the Test set means either my model predicts the sample correctly and achieves 100% recall for that class, or else 0% if it fails to predict correctly. How should I train my ML models in this case? Also, collecting more samples isn't possible.",8,29,2025-06-07 15:58:25,Flexed_Panda,-0.3612,0.025,0.036,0.939,-0.0027777777777777,0.8055555555555556
[D] Got access to Gemini Diffusion (text-based) and it's lightning fast,"[Pretty good at reasoning tasks as well. And it's blazing fast. Hope this comes to commercial models soon!](https://preview.redd.it/289rkjq74i5f1.png?width=1833&format=png&auto=webp&s=109ae25fc9f2f837fc541ea3aab27bbc2b85512f)

",54,21,2025-06-07 12:46:57,hiskuu,0.8016,0.24,0.0,0.76,0.2699999999999999,0.56
[R] Log-Linear Attention,"Super new research, from the authors of FlashAttention and Mamba(2):  
[https://arxiv.org/abs/2506.04761](https://arxiv.org/abs/2506.04761)

Long Story Short: They extend Mamba2 to have state that can is not fixed and can grow in time, directly increasing Long Range Performance. This seem a sweet point between traditional Mamba2 where the state is fixed sized, being an bottleneck for long sequences, and Attention which is stateless, but need to store past KV pairs! All with specialised Triton kernels!",127,4,2025-06-07 08:34:07,Potential_Duty_6095,0.658,0.082,0.0,0.918,-0.0246867715617715,0.467016317016317
[D] Dramatizing the Birth of Reinforcement Learning — A Biopic-Style Learning Experience?,"Hello everyone

I have an idea I’d like to share and get feedback on.

What if there was a dramatized, dialogue-driven series that reconstructs the invention and evolution of Reinforcement Learning — as if you were watching it happen in real time?

Not just a documentary or lecture, but something like:
Oppenheimer meets Khan Academy meets Westworld.

Imagine:

Researchers arguing over key concepts like TD(lambda)

Moments where policy gradients are first scribbled on a chalkboard

Theorems and proofs explained through conversations

Intense debates, critiques — the actual story of how RL was developed


It wouldn’t be slow chalkboard derivations, but immersive scenes filled with mathematically accurate dialogue, creative tension, and the feel of doing real research.

The idea is that this could be a better way to learn RL (and potentially other fields) — by reconstructing the discovery process in an engaging, narrative format that mirrors how real ideas unfold.

Has anything like this been done before?
Do you think it’s worth pursuing — even as a small pilot?
Would you watch something like this?

Appreciate any thoughts or feedback.

Thanks!",0,4,2025-06-07 04:46:51,Worldly_Inside9464,0.9852,0.189,0.033,0.779,0.1697368421052631,0.4653508771929824
[D] Reproducing/Implementing Research Papers,"I'm currently pursuing a Master’s in Data Science & Applied Statistics (Non-Thesis track). I don’t have experience working with research papers, but I’m considering reproducing or implementing a research paper from scratch (Attention, ResNet & BERT) and showcasing it on my resume.

I was wondering how beneficial would this be for gaining experience or standing out to employers? Thank you in advance!",33,12,2025-06-07 03:22:54,thapaa3,0.902,0.163,0.0,0.837,0.0,0.4
[R] How to handle internal integrators with linear regression?,"For linear regression problems, I was wondering how internal integrators are handled. For example, if the estimated output y_hat = integral(m*x + b), where x is my input, and m and b are my weights and biases, how is back propagation handled?

I am ultimately trying to use this to detect cross coupling and biases in force vectors, but my observable (y_actual) is velocities.",1,8,2025-06-06 20:07:17,not_kevin_durant_7,-0.2982,0.0,0.033,0.967,0.0,0.2
[R] Better quantization: Yet Another Quantization Algorithm,"We're introducing Yet Another Quantization Algorithm, a new quantization algorithm that better preserves the original model's outputs after quantization. YAQA reduces the KL by >30% over QTIP and achieves an even lower KL than Google's QAT model on Gemma 3.

See the paper [https://arxiv.org/pdf/2505.22988](https://arxiv.org/pdf/2505.22988) and code [https://github.com/Cornell-RelaxML/yaqa](https://github.com/Cornell-RelaxML/yaqa) for more details. We also have some prequantized Llama 3.1 70B Instruct models at [https://huggingface.co/collections/relaxml/yaqa-6837d4c8896eb9ceb7cb899e](https://huggingface.co/collections/relaxml/yaqa-6837d4c8896eb9ceb7cb899e)",42,3,2025-06-06 16:13:09,tsengalb99,0.7096,0.109,0.03,0.861,0.4022727272727272,0.5409090909090909
[P] Built an Open-Source Educational AI Platform,"I'm a data science engineering student from Cameroon, and I just completed my final year project that I'd like to share with you all.

# What I Built:

I created an open-source educational AI platform that combines document management with AI-powered learning tools. Users can:

* Create and share document repositories
* Select repos to feed into a RAG system that powers an LLM
* Generate courses and quizzes from their selected documents
* Perform math operations through a custom SQL-like query language I built for sympy integration

# The Tech Stack:

* **Frontend**: Streamlit
* **Backend**: Supabase
* **Embeddings**: all-MiniLM-L6-v2
* **LLM**: Gemini
* **Custom Feature**: ""Sympy Query Language"" - SQL-style syntax for mathematical operations

# The Motivation:

Living in Cameroon, I wanted to build something accessible for students and educators in resource-constrained environments. Every design decision prioritized cost-effectiveness while maintaining interactive and personalized learning features.

# What I'm Looking For:

**1. Testing & Feedback**: I need honest feedback on bugs, UX issues, confusing features, or any problems you encounter.

**2. Expert Advice**: As someone still learning, I'd appreciate suggestions for improvements from experienced professionals. What would you do differently?

**3. Career Readiness Assessment**: Do my skills seem ready for the job market? I'm curious about where I stand professionally.

**4. Collaboration**: If this project interests you and you'd like to contribute, I'm open to collaboration.

# Final Thoughts:

This is my first major project that I'm sharing publicly. I learned a lot building it and believe it could be useful for students and educators, particularly in environments with limited resources.

The code is open-source because I believe in knowledge sharing and because I know there's room for improvement with community input.

**TL;DR**: Built an educational AI platform combining document management with AI-powered learning tools. Seeking feedback, advice, and potential collaborators.

Thanks for reading, and I appreciate any feedback you can share.

[\[Link to project\]](https://ndolecm.streamlit.app/) | [\[GitHub repo\]](https://github.com/heretounderstand/ndole/)",4,2,2025-06-06 14:31:34,Few_Challenge1726,0.9907,0.16,0.02,0.82,0.1449016563146997,0.4609213250517597
"[R] LLMs are Locally Linear Mappings: Qwen 3, Gemma 3 and Llama 3 can be converted to exactly equivalent locally linear systems for interpretability","[https://arxiv.org/abs/2505.24293](https://arxiv.org/abs/2505.24293)

[https://github.com/jamesgolden1/llms-are-llms](https://github.com/jamesgolden1/llms-are-llms)

Hello all, I'd like to share my new research describing an alternative approach to LLM interpretability. I show that transformer decoder LLMs can be made locally linear at inference time without changing outputs or weights.

**Result**: LLMs can be converted into nearly exactly equivalent linear systems that reconstruct the next-token output for any given input text sequence. Instead of 25+ layers of nonlinear computations, this method computes a single set of matrix multiplications that linearly operates on the input embedding vectors and nearly exactly reconstructs the output embedding for a single token prediction.

**Method**: A ""linear path"" through the transformer is identified, the nonlinear components are detached from the gradient, and the Jacobian with respect to the input embeddings is computed. This yields the ""detached Jacobian"", which is the set of matrices that operate linearly on input embeddings to reproduce the predicted output embedding with \~10⁻⁶ error for float32 models.

**Interpretability**: This method provides nearly-exact token attribution rather than approximate attention weights - tools from linear algebra like the SVD are used to understand which concepts drive predictions

**Scope**: Works across Qwen 3, Gemma 3, Llama 3, Phi 4, Ministral and OLMo 2 (tested up to 70B parameters at q4).

**Practical**: The method works on free Colab T4 instances for Gemma 3 4B and Llama 3.2 3B models.

**Concept steering**: Preliminary results are shown for using the detached Jacobian as a linear conceptual steering operator in mid to late layers for guided generation of 8B models.

**Trade-offs and costs**: The detached Jacobian linear system is only valid for that specific input sequence (and must be computed from scratch for each new sequence). This is slow (10 sec to compute the Jacobian for Llama 3.2 3B on a T4, up to minutes for models > 30B parameters), VRAM intensive and currently limited to very short sequences, but I plan to continue working on this aspect.

**Applications**: In addition to steering, there is some potential for safety analysis (bias detection, deceptive content).

**Background**: This extends prior work on [adaptive linear networks (Mohan, Khadkhodaie, Simoncelli et al.)](https://arxiv.org/abs/1906.05478) and [locally linear image diffusion models (Khadkhodaie, Simoncelli, et al.)](https://arxiv.org/abs/2310.02557) to transformer decoder architectures, building on [decoder circuit analysis (Elhage Nanda Olsson et al)](https://transformer-circuits.pub/2021/framework/index.html).

**Abstract**

We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Additionally, we present preliminary results on the detached Jacobian as a steering operator for inserting concepts into inference responses. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.",242,46,2025-06-06 12:55:36,jamesvoltage,0.9418,0.04,0.019,0.941,0.057834928229665,0.325633971291866
"[P] EvalGit, A tool to track your model's performance over time.","I just released EvalGit, a small but focused CLI tool to log and track ML evaluation metrics locally.

Most existing tools I’ve seen are either heavyweight, tied to cloud platforms, or not easily scriptable. I wanted something minimal, local, and Git-friendly; so I built this.

EvalGit:

\- Stores evaluation results (per model + dataset) in SQLite- Lets you query logs and generate Markdown reports

\- Makes it easy to version your metrics and document progress

\- No dashboards. No login. Just a reproducible local flow.It’s open-source, early-stage, and I’d love thoughts or contributions from others who care about reliable, local-first ML tooling.

If you are a student who wants to get more hands-on experience this project can help you.

Repo: [https://github.com/fadlgh/evalgit](https://github.com/fadlgh/evalgit)

If you’ve ever written evaluation metrics to a .txt file and lost it two weeks later, this might help. And please star the repo if possible :)",6,0,2025-06-06 09:22:16,Horror_Job_566,0.9798,0.192,0.061,0.748,0.094047619047619,0.5261904761904762
[R] What do you all think of the latest Apple paper on current LLM capabilities?,"This new Apple paper focusses on limited true reasoning capabilities in a true ""human"" way and goes into details of where LLMs and LRMs are failing on highly complex tasks.

Interesting finding around LRMs reducing their reasoning steps as the task complexity increases and overall lack of true reasoning.",103,52,2025-06-06 08:46:35,Sad_Hall_2216,0.5945,0.152,0.1,0.748,0.1649940968122786,0.4406729634002361
[D] Is there an video or article or book where a lot of real world datasets are used to train industry level LLM with all the code?,"Is there an video or article or book where a lot of real world datasets are used to train industry level LLM with all the code? Everything I can find is toy models trained with toy datasets, that I played with tons of times already. I know GPT3 or Llama papers gives some information about what datasets were used, but I wanna see insights from an expert on how he trains with the data realtime to prevent all sorts failure modes, to make the model have good diverse outputs, to make it have a lot of stable knowledge, to make it do many different tasks when prompted, to not overfit, etc. 

I guess ""Build a Large Language Model (From Scratch)"" by Sebastian Raschka is the closest to this ideal that exists, even if it's not exactly what I want. He has chapters on Pretraining on Unlabeled Data, Finetuning for Text Classification, Finetuning to Follow Instructions.
https://youtu.be/Zar2TJv-sE0

In that video he has simple datasets, like just pretraining with one book. I wanna see full training pipeline with mixed diverse quality datasets that are cleaned, balanced, blended or/and maybe with ordering for curriculum learning. And I wanna methods for stabilizing training, preventing catastrophic forgetting and mode collapse, etc. in a better model. And making the model behave like assistant, make summaries that make sense, etc. 

At least there's this RedPajama open reproduction of the LLaMA training dataset.  <https://www.together.ai/blog/redpajama-data-v2>
Now I wanna see someone train a model using this dataset or a similar dataset. I suspect it should be more than just running this training pipeline for as long as you want, when it comes to bigger frontier models. 
I just found this GitHub repo to set it for single training run.
<https://github.com/techconative/llm-finetune/blob/main/tutorials/pretrain_redpajama.md> <https://github.com/techconative/llm-finetune/blob/main/pretrain/redpajama.py>
There's this video on it too but they don't show training in detail.
https://www.youtube.com/live/_HFxuQUg51k?si=aOzrC85OkE68MeNa
There's also SlimPajama.

Then there's also The Pile dataset, which is also very diverse dataset. <https://arxiv.org/abs/2101.00027> 
which is used in single training run here. <https://github.com/FareedKhan-dev/train-llm-from-scratch> 

There's also OLMo 2 LLMs, that has open source everything: models, architecture, data, pretraining/posttraining/eval code etc. https://arxiv.org/abs/2501.00656

And more insights into creating or extending these datasets than just what's in their papers could also be nice.

I wanna see the full complexity of training a full better model in all it's glory with as many implementation details as possible. It's so hard to find such resources.

Do you know any resource(s) closer to this ideal?

Edit: I think I found the closest thing to what I wanted! Let's pretrain a 3B LLM from scratch: on 16+ H100 GPUs https://www.youtube.com/watch?v=aPzbR1s1O_8",2,8,2025-06-06 06:50:26,Happysedits,0.9828,0.099,0.046,0.855,0.2072150072150071,0.5001803751803752
[R] 100M Open source notebooklm speech model,"I've built an open source notebooklm model with two 4090's

[github.com/fluxions-ai/vui](http://github.com/fluxions-ai/vui)

demos:

[https://x.com/harrycblum/status/1930709683242713496](https://x.com/harrycblum/status/1930709683242713496)",17,1,2025-06-05 23:19:38,Useful-Performance42,0.0,0.0,0.0,1.0,0.0,0.5
[D] Robust ML model producing image feature vector for similarity search.,"Is there any model that can extract image features for similarity search and it is immune to slight blur, slight rotation and different illumination?

I tried MobileNet and EfficientNet models, they are lightweight to run on mobile but they do not match images very well.

My use-case is card scanning. A card can be localized into multiple languages but it is still the same card, only the text is different. If the photo is near perfect - no rotations, good lighting conditions, etc. it can find the same card even if the card on the photo is in a different language. However, even slight blur will mess the search completely.

Thanks for any advice.",5,17,2025-06-05 19:05:03,_dave_maxwell_,0.9264,0.141,0.044,0.815,0.0953124999999999,0.3890625
[R] Zero-Shot Vision Encoder Grafting via LLM Surrogates,"The previous post was removed due to a policy that prohibits sharing paper links only. Apologies if you’ve seen this post again. :)

Hope you find this work interesting.

https://preview.redd.it/faek3d89r65f1.jpg?width=2401&format=pjpg&auto=webp&s=2fd3bbf6ab8fef7eeca142bbcbc00c989b0ecd26

In short, this paper found that modern LLMs have a similar token transformation dynamic across layers — from input to output — characterized by two distinct transition phases. This work shows that it is possible to build a smaller surrogate model for any target LLM, enabling alignment during the early stages of training.

\[[arXiv paper](https://arxiv.org/abs/2505.22664)\] \[[code](https://github.com/facebookresearch/zero)\]",2,3,2025-06-05 16:33:47,pidoyu,0.9325,0.162,0.0,0.838,0.1006410256410256,0.4852564102564102
[P]  Need advice on my steam project,"Hey r/MachineLearning! I'm a  masters student and just wrapped up my big data analytics project. Spent a couple months on this and finally got something working that I'm pretty excited about.

TL;DR: built distributed transformer system for analyzing game reviews. Went from 30min to 2min processing time. Now unsure what to do with it? Looking for advice on next steps and feedback

github link:  [https://github.com/Matrix030/SteamLens](https://github.com/Matrix030/SteamLens)

https://preview.redd.it/p13mdqmct45f1.png?width=2348&format=png&auto=webp&s=5f5c44b7c5a618526372e26cb0f625f2970a3703

https://preview.redd.it/2wotiqmct45f1.png?width=2358&format=png&auto=webp&s=c98ee0945138f3dddbc9f4589ac8cbc3fc104a20

The Problem That Started Everything As a gamer, I always wondered how indie developers deal with hundreds of thousands of reviews. Like, the Lethal Company dev has 300k+ reviews - how do you even begin to process that feedback? There's literally no good tool for game developers to understand what players actually think about specific aspects of their games.

So I decided to build one myself for my big data project.

My Setup I'm running this on my desktop: Ryzen 9 7900X, 32GB RAM, RTX 4080 Super (16GB VRAM). Scraped Steam review data using their web API - ended up with datasets  of 40Gb containing 17M+ reviews ([available on Kaggle](https://www.kaggle.com/datasets/rishikeshgharat/steam-games-data-40-gb)).

The Sequential Nightmare My first approach was the obvious one - just process everything sequentially. 400k reviews took 30+ minutes. For my project timeline, this was painful. But more importantly, I realized no indie developer would ever use a tool that takes half an hour to analyze their reviews.

The Breakthrough (And Near Mental Breakdown) The real challenge wasn't the data processing - it was parallelizing transformers. These models are notoriously hard to distribute because of how PyTorch handles tensors and GPU memory.

My first ""working"" version gave each Dask worker its own copy of the transformer model. It worked but was eating 6x more memory than it should. With 6 workers, I was basically loading the same model 6 times.

Then came the 3AM debugging session from hell. Tensor serialization errors everywhere. CUDA tensors refusing to move between processes. Memory leaks. The works.

The fix that saved my sanity: publish the transformer model once to the Dask cluster and give each worker a handle to the same model instance. Memory usage dropped 6x, and suddenly everything was fast and stable.

What I Built The system automatically:

* Detects your hardware (CPU cores, GPU, RAM)
* Spawns optimal number of workers
* Loads transformer models once and shares across workers
* Processes reviews in parallel with intelligent batching
* Separates positive/negative sentiment before summarizing

Results That Made My Professor Happy Same 400k reviews: 30 minutes → 2 minutes (15x speedup)

The Real-World Impact This isn't just a cool technical exercise. Indie developers like the person behind Lethal Company or Stardew Valley could actually use this. Instead of manually reading through hundreds of thousands of reviews, they get automated insights like:

""Combat System - Players Love: Responsive controls and satisfying mechanics"" ""Combat System - Players Hate: Balance issues with weapon X""

Hardware Optimization:

* RTX 4080 Super: 96 samples per batch
* CPU fallback: 16 samples per batch
* Auto-cleanup prevents GPU memory explosions

The Dask Architecture:

* Dynamic worker spawning based on system specs
* Intelligent data partitioning
* Fault tolerance for when things inevitably break

Mistakes That Taught Me Everything

1. Trying to serialize CUDA tensors (learned this the hard way)
2. Not cleaning up GPU memory between batches
3. Setting batch sizes too high and crashing my system multiple times
4. Underestimating how painful distributed debugging would be

Current Limitations (Being Honest)

* Single machine only (no multi-node clusters yet)
* GPU memory still bottlenecks really massive datasets
* Error handling could be way better
* Only works with English reviews right now

Where I'm Stuck (And Why I'm Here) I finished my project, it works great, but now I'm not sure what to do with it.

But honestly? I have no idea which direction makes the most sense.

Questions for the Reddit Brain Trust:

1. Any obvious improvements to the distributed architecture?
2. Should I focus on scaling this up or polishing what I have?
3. Anyone know if game developers would actually find this useful?

The ""What's Next"" Problem I'm genuinely unsure about next steps. Part of me wants to keep improving the technical side (multi-GPU support, better scaling, model quantization). Part of me thinks I should focus on making it more user-friendly for actual game developers.

Also wondering if this could work for other domains - like analyzing product reviews on Amazon, app store reviews, etc.

Technical Challenges Still Bugging Me:

* Multi-GPU scaling within single machine
* Better memory optimization strategies
* Handling truly massive datasets (10M+ reviews)
* Real-time processing instead of batch-only

Looking for advice on next steps and feedback from anyone who's tackled similar distributed ML challenges!

Thanks for reading - any thoughts appreciated! 🎮",6,5,2025-06-05 16:23:52,Matrix_030,0.9974,0.16,0.079,0.761,0.0895452113429641,0.4520768682004636
[P][R]Is Implementing Variational Schrödinger Momentum Diffusion (VSMD) a Good ML Project for a new guy in ml? Seeking Learning Resources!,"As it says I in learning of ml to implement the research paper Variational Schrödinger Momentum Diffusion (VSMD) . 

As for a guy who is starting ml is it good project to learn .
I have read the research paper and don't understand  how it works and how long will it take to learn it .
Can you suggest the resources for learning ml from scratch .
Anyone willing to join the project?
Thank you!!",13,20,2025-06-05 14:01:47,Intelligent_Boot_671,0.8942,0.125,0.0,0.875,0.3185132575757575,0.4840909090909091
[R] Atlas: Learning to Optimally Memorize the Context at Test Time,"**TL;DR:** The team from Google Research continues to publish new SotA architectures for autoregressive language modelling, backed by thorough theoretical considerations.

**Paper:** [https://www.arxiv.org/pdf/2505.23735](https://www.arxiv.org/pdf/2505.23735)

**Abstract:**

>Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80% accuracy in 10M context length of BABILong benchmark.

Visual Highlights:

https://preview.redd.it/uo3umo13835f1.png?width=1201&format=png&auto=webp&s=7caf036556ccaae6821a471449ea885345ec42ea

https://preview.redd.it/37zdk764835f1.png?width=1301&format=png&auto=webp&s=16ea25baa246247a254e3ad0a071fc36c8178951

https://preview.redd.it/yij6yc55835f1.png?width=887&format=png&auto=webp&s=b4c4c28e9ce5abf43f1ecc301293084d6f86a45a

[Note that Atlas\(MAG\) and Atlas\(MAL\) are hybrid architectures too.](https://preview.redd.it/lhreknhq835f1.png?width=1208&format=png&auto=webp&s=0c3b92d3c7f7c1b2528c32a24897d36c6d1f0844)

https://preview.redd.it/a724x7n2a35f1.png?width=1203&format=png&auto=webp&s=1c9e7f4328f8dd10593560478e03394bf886a2e2

[Transformer behaviour on the left panel can be explained by training the model on 4k context length, without any subsequent extension. The right panel looks super-impressive](https://preview.redd.it/b2v51646a35f1.png?width=1139&format=png&auto=webp&s=5b5250bd711f7a7e3b01fb5e99a74fd9852b8b26)

",72,12,2025-06-05 11:00:58,StartledWatermelon,0.9688,0.088,0.021,0.892,0.0853233225108225,0.3870623647186148
[D] PhD in the EU,"Hi guys, I am incoming MS student at one of T5 CS institutes in the US in a fairly competitive program. I want to do a PhD and plan to shift to EU for personal reasons. I want to carry out research in computational materials science, but this may change over the course of my degree. I basically want some real advice from people currently in the EU about funding, employment opportunities,teaching opportunities, etc. I saw some posts about DeepMind fellowships, Meta fellowship etc. Are part-time work part-time PhDs common?",58,40,2025-06-05 08:03:29,simple-Flat0263,0.6705,0.093,0.0,0.907,0.1199999999999999,0.48
[D] Relevance of NeurIPS competition winners in academia,"Hi, I was looking at past competitions and I was wondering if having a go at one of these conferences is worth my time. My goal is to build my resume for when I apply for a PhD in the US this upcoming admission cycle. I want to do a PhD in CS/ML. I already have work in theoretical machine learning (1 currently in preprint and another to be sent at AISTATS). I am currently working in a lab which also does theory. I wanted to however exhibit my coding and applied ML capabilities in my CV as well. This leads me here. 

Are NeurIPS competitions well regarded in the academia? Do you get published if you end up winning? Has anyone known a winner/ is a winner in this sub?

If not this, what other avenues should I pursue for my goal? Thanks in advance. ",45,15,2025-06-05 01:03:45,GiftBrilliant6983,0.9615,0.14,0.0,0.86,0.078125,0.321875
[P] Responsible Prompting API - Opensource project - Feedback appreciated!,"Hi everyone!

I am an intern at IBM Research in the Responsible Tech team.

We are working on an open-source project called the Responsible Prompting API. This is the [Github](https://github.com/IBM/responsible-prompting-api).

It is a lightweight system that provides recommendations to tweak the prompt to an LLM so that the output is more responsible (less harmful, more productive, more accurate, etc...) and all of this is done pre-inference. This separates the system from the existing techniques like alignment fine-tuning (training time) and guardrails (post-inference).

The team's vision is that it will be helpful for domain experts with little to no prompting knowledge. They know what they want to ask but maybe not how best to convey it to the LLM. So, this system can help them be more precise, include socially good values, remove any potential harms. Again, this is only a recommender system...so, the user can choose to use or ignore the recommendations.

This system will also help the user be more precise in their prompting. This will potentially reduce the number of iterations in tweaking the prompt to reach the desired outputs saving the time and effort.

On the safety side, it won't be a replacement for guardrails. But it definitely would reduce the amount of harmful outputs, potentially saving up on the inference costs/time on outputs that would end up being rejected by the guardrails.

[This paper](https://arxiv.org/abs/2504.08757) talks about the technical details of this system if anyone's interested. And more importantly, [this paper](https://dl.acm.org/doi/10.1145/3706598.3713365), presented at CHI'25, contains the results of a user study in a pool of users who use LLMs in the daily life for different types of workflows (technical, business consulting, etc...). We are working on improving the system further based on the feedback received.

At the core of this system is a values database, which we believe would benefit greatly from contributions from different parts of the world with different perspectives and values. We are working on growing a community around it!

So, I wanted to put this project out here to ask the community for feedback and support. Feel free to let us know what you all think about this system / project as a whole (be as critical as you want to be), suggest features you would like to see, point out things that are frustrating, identify other potential use-cases that we might have missed, etc...

Here is a [demo](https://huggingface.co/spaces/santanavagner/responsible-prompting-demo) hosted on HuggingFace that you can try out this project in. Edit the prompt to start seeing recommendations. Click on the values recommended to accept/remove the suggestion in your prompt. (In case the inference limit is reached on this space because of multiple users, you can duplicate the space and add your HF\_TOKEN to try this out.)

Feel free to comment / DM me regarding any questions, feedback or comment about this project. Hope you all find it valuable!",2,4,2025-06-04 20:43:41,MysticSlice7878,0.9963,0.18,0.05,0.77,0.2117886178861788,0.5689024390243902
[P] Metadata-Augmented Transformers: Early Results & Call for Collaboration,"Transformers typically process sequences of plain tokens. We're exploring **metadata augmentation** to create semantically richer and more structured contexts. We introduce a **Metadata-Enhanced Transformer** that layers metadata on top of raw data. Early experiments show that this augmentation:

* Accelerates training convergence
* Lowers training loss
* Improves generalization
* Amplifies scaling benefits

Code, datasets, and test results: [GitHub – Metadata\_Enhanced\_Transformer](https://github.com/iliaMalinovskii/Metadata_Enhanced_Transformer)

This is a work in progress, and I’m looking for both feedback and collaborators interested in joint research.

Would love to hear your thoughts. Happy to dive deeper in replies or DMs.",0,1,2025-06-04 18:19:56,FaithlessnessEast838,0.9694,0.231,0.034,0.735,0.1262071262071262,0.5471528471528472
"[D] need real advice.. entity matching across messy scraped data, central model? field-by-field logic?","SHOUTOUT to @Solid_Company_8717 for an amazing answer in the comments below! and thank you to all that contributed!

MY ORIGINAL POST
YouTube/search engines suck these days

I’m in the weeds trying to unify messy business data across a ton of sources, directories, niche sites, scraped HTML and api responses, think sites like yellowpages and license verification like food and beverage.

So the goal is to ingest raw blob, dictionary string or imperfect parsed text

And spit out a clean, unified dictionary, aligning the right field and key, adding like logic tags like errors, missing fields for pipeline processing later with data enrichment. 

What’s making my brain melt:
- Fields like “occupation” and their values don’t follow specific rules across sites. So like do I build something to identify key names? Or entities? Do I use ai? Do I go word by word and find names/phrases that are occupation types? 

Less important but sometimes you have to infer based on the sites niche, the search Query, description, company name, and as a last result I’ll use a search engine to infer.

Things I’m considering
1. Doing one intelligent pass like all in one main clean up layer..

2. Building tools per field: like a tailored occupation detector, a company or person name normalizer, etc.

extra Questions
- Should I build an overall dashboard to train/evaluate/test models or just write isolated scripts? How do I know this for future things too? 
- Are there prebuilt libraries I’m missing that actually work across messy sources?
- Is ML even worth it for this, or should I stay rule-based?


I’m looking for how real people solved this or something similar. Feel free to mention if I’m on or off track with my approach, or how I could tackle this through different lens

Please help, especially if you’ve done this kind of thing for real world use.. scraped data, inferred context, tried to match entities from vague clues. Please drop tools, frameworks, or stories.

So hard to decide these days, for me anyways 
",3,10,2025-06-04 16:43:38,AbyssTricks,0.9838,0.151,0.067,0.782,0.0946975478225478,0.4487662800162801
[N] Nvidia’s Blackwell Conquers Largest LLM Training Benchmark,"New MLPerf training results are in, and Nvidia's Blackwell GPUs continue to dominate across all six benchmarks. That said, the computers built around the newest AMD GPU, MI325X, matched the performance of Nvidia’s H200, Blackwell’s predecessor, on the most popular LLM fine-tuning benchmark.   
[https://spectrum.ieee.org/mlperf-training-5](https://spectrum.ieee.org/mlperf-training-5)",62,8,2025-06-04 16:22:46,IEEESpectrum,0.3804,0.057,0.027,0.916,0.4121212121212121,0.6181818181818182
[D] Issue in result reproduction of DeepLabV3 model on Cityscapes dataset,"Hi all,  
Recently I was training a DeepLabV3 (initialised the model through the API of segmentation models pytorch library) model for semantic segmentation on Cityscapes dataset, I was not able to reproduce the scores mentioned in the DeepLab paper. The best mIOU I am able to achieve is 0.7. Would really appreciate some advice on what I can do to improve my model performance.

My training config:

1. Preprocessing - standard ImageNet preprocessing
2. Data augmentations - Random Crop of (512,1024), random scaling in the range \[0.5,2.0\] followed by resize to (512,1024), random color jitter, random horizontal flipping
3. Optimiser - SGD with momentum 0.9 and initial learning rate of 0.01.
4. Learning rate schedule - polynomial LR scheduling with decay factor of 0.9.
5. Trained DeepLabV3 for 40k iterations with batch size 8.",0,1,2025-06-04 15:26:10,Hour_Amphibian9738,0.8122,0.07,0.019,0.912,-0.05,0.3636363636363636
[D] Latest Work in Transformation-based Models?,"It seems like there was a short period of time in the '90s where transformation-based models (like those from Eric Brill) were state-of-the-art. What's happened since then?

Since they're so human-readable, I would imagine they are quite good for non-generative, classification tasks.",0,0,2025-06-04 15:12:13,ChiliPepperHott,0.723,0.121,0.0,0.879,0.3999999999999999,0.6
[D] hosting Deepseek on Prem,"I have a client who wants to bypass API calls to LLMs (throughput limits) by installing Deepseek or some Ollama hosted model.

What is the best hardware setup for hosting Deepseek locally? Is a 3090 better than a 5070 gpu? Vram makes a difference, but is there a diminishing return here? Whats the minimum viable GPU setup for on par/ better performance than cloud API?

My client is a mac user, is there a linux setup you use for hosting Deepseek locally?

What’s your experience with inference speed vs. API calls? How does local performance compare to cloud API latency?

For those that have made the switch, what surprised you?

What are the pros/cons from your experience?",24,14,2025-06-04 14:55:54,endle2020,0.8936,0.096,0.0,0.904,0.2875,0.2875
[P] Reasoning Gym: Reasoning Environments for Reinforcement Learning with Verifiable Rewards,"We recently released Reasoning Gym, which we hope can be a valuable resource for ML researchers working on reasoning models, reinforcement learning (specifically RLVR), and evaluation. The key feature is the ability to generate unlimited samples across 100+ diverse tasks, with configurable difficulty and automatically verifiable rewards.

It would be great to get some feedback from the ML community on this as we continue to work on it. Is RG useful for you? What can we do to make it easier to use? Do you have ideas for new tasks we could add generators for? Contributions are also welcome - it's all open-source!

We have already seen some adoption for RLVR, such as by NVIDIA researchers in the ProRL paper, and in Will Brown's popular *verifiers* RL library. Personally I'd be excited to see RG used for evaluation too - check out our paper for zero-shot performance of some popular LLMs and reasoning models, as well as some RLVR experiment results.

Repo: [https://github.com/open-thought/reasoning-gym/](https://github.com/open-thought/reasoning-gym/)

Paper: [https://arxiv.org/abs/2505.24760](https://arxiv.org/abs/2505.24760)

Package: [https://pypi.org/project/reasoning-gym/](https://pypi.org/project/reasoning-gym/)",11,0,2025-06-04 12:42:19,OllieStanley,0.9871,0.19,0.012,0.798,0.1201048951048951,0.7234265734265736
[R]Time Blindness: Why Video-Language Models Can't See What Humans Can?,"Found this paper pretty interesting. None of the models got anything right.

arxiv link: [https://arxiv.org/abs/2505.24867](https://arxiv.org/abs/2505.24867)

Abstract:

Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: [https://timeblindness.github.io/](https://timeblindness.github.io/) .",158,40,2025-06-04 11:58:39,dreamewaj,0.2023,0.071,0.072,0.856,0.1323529411764705,0.5815126050420169
[D] Imbalance of 1:200 with PR of 0.47 ???,Here's the results. It makes me so confused. Thank you for all your kind discussions and advice.,20,34,2025-06-04 05:57:08,rongxw,0.5757,0.205,0.091,0.703,0.0999999999999999,0.8
[D] Has there been an effective universal method for continual learning/online learning for LLMs?,"For context: (I'm a CS undergrad student trying to make a small toy project). I'm using CodeLlama for text-to-code (java) with repository context. I've tried using vector database to retrieve ""potentially relating"" code context but it's a hit or miss. In another experiment, I also tried RL (with LoRA) thinking this might encourage the LLM to generate more syntactically correct codes and avoid making mistakes (give bonus when the code passes compiler checking, penalty when LLM's response doesn't follow a specified template or fails at compilation time). The longer the training goes, the more answers obey the template than when not using RL. However, I see a decline in the code's semantical quality (e.g: same task question, in 1st, 2nd training loop, the generated code can handle edge cases, which is good; in 3rd loop, the code doesn't include such step anymore; in 4th loop, the output contain only code-comment marks).

After the experiments, it's apparent to me that I can't just arbitrary RL tuning the model. Why I wanted to use RL in the first place was that when the model makes a mistake, I would inform it of the error and ask it to recover from such mistake. So keeping a history of wrongly recovered generation in the prompt would be too much.

Has there been a universal method to do proper continual training? I appreciate all of your comments!!!",10,5,2025-06-04 03:10:53,AdOverall4214,-0.7897,0.071,0.101,0.829,0.0318181818181818,0.4412878787878787
[D] Scale ML research scientist/engineer interviews,"Has anyone here done the onsite interviews for a ML research scientist/engineer role at Scale AI?

If so, any tips/advice? Especially for the ML coding and behavioral rounds.

Thanks!",37,11,2025-06-04 02:42:11,carrotjuice999,0.5502,0.097,0.0,0.903,0.125,0.6
"[R]  Supervised classification on flow cytometry data — small sample size (50 samples, 3 classes)","Hi all,

I'm a biologist working with flow cytometry data (36 features, 50 samples across 3 disease severity groups). PCA didn’t show clear clustering — PC1 and PC2 only explain \~30% of the variance. The data feels very high-dimensional.

Now should I try supervised classification?

My questions:

1. With so few samples, should I do a train/val/test split, or just use cross-validation?
2. Any tips or workflows for supervised learning with high-dimensional, low-sample-size data?
3. any best practices or things to avoid?

Thanks in advance!

",2,3,2025-06-04 02:20:18,Previous-Duck6153,0.8674,0.108,0.022,0.87,0.1571428571428571,0.3833333333333333
[P] PyTorch Implementation for Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks,"Hey everyone,

I implemented FGVis introduced in the paper ""Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks"" by Wagner et al. (CVPR 2019) for my work. FGVis is a method to identify the pixels of an image that are relevant for a prediction.

Code: [https://github.com/spravil/FGVis](https://github.com/spravil/FGVis)

",6,1,2025-06-03 11:09:43,spravil,0.0,0.0,0.0,1.0,-0.0875,0.475
[R] Implementing Mean Flows For One-Step Generative Modelling,"Thought this would be useful to share for anyone else interested in this recent paper, on modifying flow-matching to improve one-step generative modelling (faster inference), called mean flow ( [https://arxiv.org/abs/2505.13447v1](https://arxiv.org/abs/2505.13447v1) ).

It's a simple idea and the shown 1-step results are good, but I saw criticism that this idea requires too much effort in training.

I decided to try coding it up myself, and test on simple 2D distributions. I ended up making a small tutorial on my implementation and results in this google colab: [https://colab.research.google.com/drive/18HeOrhQ\_5u-TvHhfxHr8\_t\_03pX-tHO-](https://colab.research.google.com/drive/18HeOrhQ_5u-TvHhfxHr8_t_03pX-tHO-)

My results were:

\- Great results for 1 step generation compared to flow matching (haha)

\- It takes a lot more epochs to train, has difficulty learning harder problems

\- Multi-step generation results are inferior in quality to flow matching

\- Something I couldn't really quantify but the modified loss with gradients seems... unstable? hard to train?",27,5,2025-06-03 22:51:01,modelling_is_fun,-0.8504,0.09,0.134,0.776,0.0666666666666666,0.4312324929971989
[R] SocialSim’25: Social Simulations with LLMs — Call for Papers + Shared Task,"We’re organizing **SocialSim’25: Social Simulations with LLMs**, a workshop at **COLM 2025 in Montreal (Oct 10)**. This workshop explores how large language models can simulate social behavior online—from user actions to moderation dynamics and social interventions.

We’re looking for contributions on:

* Agent-based LLM simulations
* Behavioral prediction and persona modeling
* Evaluation of online harms and mitigation strategies

📝 **Call for Papers deadline**: June 23, 2025 (AoE)

We also launched a **Kaggle competition** as part of the shared task—predict next actions from social media traces. Great for testing persona-driven models!

Edit: Links are in the comment!",9,1,2025-06-03 20:00:52,RSTZZZ,0.8353,0.117,0.028,0.855,0.1726190476190476,0.1889880952380952
[D] Poor classification performance but good retrieval performance,"I am currently training a neural network on a classification task (more specifically I use a kind of margin loss called Arcface).

When I evaluate in classification mode, then I have something like 30-40% accuracy but if I evaluate using my training set as a database and running a knn on embeddings (so i get to tests samples labels corresponding to closed neighbours in training set) then I get 70-80% accuracy !

I think I need some insights about this behavior.",7,6,2025-06-03 19:59:11,LelouchZer12,0.5255,0.09,0.061,0.85,0.2125,0.5166666666666667
[D] CPU time correlates with embedding entropy - related to recent thermodynamic AI work?,"CPU time correlates with embedding entropy - related to recent thermodynamic AI work?

Hey r/MachineLearning,

I've been optimizing embedding pipelines and found something that might connect to recent papers on ""thermodynamic AI"" approaches.

**What I'm seeing:**
- Strong correlation between CPU processing time and Shannon entropy of embedding coordinates  
- Different content types cluster into distinct ""phases"" 
- Effect persists across multiple sentence-transformer models
- Stronger when normalization is disabled (preserves embedding magnitude)

**Related work I found:**
- Recent theoretical work on thermodynamic frameworks for LLMs 
- Papers using semantic entropy for hallucination detection (different entropy calculation though)
- Some work on embedding norms correlating with information content

**My questions:**
1. Has anyone else measured direct CPU-entropy correlations in embeddings?
2. Are there established frameworks connecting embedding geometry to computational cost?
3. The ""phase-like"" clustering - is this a known phenomenon or worth investigating?

I'm seeing patterns that suggest information might have measurable ""thermodynamic-like"" properties, but I'm not sure if this is novel or just rediscovering known relationships.


Any pointers to relevant literature would be appreciated!",0,16,2025-06-03 19:01:37,notreallymetho,0.912,0.085,0.013,0.902,0.0701754385964912,0.3801169590643274
"[R] GuidedQuant: Boost layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit quantization) (ICML 2025)","**Paper (ICML 2025):** [https://arxiv.org/abs/2505.07004](https://arxiv.org/abs/2505.07004)

**Code:** [https://github.com/snu-mllab/GuidedQuant](https://github.com/snu-mllab/GuidedQuant)

**HuggingFace Collection:** 2\~4-bit quantized Qwen3-32B, gemma-3-27b-it, Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct  → [Link](https://huggingface.co/collections/jusjinuk/instruction-tuned-models-guidedquant-68334269c44cd3eb21f7bd61)

**TL;DR:** **GuidedQuant** boosts layer-wise PTQ methods by integrating end loss guidance into the objective. We also introduce **LNQ**, a non-uniform scalar quantization algorithm which is guaranteed to monotonically decrease the quantization objective value.

**Demo:**

[Qualitative example output of 2-bit quantized Llama-3.3-70B-Instruct model, running on a single RTX 3090 GPU.](https://preview.redd.it/65tegdfv6r4f1.png?width=4377&format=png&auto=webp&s=6ce7846ac64f37028023c039606501ef1967f2ae)

**Summary:**

GuidedQuant objective weights layer-wise output errors with **per-feature gradients** with respect to the end loss. This corresponds to block-diagonal Fisher information which preserves intra-channel dependencies. Thus, GuidedQuant shows advantage over layer-wise PTQ methods (e.g., GPTQ) and diagonal Fisher methods (e.g., SqueezeLLM)

https://preview.redd.it/cbjtos9g9r4f1.jpg?width=640&format=pjpg&auto=webp&s=dc7079d4c3219a0a304ea40394f7fa88d5f5dada

**GuidedQuant** objective can be plugged into any layer-wise PTQ backend, improving state-of-the-art methods across weight-only scalar, weight-only vector, and weight-and-activation quantization.

https://preview.redd.it/gobxvr2s9r4f1.jpg?width=640&format=pjpg&auto=webp&s=582cd05e87b6c1fdcc9ea2782b454defe755c197

We further introduce **LNQ**: an non-uniform quantization method that alternates a **closed-form codebook update** and a **coordinate-descent assignment update**, giving a provable descent property

**Blog post:** [https://jusjinuk.me/blog/guidedquant/](https://jusjinuk.me/blog/guidedquant/)

As long-time fans of the community, we hope you find our work interesting and look forward to your feedback!

Thank you!",12,0,2025-06-03 18:17:00,jusjinuk,0.9487,0.127,0.044,0.829,0.1014030612244898,0.2306122448979592
[D]: Tensorboard alternatives,"Hello everyone, I realize this might be outdated topic for a post, but TensorBoard very convenient for my typical use case:



I frequently rent cloud GPUs for daily work and sometimes I switch to a different few hours. As a result, I need to set up my environment as efficiently as possible.



With tb I could simply execute '%load\_ext tensorboard' followed by '%tensorboard --logdir dir --port port' and then:



from torch.utils.tensorboard Summary

writer = SummaryWriter()

writer.add\_\*...



I found this minimal setup significantly less bloated than in other frameworks. Additionally, with this method it straightforward to set up local server

Also for some reason, so many alternatives requires the stupid login at the beginning..

Are there any modern alternatives I should consider? Ideally, I am looking for a lightweight package with easy local instance setup",22,31,2025-06-03 16:41:52,Potential_Hippo1724,0.7579,0.082,0.034,0.884,0.0375,0.4420238095238095
[D] what is the cheapest double descent experiment?,"As title says, what is the cheapest double descent experiment that can be done?",49,18,2025-06-03 15:05:39,Designer-Air8060,0.0,0.0,0.0,1.0,0.0,0.0
[D] What are your experiences with the European ELLIS program and would you recommend it?,"Hi everyone,

I am a Master student in math in Germany interested in the theory and math foundationals of learning theory and neural networks. Recently I leraned that there is a program called ELLIS (European Laboratory for Learning and Intelligent Systems) in Europe, which is not mentioned a lot here.

I am interested in applying to some schools in this program, so I was wondering if you could share your thoughts and experience with this program -- such as the admission difficulty, how do you like your ""grad school experience"", and so on?

Many thanks!",27,8,2025-06-03 09:13:54,hedgehog0,0.9434,0.166,0.021,0.813,0.2277777777777777,0.3722222222222222
Best way to figure out drawbacks of the methodology from a certain paper [D],"In today's competitive atmosphere, authors usualy tout SOTA results, in whatever narrow sub-sub-domain. Older generations were more honest about ""drawbacks"", ""limitations"", and ""directions for future research"". Many (not all) modern papers either skip these sections or treat them like a marketing brochure. 

An unrelated 3rd person (like me) needs a balanced view of what's good/bad about some methodology. Someone with a very high IQ and vast exposure/experience will probably find it easier to critique a paper after 1-2 reads. But that's not most people. Certainly not me. 

Is there an easier way for mere mortals to get a more balanced perspective on where to place the significance of a piece of research? 

In many cases, I have found that subsequent publications, who cite these papers, mention about their drawbacks. I suppose, one way would be to collect all future papers that cite paper X and use AI to search all the negative or neutral things they have to say about paper X. This pipeline could probably be put together without too much difficulty. 

Is there a more Luddite approach?  ",34,13,2025-06-03 06:57:58,datashri,0.8555,0.112,0.046,0.842,0.1696277056277056,0.4308268398268398
[R] Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space,"Abstract

>Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, process ing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like “soft” reasoning by generating soft, abstract concept tokens in a contin uous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple mean ings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning.

If you’re into reasoning models, continuous representations, or just want to see at where AI reasoning might go beyond token-limited models, I think you’ll enjoy this paper. Might be worth looking into!

Paper link: [\[2505.15778\] Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)",45,4,2025-06-03 01:37:20,hiskuu,0.9476,0.075,0.012,0.913,0.0751535087719298,0.4296710526315788
[D] Creating/constructing a basis set from a embedding space?,"Say I have a small library of item (10k) and I have a 100-dimensional embeddings for each item. **I want to pick a sub-set of the items that best ""represents"" the dataset.** Thinking this set might be small, 10-100 in size. 

* ""Best"" can mean many things, explained variance, diversity.
* PCA would not work since it's a linear combination of items in the set.
* What are some ways to build/select a ""basis set"" for this embeddings space?
* What are some ways of doing this?
* If we have two ""basis sets"", A and B, what some metrics I could use to compare them?

  
**Edit:** Updated text for clarity.",9,32,2025-06-02 19:45:25,LetsTacoooo,0.8466,0.083,0.0,0.917,0.28125,0.4312500000000001
"[D] Requesting Feedback: PCA Chapter, From My Upcoming ML Book (Full PDF Included)","Hey all,

I have finished writing a chapter on Principal Component Analysis (PCA) for a **machine learning book** I’m working on. The chapter explains PCA in depth with step-by-step math, practical code, and some real-world examples. My main goal is to make things as clear and practical as possible.

If anyone has a few minutes, **I’d really appreciate any feedback**; especially about clarity, flow, or anything that’s confusing or could use improvement. The PDF is about 36 pages, but **you absolutely don’t need to read every page**. Just skim through, focus on any section that grabs your attention, and share whatever feedback or gut reactions you have.

**Direct download (no sign-in required):**  
👉 [PDF link to Drive](https://drive.google.com/file/d/19_acGd0oD0A5_EMnt-5xEEUNll8UH-kB/view?usp=sharing)

Thanks in advance for any comments or thoughts, small or big!

H.",0,5,2025-06-02 19:14:26,Responsible_Cow2236,0.9058,0.101,0.01,0.889,0.0435897435897435,0.458974358974359
"[D] Looking for some ideas on what to do with, effectively, a time-series of correlation coefficients","Hi all

I have a data set, which is basically wine scores from various critics by vintage since 2019.

Within each vintage, its obviously trivial to produce a correlation of each critic to each other critic. But what I have, now, is effectively \~6 correlation matricies, one representing each year (e.g. 2019, 2020, 2021, etc)

I'd love to try to extract some patterns out of othis... Does anyone have any idea on what I could do?

I was thinking of trying to find something like, ""most consistent"" correlation between critic pairs, but I was wondering if there was something more complicated like a matrix factorisation approach to try to group critics who like one type of wine over other type of wines (e.g. overextracted wines vs not)

I'd love some ideas, this is a hobby project rather than anything professional/commercial.

The raw data set themselves, you can imagine as basically:

Wine/Critic {A, B, C}

Wine A, 95, 93, 91

Wine B, 99, 98, 99

And then that data set is replicated across 6 vintages (note some critics ""shift"", as do wines)

Thank you all",3,11,2025-06-02 18:47:24,reddithenry,0.9698,0.139,0.064,0.797,0.1322485207100592,0.5585798816568046
"[D] TMLR paper quality seems better than CVPR, ICLR.","I found that quality and correctness-wise TMLR papers seem to be be better than CVPR and ICLR papers on an average with the latter having huge variance in the paper quality. Do people think so as well? If so, why?",174,20,2025-06-02 15:22:13,tibetbefree,0.8701,0.198,0.0,0.802,0.25,0.4599999999999999
[D] Is overfitting still relevant in the era double descent?,"According to double descent, it should be the case that increasing the capacity will result in a lower testing error. Does this mean we should use the most complex/high capacity model class for every problem/task?

**Update**

What really bothers is the following:

[Image origin: https:\/\/en.wikipedia.org\/wiki\/Double\_descent#\/media\/File:Double\_descent\_in\_a\_two-layer\_neural\_network\_\(Figure\_3a\_from\_Rocks\_et\_al.\_2022\).png](https://preview.redd.it/w4ic7f59dj4f1.png?width=1920&format=png&auto=webp&s=447e9194b9ffd3ddf444778768801de2c7b0ca59)

Lets assume we are training a transformer with 10 billion parameters for text classification with only 1 example. Strictly speaking by the black curve, we should get the best performance, or at least, better than training with a 100B dataset. Can someone explain why this is possible/impossible?",77,35,2025-06-02 11:51:10,Seiko-Senpai,0.4708,0.075,0.065,0.861,0.1517361111111111,0.4184027777777778
[P] Building a Face Swap Tool Using GANs – What Libraries or Models Should I Explore?,"Hi everyone,

I'm working on a project where I want to build a face-swapping program. The idea is to take an input image, detect and extract the face (for example using OpenCV), and then replace it with a completely different, synthetic face that still fits naturally into the original photo — ideally, in a way that makes it hard to tell the image was modified.

I've previously experimented with generating faces using NVIDIA's StyleGAN3 (specifically, the pretrained `stylegan3-t-ffhq-1024x1024` model), but from what I remember, there wasn’t an easy way to control attributes like age, gender, or skin tone — unless I missed something. If anyone knows how to steer StyleGAN3 in this way, I'd love to hear about it.

What I’m aiming for is:

* A system that takes an image and swaps the face with a realistic-looking, completely new synthetic face.
* The new face should not resemble the original one at all, but still match the context (lighting, angle, etc.).
* I'd like to have some control over attributes like age, gender, and ethnicity for the generated faces.

Does anyone here have experience with this type of project? Could you suggest any libraries, tools, or models I should look into? Any advice on how to approach the face blending step (to make the new face look seamless in the original image) would also be much appreciated.

Thanks in advance!",2,1,2025-06-01 16:36:20,TopCap7846,0.9883,0.152,0.015,0.833,0.2224431818181818,0.5097064393939392
[R] System Prompt Learning: A Third Paradigm for LLM Learning Beyond Pretraining and Fine-tuning,"**TL;DR**: We implemented a system that enables LLMs to learn explicit problem-solving strategies from experience, achieving significant improvements on mathematical reasoning benchmarks while maintaining full interpretability of learned knowledge.

# Background & Motivation

Current LLMs learn through two primary paradigms: (1) pretraining on massive corpora and (2) fine-tuning via supervised/reinforcement learning. However, there's a notable gap between production systems (which use sophisticated, hand-crafted system prompts) and research/development settings (which typically use minimal prompting).

This work explores Andrej Karpathy's proposed ""third paradigm"": **System Prompt Learning** \- enabling models to learn and maintain explicit problem-solving strategies through experience.

# Methodology

**System Prompt Learning (SPL)** operates through several key components:

1. **Problem Classification**: Automatic categorization of queries into 16 problem types using the LLM itself
2. **Strategy Generation**: LLM-powered creation of step-by-step problem-solving strategies for new problem types
3. **Strategy Database**: Persistent storage with performance tracking (success rate, usage frequency, etc.)
4. **Strategy Selection**: Similarity-based retrieval of top-k strategies for inference (k≤3)
5. **Performance Evaluation**: Post-completion assessment of strategy effectiveness
6. **Strategy Refinement**: Periodic improvement based on accumulated experience

**Key Design Decisions**:

* Dual limits: storage limit (max 10 strategies per type) and inference limit (max 3 strategies per query)
* Minimum performance threshold (40% success rate, ≥5 attempts) for strategy deployment
* Human-readable strategy representation for interpretability
* Maintenance operations (merging similar strategies, pruning poor performers)

# Experimental Setup

**Model**: gemini-2.0-flash-lite  
**Training**: 400 instances from OptILLMBench training split  
**Evaluation**: Separate test sets across multiple benchmarks  
**Metrics**: Accuracy on mathematical reasoning tasks

# Results

|Benchmark|Baseline|SPL|Improvement|
|:-|:-|:-|:-|
|OptILLMBench|61.0%|65.0%|\+4.0%|
|MATH-500|85.0%|85.6%|\+0.6%|
|Arena Hard|29.0%|37.6%|**+8.6%**|
|AIME24|23.33%|30.0%|**+6.67%**|

**Learning Dynamics** (after 500 queries):

* 129 strategies created across problem types
* 97 strategies refined through experience
* 28 strategies merged (similarity-based consolidation)
* 346 successful problem resolutions

Notably, improvements are most pronounced on challenging benchmarks (Arena Hard, AIME24) where strategic reasoning provides the greatest advantage.

# Technical Contributions

1. **Novel Learning Paradigm**: First implementation of experience-driven strategy learning for LLMs
2. **Interpretable Knowledge Representation**: All learned strategies are human-readable and editable
3. **Adaptive Strategy Management**: Dynamic creation, selection, and refinement based on performance
4. **Zero-Shot Generalization**: Strategies learned on one problem generalize to similar problems

# Example Learned Strategy

For word problems, the system converged on:

    1. Understand: Read carefully, identify unknowns, list given information
    2. Plan: Define variables with units, identify relationships, write equations  
    3. Solve: Step-by-step calculation with unit tracking
    4. Verify: Check reasonableness, state final answer with units

This strategy achieved 44.3% success rate across 192 applications.

# Broader Implications

**For ML Research**:

* Demonstrates feasibility of transparent, incremental learning in LLMs
* Bridges the gap between implicit knowledge (weights) and explicit knowledge (strategies)
* Provides a framework for cumulative learning without parameter updates

**For AI Safety**:

* Full interpretability of learned knowledge
* Human oversight and editing capabilities
* Transparent decision-making process

**Limitations**:

* Currently limited to text-based reasoning tasks
* Strategy quality depends on underlying model capabilities
* Manual problem type taxonomy (though extensible)

# Implementation

Open-source implementation available as a plugin in optillm. Key features:

* Model-agnostic (works with any OpenAI-compatible API)
* Persistent strategy storage with versioning
* Configurable learning/inference modes
* Integration with existing inference optimization techniques

**Code**: [https://github.com/codelion/optillm/tree/main/optillm/plugins/spl](https://github.com/codelion/optillm/tree/main/optillm/plugins/spl)

# Future Directions

1. **Multimodal Extension**: Incorporating visual/audio problem-solving strategies
2. **Meta-Learning**: Learning to learn strategies more efficiently
3. **Collaborative Learning**: Sharing strategies across model instances
4. **Domain Specialization**: Developing expertise in specific fields through targeted exposure

This work represents an early step toward LLMs that genuinely improve through use while maintaining full transparency in their learning process.

**Paper/Technical Report**: [https://huggingface.co/blog/codelion/system-prompt-learning](https://huggingface.co/blog/codelion/system-prompt-learning)  
**Original Inspiration**: [https://x.com/karpathy/status/1921368644069765486](https://x.com/karpathy/status/1921368644069765486)

Thoughts on extending this approach? Interested in the implications for continual learning research?",5,1,2025-06-02 07:13:46,asankhs,0.9836,0.096,0.044,0.86,0.1010679572000326,0.5059258351711182
[D] How to train a model for Speech Emotion Recognition without a transformer?,"(I'm sorry if this is the wrong tag for the post, or if the post is not supposed to be here, I just need some help with this)

Hey guys, I'm building a speech analyzer and I'd like to extract the emotion from the speech for that. But the thing is, I'll be deploying it online so I'll have very limited resources when the model will be in inference mode so I can't use a Transformer like wav2vec for this, as the inference time will be through the roof with transformers so I need to use Classical ML or Deep Learning models for this only.

So far, I've been using the CREMA-D dataset and have extracted audio features using Librosa (first extracted ZCR, Pitch, Energy, Chroma and MFCC, then added Deltas and Spectrogram), along with a custom scaler for all the different features, and then fed those into multiple classifiers (SVM, 1D CNN, XGB) but it seems that the accuracy is around 50% for all of them (and it decreased when I added more features). I also tried feeding in raw audio to an LSTM to get the emotion but that didn't work as well.

Can someone please please suggest what I should do for this, or give some resources as to where I can learn to do this from? It would be really really helpful as this is my first time working with audio with ML and I'm very confused as to what to here.

(P.S.: Mods I agree this is noob's question, but I've tried my best to make it non-low-effort)",2,6,2025-06-02 06:31:55,Defiant_Strike823,0.9639,0.105,0.036,0.859,0.0033160956690368,0.5190540831717303
[D] MCP Client with Local Ollama LLM + Multi-Server Tools,"Built a minimal MCP client that runs with a local Ollama LLM. You can hook up multiple MCP servers via a simple config.json. The client merges all tools into one interface and routes calls automatically. No LLM API keys.

Repo: https://github.com/Nagharjun17/MCP-Ollama-Client

Would love thoughts from anyone working on local agents or tool-use pipelines.",5,0,2025-06-02 03:51:08,Wise-Grand-8374,0.4588,0.066,0.035,0.899,0.0571428571428571,0.2224489795918367
"[P] Evolving Modular Priors to Actually Solve ARC and Generalize, Not Just Memorize","I've been looking into ARC (Abstraction and Reasoning Corpus) and what’s actually needed for general intelligence or even real abstraction, and I keep coming back to this:

Most current AI approaches (LLMs, neural networks, transformers, etc) fail when it comes to abstraction and actual generalization, ARC is basically the proof.

So I started thinking, if humans can generalize and abstract because we have these evolved priors (symmetry detection, object permanence, grouping, causality bias, etc), why don’t we try to evolve something similar in AI instead of hand-designing architectures or relying on NNs to “discover” them magically?

The Approach

What I’m proposing is using evolutionary algorithms (EAs) not to optimize weights, but to actually evolve a set of modular, recombinable priors, the kind of low-level cognitive tools that humans naturally have.
The idea is that you start with a set of basic building blocks (maybe something equivalent to “move,” in Turing Machine terms), and then you let evolution figure out which combinations of these priors are most effective for solving a wide set of ARC problems, ideally generalizing to new ones.

If this works, you’d end up with a “toolkit” of modules that can be recombined to handle new, unseen problems (including maybe stuff like Raven’s Matrices, not just ARC).

Why Evolve Instead of Train?

Current deep learning is just “find the weights that work for this data.”
But evolving priors is more like: “find the reusable strategies that encode the structure of the environment.”
Evolution is what gave us our priors in the first place as organisms, we’re just shortcutting the timescale.

Minimal Version

Instead of trying to solve all of ARC, you could just:

Pick a small subset of ARC tasks (say, 5-10 that share some abstraction, like symmetry or color mapping)

Start with a minimal set of hardcoded priors/modules (e.g., symmetry, repetition, transformation)

Use an EA to evolve how these modules combine, and see if you can generalize to similar held-out tasks


If that works even a little, you know you’re onto something.

Longer-term

Theoretically, if you can get this to work in ARC or grid puzzles, you could apply the same principles to other domains, like trading/financial markets, where “generalization” matters even more because the world is non-stationary and always changing.

Why This? Why Now?

There’s a whole tradition of seeing intelligence as basically “whatever system best encodes/interprets its environment.” I got interested in this because current AI doesn’t really encode, it just memorizes and interpolates.

Relevant books/papers I found useful for this line of thinking:

Building Machines That Learn and Think Like People (Lake et al.)

On the Measure of Intelligence (Chollet, the ARC guy)

NEAT/HyperNEAT (Stanley) for evolving neural architectures and modularity

Stuff on the Bayesian Brain, Embodied Mind, and the free energy principle (Friston) if you want the theoretical/biological angle


Has anyone tried this?

Most evolutionary computation stuff is either evolving weights or evolving full black-box networks, not evolving explicit, modular priors that can be recombined.
If there’s something I missed or someone has tried this (and failed/succeeded), please point me to it.

If anyone’s interested in this or wants to collaborate/share resources, let me know. I’m currently unemployed so I actually have time to mess around and document this if there’s enough interest.

If you’ve done anything like this or have ideas for simple experiments, drop a comment.

Cheers.
",3,3,2025-06-02 03:35:37,Physine,0.9965,0.138,0.037,0.825,0.1565730837789661,0.4068542568542569
"[P] Open Source Photo Quality Analyzer: Get Technical Scores for Your Images (Python, YOLO, OpenCV CLI)","Hey everyone,

I've built a Python CLI script, the **Photo Quality Analyzer**, to give your photos quick, objective technical scores. It uses CV (YOLO) to intelligently check focus on main subjects, plus overall sharpness, exposure, and more.

You get detailed scores, a plain English summary of *why*, and it can even auto-sort your images into quality-based folders

**GitHub Repo:** [`https://github.com/prasadabhishek/photo-quality-analyzer`](https://github.com/prasadabhishek/photo-quality-analyzer)

It's open source and definitely a work in progress. I'd love your feedback on its usefulness, any bugs you spot, or ideas for improvement. Contributions are welcome too!

Let me know if you give it a spin.",5,0,2025-06-02 00:29:45,Correct_Pin118,0.972,0.192,0.0,0.808,0.2178571428571428,0.4150297619047619
Looking for more image enhancement methods [R],"My knowledge of deep learning is mostly confined to denoising images. So basically applying transformers and cnn to that task, some of my favorite papers are Attention is all you need, swin transformer, swinIR, high resolution single-photon imaging with physics informed deep learning and GM-MOE: Low-Light Enhancement with gated mechanism mixture of experts. I’d love to be recommended some technical papers to learn new techniques for this sort of thing.",2,2,2025-06-01 20:25:02,IEgoLift-_-,0.8402,0.108,0.0,0.892,0.2551515151515151,0.4993939393939393
[D] Advice on processing ~1M jobs/month with LLaMA for cost savings,"I'm using GPT-4o-mini to process \~1 million jobs/month. It's doing things like deduplication, classification, title normalization, and enrichment. Right now, our GPT-4o-mini usage is costing me thousands/month (I'm paying for it out of pocket, no investors).

This setup is fast and easy, but the cost is starting to hurt. I'm considering distilling this pipeline into an open-source LLM, like LLaMA 3 or Mistral, to reduce inference costs, most likely self-hosted on GPU on Google Coud.

Questions:

\* Has anyone done a similar migration? What were your real-world cost savings (e.g., from GPT-4o to self-hosted LLaMA/Mistral)

\* Any recommended distillation workflows? I'd be fine using GPT-4o to fine-tune an open model on our own tasks.

\* Are there best practices for reducing inference costs even further (e.g., batching, quantization, routing tasks through smaller models first)?

\* Is anyone running LLM inference on consumer GPUs for light-to-medium workloads successfully?

Would love to hear what’s worked for others!

",1,11,2025-06-01 19:59:24,hamed_n,0.9729,0.151,0.033,0.816,0.3162946428571428,0.5720238095238095
[P] Interactive Pytorch visualization package that works in notebooks with 1 line of code,"I have been working on an open source package ""[torchvista](https://github.com/sachinhosmani/torchvista)"" that helps you visualize the forward pass of your Pytorch model as an interactive graph in web-based notebooks like Jupyter, Colab and Kaggle.

Some of the key features I wanted to add that were missing in the other tools I researched were

1. interactive visualization: including modular exploration of nested modules (by collapsing and expanding modules to hide/reveal details), dragging and zooming
2. providing a clear view of the shapes of various tensors that flow through the graph
3. error tolerance: produce a partial graph even if there are failures like tensor shape mismatches, thereby making it easier to debug problems while you build models
4. notebook support: ability to run within web-based notebooks like Jupyter and Colab

Here is the [Github repo](https://github.com/sachinhosmani/torchvista) with simple instructions to use it. And here is a walkthrough [Google Colab](https://colab.research.google.com/drive/1wrWKhpvGiqHhE0Lb1HnFGeOcS4uBqGXw?usp=sharing#scrollTo=tUKHO2YFKi55) notebook to see it in action (you need to be signed in to Google to see the outputs).

And here are some interactive demos I made that you can view in the browser:

* [MobileVitCollapsed](https://sachinhosmani.github.io/torchvista/MobileVitCollapsed.html)
* [LinearModel](https://sachinhosmani.github.io/torchvista/LinearModel.html)
* [Full list of demos](https://sachinhosmani.github.io/torchvista/)

I’d love to hear your feedback!

Thank you!",286,27,2025-06-01 19:41:50,Dev-Table,0.9523,0.141,0.056,0.803,0.0823863636363636,0.4286796536796536
[D] How are single-author papers in top-tier venues viewed by faculty search committees and industry hiring managers?,"For those with experience on faculty search committees *or* in hiring for research roles in industry (e.g., at AI labs, big tech, or startups): how seriously are *single-author* papers by PhD candidates taken when evaluating candidates?

Suppose a candidate has a single-authored paper published at a top-tier venue (e.g., NeurIPS, ICML, ICLR, EMNLP, etc.), and the work is technically sound and original. How is that interpreted?

* In academia, does it signal independence and research leadership?
* In industry, does it carry weight in showing initiative and technical depth, or is collaborative work more highly valued?

I’m also curious how this compares to co-authored papers with senior figures or large lab collaborations. Do single-author works help a candidate stand out, or are they undervalued relative to high-impact team efforts?

Would love to hear from folks who have hired for research positions—academic or industrial—and how you've weighed these kinds of contributions.

  
thanks!",61,110,2025-06-01 17:07:31,South-Conference-395,0.9547,0.109,0.01,0.882,0.163829365079365,0.4404365079365079
"[R] Equivariance is dead, long live equivariance?","A new blogpost on Geometric Deep Learning for molecular structure modelling.

When should you bake symmetries into your architecture versus just scaling up — an attempt at a nuanced take on a hotly debated topic.",0,2,2025-06-01 15:17:45,chaitjo,-0.6486,0.0,0.104,0.896,0.0454545454545454,0.5007575757575758
[P] Steam Recommender,"Hello ML Enjoyers!

I have recently created a steam game finder that helps users find games similar to their own favorite game,

I pulled reviews form multiple sources then used sentiment with some regex to help me find insightful ones then with some procedural tag generation along with a hierarchical genre umbrella tree i created game vectors in category trees, to traverse my db I use vector similarity and walk up my hierarchical tree.

my goal is to create a tool to help me and hopefully many others find games not by relevancy but purely by similarity. Ideally as I work on it finding hidden gems will be easy.

I created this project to prepare for my software engineering final in undergrad so its **very rough**, this is not a finished product at all by any means. **Let me know** if there are any features you would like to see or suggest some algorithms to incorporate. 

check it out on : [https://nextsteamgame.com/](https://nextsteamgame.com/)",45,13,2025-06-01 14:53:13,Expensive-Ad8916,0.975,0.175,0.0,0.825,0.0563095238095238,0.5960416666666666
Need recommendations for cheap on-demand single vector embedding [D],"I'll have a couple 1000 monthly searches where users will send me an image and I'll need to create an embedding, perform a search with the vector and return results.

I am looking for advice about how to set up this embedding calculation (batch=1) for every search so that the user can get results in a decent time?

GPU memory required: probably 8-10GB.

Is there any ""serverless"" service that I can use for this? Seems very expensive to rent a server with GPU for a full month. If first, what services do you recommend?",5,13,2025-06-01 11:52:51,MooshyTendies,0.6298,0.052,0.0,0.948,0.0742063492063492,0.5623809523809523
[D] How do you see funding into the field changing over the next decade?,"Over the past decade, we have seen enormous investment into ML from both academia and industry. Much of it seems to be driven by optimistic projections of what ML systems (especially GenAI) might be able to do in the future.

However, I am wondering if this momentum is sustainable. If progress flattens or ROI doesn't turn out to be quite as high as predicted, could we see a sharp decline in funding? Additionally, a lot of people are trying to pivot or break into ML research which might further intensify competition.

How do you see this affecting the academic and industrial job markets, availability of academic funding for research, or the field in general?

I am considering a PhD in ML so I'd appreciate perspectives on the medium-term outlook from both academics and professionals. Thanks!",22,18,2025-06-01 11:25:09,PanemPlayz,0.8959,0.078,0.0,0.922,0.0560714285714285,0.3992857142857143
[P] OSS Release: LLM Gateway — open-source multi-provider LLM router (self-host or 5 % flat fee hosted) Openrouter alternative,,1,0,2025-06-01 11:17:23,smakosh,0.0,0.0,0.0,1.0,-0.025,0.125
[D] What should be the methodology for forecasting,"We are doing a project on sales forecasting using machine learning , We have a dataset of a retail store from 2017 to 2019 , which has 14200 datapoints .

We want to use machine learning to built a accurate prediction model

I want to know what should be my methodology , which algorithms to use ? I have to show in a flow chart",10,5,2025-06-01 11:04:51,Imaginary-Spring-779,0.1531,0.042,0.0,0.958,0.4000000000000001,0.6333333333333334
To all the researchers here! How you approach to AI/ML research of the future?[D],"I have a interview coming up for AI research internship role. In the mail, they specifically mentioned that they will discuss my projects and my approach to AI/ML research of the future. So, I am trying to get different answers for the question ""my approach to AI/ML research of the future"". This is my first ever interview and so I want to clear it. So, how will you guys approach this question?

Also any tips for interview will be helpful. Thanks in advance!!

EDIT: my views on this question or how I will answer this question is: I personally think that the LLM reasoning will be the main focus of the future AI research. because in the all latest llms as far as I know, core attention mechanism remains same and the performance  was improved in post training. plus the new architectures focusing on faster inference while maintaining performance will also play more important role. such as LLaDA(recently released). but I think companies will utilizes these architecture. but we will see more such architectures. and more research in mechanistic interpretability will be done. because if we will be able to understand llm comes to a specific output or specific token then its like understanding our brain. and we will be able to truly achieve reasoning. and yah there will be a surge of ai researcher(AI).  
  
there are other things such as small llms etc. which i think not in research but in the development will be very useful.  
  
of-course there are other development in research which i am not aware about and have limited knowledge. but as per my current knowledge, reasoning and interpretability will be future in my personal opinion. ",0,7,2025-06-01 09:46:41,anonymous_anki,0.9624,0.084,0.008,0.908,0.126033385806113,0.3900728059818969
[D] How to use LLMs for Data Analysis?,"Hi all,
I’ve been experimenting with using LLMs to assist with business data analysis, both via OpenAI’s ChatGPT interface and through API integrations with our own RAG-based product. I’d like to share our experience and ask for guidance on how to approach these use cases properly.

We know that LLMs can’t understand numbers or math operation, so we ran a structured test using a CSV dataset with customer revenue data over the years 2022–2024. On the ChatGPT web interface, the results were surprisingly good: it was able to read the CSV, write Python code behind the scenes, and generate answers to both simple and moderately complex analytical questions. A small issue occurred when it counted the number of companies with revenue above 100k (it returned 74 instead of 73 because it included the header) but overall, it handled things pretty well.

The problem is that when we try to replicate this via API (e.g. using GPT-4o with Assistants APIs and code-interpreter enabled), the experience is completely different. The code interpreter is clunky and unreliable: the model sometimes writes partial code, fails to run it properly, or simply returns nothing useful. When using our own RAG-based system (which integrates GPT-4 with context injection), the experience is worse: since the model doesn’t execute code, it fails all tasks that require computation or even basic filtering beyond a few rows.

We tested a range of questions, increasing in complexity:

1) Basic data lookup (e.g., revenue of company X in 2022): OK
2) Filtering (e.g., all clients with revenue > 75k in 2023): incomplete results, model stops at 8-12 rows
3) Comparative analysis (growth, revenue changes over time): inconsistent
4) Grouping/classification (revenue buckets, stability over years): fails or hallucinates
5) Forecasting or “what-if” scenarios: almost never works via API
6) Strategic questions (e.g. which clients to target for upselling): too vague, often speculative or generic

In the ChatGPT UI, these advanced use cases work because it generates and runs Python code in a sandbox. But that capability isn’t exposed in a robust way via API (at least not yet), and certainly not in a way that you can fully control or trust in a production environment.

So here are my questions to this community:
1) What’s the best way today to enable controlled data analysis via LLM APIs? And what is the best LLM to do this?
2) Is there a practical way to run the equivalent of the ChatGPT Code Interpreter behind an API call and reliably get structured results?
3) Are there open-source agent frameworks that can replicate this kind of loop: understand question > write and execute code > return verified output?
4) Have you found a combination of tools (e.g., LangChain, OpenInterpreter, GPT-4, local LLMs + sandbox) that works well for business-grade data analysis?
5) How do you manage the trade-off between giving autonomy to the model and ensuring you don’t get hallucinated or misleading results?

We’re building a platform for business users, so trust and reproducibility are key. Happy to share more details if it helps others trying to solve similar problems.

Thanks in advance.
",0,11,2025-06-01 08:09:46,Obliviux,0.9895,0.127,0.063,0.81,0.0895604395604395,0.4425824175824175
"[D] Researchers and engineers in academia as well as industry, which books did you find the most useful in creating your knowledge base and skill set?","Please mention the niche you work in and in what capacity. If at all possible you can share link to your works.

Now, coming to the question. Assuming that you actively work in machine learning related fields, which books gave you the greatest benefit till now? It can be books from foundational math topics or engineering skills topics also.

I am a second year grad student (topic not yet finalised, mostly something in computer vision).

I am reading **Probability Theory by E.T. Jaynes** and for programming **Structure and Interpretation of Computer Programs by Abelson and Sussman.** Both are blowing my mind in a tremendously good way.

Edit: Thanks everyone for your lovely comments and fav suggestions. Although I expected more math books, but, everyone seem to mention their fav ML book only. ",99,29,2025-06-01 07:19:05,,0.9669,0.165,0.0,0.835,0.2111111111111111,0.5633333333333332
Machine learning copy system [P],"Hi, I'm a tutor for some programming courses, and as a hobby, I'm developing a Python program to detect copying among students. I want to do it using machine learning, something similar to JPlag. I'd like to know if you have any recommendations for a machine learning model that would make it work better.",0,3,2025-06-01 04:53:55,Ordinary_Pin_7636,0.6908,0.116,0.0,0.884,0.25,0.45
[D] Paramorphic Learning,"I've been developing a conceptual paradigm called **Paramorphic Learning (PL)** and wanted to share it here to get your thoughts.

At its heart, PL is about how a learning agent or computational mind could intentionally and systematically transform its own internal form. This isn't just acquiring new facts, but changing how it operates, modifying its core decision-making policies, or even reorganizing its knowledge base (its ""memories"").

The core idea is an evolution of the agent's internal structure to meet new constraints, tasks, or efficiency needs, while preserving or enhancing its acquired knowledge. I call it ""Paramorphic"" from ""para-"" (altered) + ""-morphic"" (form) – signifying this change in form while its underlying learned intelligence purposefully evolves.

**Guiding Principles of PL I'm working with:**

* **Knowledge Preservation & Evolution:** Leverage and evolve existing knowledge, don't discard it.
* **Malleable Form:** Internal architecture and strategies are fluid, not static blueprints.
* **Objective-Driven Transformation:** Changes are purposeful (e.g., efficiency, adapting to new tasks, refining decisions).
* **Adaptive Lifecycle:** Continuous evolution, ideally without constant full retraining.

**What could this look like in practice for a learning agent?**

* **Adaptive Operational Strategies:** Instead of fixed rules, an agent might develop a sophisticated internal policy to dynamically adjust its operational mode (e.g., research vs. creative synthesis vs. idle reflection) based on its state and goals.
* **Evolving Decision-Making Policies:** The mechanisms for making decisions could themselves adapt. The agent wouldn't just learn what to do, but continuously refine how it decides what to do.
* **Meta-Cognition (Self-Awareness of Form & Performance):** A dedicated internal system could:
   * Monitor its own transformations (changes in operational state, knowledge structure, decision effectiveness).
   * Identify areas for improvement (e.g., learning stagnation, ineffective strategies).
   * Purposefully guide adaptation (e.g., by prioritizing certain tasks or triggering internal ""reflections"" to find more effective forms).
* **Dynamic Knowledge Structuring:** Beyond just adding info, an agent might learn to restructure connections, identify deeper analogies, or develop new ways of representing abstract concepts to improve understanding and idea generation.

**The Challenge: Lean, Local, and Evolving Digital Minds**

A lot of inspiration for these capabilities comes from large-scale systems. My specific interest is in distilling the essence of these features (adaptive learning, meta-cognition, self-improvement) and finding ways to implement them lean, efficiently, and locally – for instance, in a browser-based entity that operates independently without massive server infrastructure. This isn't about replicating LLMs, but enabling smaller, self-contained computational intellects to exhibit more profound and autonomous growth.

While PL is a concept, I'm actively prototyping some of these core mechanisms. The goal is to develop agents that don't just learn about the world, but also learn to be more effective learners and operators within it by intelligently reshaping themselves.

**Connections & Discussion:**  
PL naturally intersects with and builds on ideas from areas like:

* Reinforcement Learning
* Knowledge Representation
* Meta-learning
* Continual Learning
* Self-adaptive systems

These are ideas I'm ultimately bringing to my experimental project, SUKOSHI, which is a little learning agent that lives and ""dreams"" entirely in your web browser.

https://preview.redd.it/zw8s0q2cr84f1.png?width=725&format=png&auto=webp&s=58eeae24efd97e42ab73dc8f405f235a3ac888c6

",0,1,2025-06-01 04:14:25,technasis,0.9981,0.163,0.003,0.833,0.1618690968122786,0.4912411452184178
[P] AI Learns to Play Final Fight (Deep Reinforcement Learning),"My code:

[paulo101977/Ai-Final-Fight](https://github.com/paulo101977/Ai-Final-Fight)",0,0,2025-06-01 03:18:46,AgeOfEmpires4AOE4,-0.0516,0.15,0.163,0.688,0.0,0.7
"[R] arXiv endorsement request, Graph NN Model of Human and Mammalian Thought","Hello all,  This is my second paper on the Graph Model.  It develops psuedocode for most of the examples given in the first paper as well as develops a model of counting.   The model posits that the symbolic operation of the neo-cortex can be represented as a bi-directional graph neural network.   The model is implemented with only a single class that uses only a single recursive function (at run time).

paper: [https://zenodo.org/records/15566041](https://zenodo.org/records/15566041)

I would greatly appreciate it if somecould endorse me for [cs.](http://cs.ai)cl or [q-bio.nc](http://q-bio.nc)

Thanks!

[https://arxiv.org/auth/endorse?x=WCXLIK](https://arxiv.org/auth/endorse?x=WCXLIK)

  
[https://arxiv.org/auth/endorse?x=F6X46W](https://arxiv.org/auth/endorse?x=F6X46W)",0,5,2025-06-01 03:10:41,pseudocoder1,0.916,0.133,0.0,0.867,0.0824675324675324,0.4829004329004329
[D] Internal transfers to Google Research / DeepMind,"Quick question about research engineer/scientist roles at DeepMind (or Google Research).

Would joining as a SWE and transferring internally be easier than joining externally?

I have two machine learning publications currently, and a couple others that I'm submitting soon. It seems that the bar is quite high for external hires at Google Research, whereas potentially joining internally as a SWE, doing 20% projects, seems like it might be easier. Google wanted to hire me as a SWE a few years back (though I ended up going to another company), but did not get an interview when I applied for research scientist. My PhD is in theoretical math from a well-known university, and a few of my classmates are in Google Research now.",105,54,2025-05-31 22:10:31,random_sydneysider,0.5499,0.046,0.0,0.954,0.0071794871794871,0.2261538461538461
[R] How can I download VFHQ dataset in India?,"I tried everything, from running scripts to using Baidu(can't log in), but I am unable to download the VFHQ dataset in India. Can someone please guide me on how to download it?",2,0,2025-05-31 20:41:21,Friendly_Cancer001,0.5122,0.082,0.0,0.918,-0.5,0.5
